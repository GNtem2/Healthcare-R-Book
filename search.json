[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Healthcare-R-Book",
    "section": "",
    "text": "Preface\nThis is a book describing the use of R in Healthcare. It is aimed at beginners of R language. The majority of the examples are taken from works in Neurology. Where possible data from other disease such as heart disease, cancer and vaccine are used. The ideas and principles can be applied to other aspects of Healthcare.\nThe book is written in the hope that clinicians and junior doctors will take a new approach towards understanding medicine and data. One of the key aspect in the journey to becoming a doctor is leaning about medical diagnosis. This is a complex process and should appropriately take into account the history and corroborating history, examination and investigations (Centor, Geha, and Manesh 2019). However, early on in medical school, students are taught list of associations and frequency of signs and symptoms related to a disease, rather than recognition of patterns. Medical students later become junior doctors and would pride themselves on the ability to generate these lists. An analogy to these lists is performing univariable regression to explore their occurrences in the condition compare to an alternative diagnosis. However, the findings from univariable regression do not convey a meaning about relationship of the variables. Similarly, the teaching that vasculitis is associated with stroke. This has led many junior doctors to search for rare association such vasculitis among young patients with stroke. In an audit over 10 years at Monash Health, there was only one case of vasculitis presenting as stroke (Kempster, McLean, and Phan 2016). Another example would be searching for temporal arteritis among elderly patients presenting with stroke. This action is performing without realising that few patients with arteritis have stroke and few patients with stroke have arteritis. Often patients developed arteritis first and their associated symptoms. Stroke may present later in the course of arteritis and sometimes even after treatment for arteritis. By contrast, a senior clinician would discuss why certain diagnoses and not others were considered in the differential diagnoses given the occurrence of selected symptoms and signs. In statistical analysis, patterns can be found using a variety of multivariate methods (Hastie, Tibshirani, and Friedman 2009). These methods are not taught except in advance Statistics or in machine learning courses. Consequently, there is a lack of appreciations of these methods and how they can be applied in Healthcare. In this book a wide variety of multivariate methods will be briefly demonstrated to give the clinicians a glimpse of the possibilities.\nStatistics is taught in at rudimentary level in school and during specialty training. Given this lack of emphasis of an important subject, it is not surprising that students and junior doctors do not embrace statistics. When it is done it is through the use of commercial statistical software such as SPSS which has a graphical user interface (GUI) and encourage the user to perform analysis by clicking. R is a statistical program which takes a completely opposite direction. It requires the user to be able to code and understand why a task should be done. This drawback means that many students, junior doctors and clinicians do not appreciate the advantage of R: free open source software, large online community who are willing to share their codes and direct access to statisticians and bioinformaticians who write the softwares. A lot of the new ideas in statistics have libraries available in R. By contrast, it would take several years for commercial software to catch-up. R can be used to scrape data from the internet or interface with data platform such as Google Maps application programming interface (API), Youtube, Twitter. Rstudio, the integrated development environment (IDE) of R, provides platform for Shiny app development, creation of web document and writing of book (such as this one).\nThis book takes on a non-traditional approach to teaching R. It emphasises learning R by examples. Often data science course spend time explaining how R treat data as vector and manipulate data symbolically. Data manipulation is the foundation of data science but can bore those new to R. That aspect is left to the next chapter on data wrangling. This chapter is an introduction to ggplot2. Another aspect of learning R is that the libraries come with many free dataset. Clinicians do not find the diamond or car or gapminder datasets useful as they are not related to medicine. On the other hand, the actual Titanic data, with passenger list and their fate, may be of interest. In this book we will try to use dataset which are directly related to medicine or topics of high interest such as the COVID-19 data. Some of the data provided here comes from publications by the Neurology Department, some are simulated and some have come from the internet (COVID-19) and some dataset are provided by R (eg fertility, cancer (lung, breast), leukemia, lymphoma, coronary artery disease, diabetes, hepatitis and microbiome). The dataset available in R in the following packages datasets, Stat2Data and mlbench. Additional datasets are available from external website such as Kaggle and UCI Machine learning Databases. For example, the heart disease data are available from https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/. It is encouraged that the reader visit these websites to obtain data for learning R. Dataset from these websites are labelled in this book as coming from the ExtData folder. Unless indicated, the data use in this book can be found in the Data-Use folder. Researchers working on animal dataset may find the principles of analysis described here useful for animal research.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nCentor, Robert M., Rabih Geha, and Reza Manesh. 2019. “The Pursuit of Diagnostic Excellence.” JAMA Network Open 2 (12): e1918040–40. https://doi.org/10.1001/jamanetworkopen.2019.18040.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference and Prediction. 2nd ed. Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/.\n\n\nKempster, P. A., C. A. McLean, and T. G. Phan. 2016. “Ten year clinical experience with stroke and cerebral vasculitis.” J Clin Neurosci 27 (May): 119–25.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Beginners’ guide to R\nIt is often said that the majority of time is spent on data cleaning and 20% on analysis. A common trap when one starts using R is that the library have not been installed or the data are in different folder to the working directory. Installing library is easy with the command install.packages. The command getwd() will tell you which directory you are in.\nSometimes, the user encounter issues with R. This is not an uncommon problem and often not due to serious errors but forgetting that R is case-sensitive. Also when copying codes from the net across, R does not recognise the “inverted comma” but has its own style of “inverted comma”. Unlike Python, R does not tolerate space between variables. R also does not like variables to be named starting with a number.\nR treats the variables in certain ways depend on their class. Some function requires that the variables are numeric. The Breast Cancer data from mlbench has 11 columns with the 9 covariates being factors and ordered factors. These issues will be dealt further in the chapter on data wrangling. One way to get an overview of the structure of the data is to use the glimpse function from dplyr.\nAnother issue is that some libraries have assign certain names to their function and which is also used by other libraries. In the course of opening multiple libraries, R may be confused and use the last library open and which may lead to error result. We will illustrate this with the forest function in the Statistics chapter.\nA selling point of R is that there is a large community of users who often post their solutions online. it’s worth spending time on stack overflow to look at similar problems that others have and the approaches to solving them. A clue to pose questions is to look at the error. If you wish to post questions for others to help then it’s encouraged that the question include some data so that the person providing the solution can understand the nature of the errors. Useful blogs are also available at https://www.r-bloggers.com/.\nThis chapter focusses on the ggplot2 and its related libraries for plotting (Wickham 2016). Base R also has plotting function but lacks the flexibility of ggplot2. Plotting is introduced early to enage clinicians who may not have the patience read the following chapter on data wrangling prior. The book is not intended to be used in a sequential fashion as the reader may find elements of this chapter relevant to them and jump to another chapter such as chapter 3 on statistics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#plot-using-base-r",
    "href": "intro.html#plot-using-base-r",
    "title": "1  Introduction",
    "section": "2.1 Plot using base R",
    "text": "2.1 Plot using base R\nBelow we illustrate bar plot using base R.\n\ndata(\"Leukemia\", package=\"Stat2Data\")\n#AML dataset-treatment\ncolnames(Leukemia)\n\n[1] \"Age\"    \"Smear\"  \"Infil\"  \"Index\"  \"Blasts\" \"Temp\"   \"Resp\"   \"Time\"  \n[9] \"Status\"\n\n#base R\nhist(Leukemia$Age, 8, xlab = \"Age\",main=\"Acute Myeloid Leukemia Dataset\")\n\n\n\n\n\n\n\n\nLine plot can be performed in base R using abline argument. The font is defined by cex argument and colour defined by col argument. The title is defined by main argument. The X and Y axes can be labelled using xlab and ylab arguments within the plot function. Below we illustrate example of drawing a line to a point and in the segment below we illustrate example of drawing a line.\n\n#set parameters of plot\nX=seq(0,1,by=.1) #define a set of point from 0 to 1, separated by 0.1.\nY=seq(0,1,by=.1)\n\n#define A and B\nA=.065; B=.44\n\n#location of point\nplot(X,Y, main=\"ROC curve\")\npoints(A,B,pch=8,col=\"red\",cex=2) #add point\nabline(coef = c(0,1)) #add diagonal line\n\n#draw line to a point\nsegments(x0=0,y0=0,x1=A,y1=B,col=\"blue\")\nsegments(x0=A,y0=B,x1=1,y=1,col=\"blue\")\n\n\n\n\n\n\n\n\nThis is an illustration of using base R to plot regression line. Later, we will illustrate using geom_smooth call from ggplot2.\n\nTPR_1=.44; FPR_1=.065\n\nplot(X,Y,main=\"Likelihood ratio graph\", xlab=\"1-Specificity\",ylab=\"Sensitivity\",cex=.25)\n  points(.02,.8,pch=8,col=\"red\",cex=2) #add point\n  df1&lt;-data.frame(c1=c(0,TPR_1),c2=c(0,FPR_1))\n  reg1&lt;-lm(c1~c2,data=df1)\n  df2&lt;-data.frame(c1=c(TPR_1,1),c2=c(FPR_1,1))\n  reg2&lt;-lm(c1~c2,data=df2)\n  abline(reg1) #draw line using coefficient reg1\n  abline(reg2) #draw line using coefficient reg2\n  text(x=FPR_1,y=TPR_1+.3,label=\"Superior\",cex=.7)\n  text(x=FPR_1+.2,y=TPR_1+.2,label=\"Absence\",cex=.7)\n  text(x=.0125,y=TPR_1-.1,label=\"Presence\",cex=.7)\n  text(x=FPR_1+.1,y=TPR_1,label=\"Inferior\",cex=.7)\n\n\n\n\n\n\n\n\nFunction can be plotted using variations of the above. This requires a formula to describe variable Y. Shading in base R is performed with the polygon function.\n\nAUC_Logistic&lt;-function (A,B,C,D){\n  #binary data\n  #A=True pos %B=False positive   %C=False negative   %D=True negative\n  \n  TPR=A/(A+C)\n  FPR=1-(D/(D+B))\n  \n  #binomial distribution sqrt(np(1-p))\n  #Statist. Med. 2002; 21:1237-1256 (DOI: 10.1002/sim.1099)\n  STDa=sqrt((A+C)*TPR*(1-TPR));\n  STDn=sqrt((B+D)*FPR*(1-FPR));\n  a=STDn/STDa;\n  theta=log((TPR/(1-TPR))/((FPR/(1-FPR))^a));\n  \n  #define a set of point from 0 to 1, separated by 0.001.\n  X=seq(0,1,by=0.001)  \n  \n  #logistic regression model\n  Y1=(X^a)/(X^a+(((1-X)^a)*exp(-1*theta)));\n  AUC=round(pracma::trapz(X,Y1),2)\n  AUC\n\n#SE using Hanley & McNeil\n#Preferred method if more than one TPR,FPR data point known\n#Hanley is less conservative than Bamber\n  Nn=B+D;\n  Na=A+C;\n  Q1=AUC/(2-AUC);\n  Q2=2*AUC^2/(1+AUC);\n  \nSEhanley=sqrt(((AUC*(1-AUC))+((Na-1)*(Q1-AUC^2))+((Nn-1)*(Q2-AUC^2)))/(Na*Nn))\n\n#SE using Bamber\n#Ns is the number of patients in the smallest group\nif (A+C&gt;B+D) {\n  Ns=B+D\n  } else {\n   Ns=A+C\n  }\nSEbamber=sqrt((AUC*(1-AUC))/(Ns-1))\n\n# plot smoothed ROC\nplot(X,Y1,main=\"ROC curve\", xlab=\"1-Specificity\",ylab=\"Sensitivity\",cex=.25)\npoints(FPR,TPR,pch=8,col=\"red\",cex=2) #add point\n\nY2=0\npolygon(c(X[X &gt;= 0 & X &lt;= 1],\n          rev(X[X &gt;= 0 & X &lt;= 1])),\n        c(Y1[X &gt;= 0 & X &lt;= 1],\n          rev(Y2[X &gt;= 0 & X &lt;= 1])),\n        col = \"#6BD7AF\")\n\n\nprint(paste(\"The Area under the ROC curve using the logistic function is\", \n            AUC,\". The Area under the ROC curve using rank sum method is\", round(.5*(TPR+(1-FPR)),2)))\n}\n\nAUC_Logistic(10,20,2,68)\n\n\n\n\n\n\n\n\n[1] \"The Area under the ROC curve using the logistic function is 0.84 . The Area under the ROC curve using rank sum method is 0.8\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#ggplot2",
    "href": "intro.html#ggplot2",
    "title": "1  Introduction",
    "section": "2.2 ggplot2",
    "text": "2.2 ggplot2\nThe plot below uses ggplot2 or grammar of graphics. The plot is built layer by layer like constructing a sentence. Plotting is a distinct advantage of R over commercial software with GUI (graphical user interface) like SPSS. A wide variety of media organisations (BBC, Economist) are using ggplot2 with their own stylised theme. The plot has a certain structure such the name of the data and aesthetics for x and y axes. For illustration purpose the aesthetics are labelled with x and y statements. The fill argument in the aesthetics indicate the variables for coloring. The colors chosen for this graph were imposed by the journal Epilepsia (Seneviratne et al. 2019). To run the examples, check that you have install the libraries. an error can occurred if you don’t have the required library. The meta-character # is used to signal that the line is meant to comment the code ie R will not read it. The install.packages command only need to be run once.\n\n2.2.1 Histogram\nThe flexibility of ggplot2 is shown here in this histogram. The legend can be altered using the scale_fill_manual function. If other colours are preferred then under values add the preferred colours.\nThere are different ways to use ggplot2: quick plot or qplot with limited options and full ggplot2 with all the options. The choice of the method depends on individual preference and as well as reason for plotting.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n#qplot\nqplot(Age, data=Leukemia, bins=8)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nNow a more complex version of histogram with ggplot with color added.\n\n#ggplot2\nggplot(data=Leukemia,aes(Age,fill=as.factor(Resp)))+\n  geom_histogram(bins=8)\n\n\n\n\n\n\n\n\nNote the legend is a bit untidy. The legend can be changed using scale_fill_manual. The color can be specified using rgb argument. This is important as some journals prefer certain color.\n\n#adding legend, changing the values 0 and 1 to treatment response\nggplot(data=Leukemia,aes(Age,fill=as.factor(Resp)))+\n  geom_histogram(bins=8)+\n  scale_fill_manual(name=\"Response\",values=c(\"#999999\",\"#56B4E9\"),\n                    breaks=c(0,1),labels=c(\"No Treatment\",\"Treatment\"))\n\n\n\n\n\n\n\n\nAdding title is easy with ggtitle.\n\n#adding title\nggplot(data=Leukemia,aes(Age,fill=as.factor(Resp)))+\n  geom_histogram(bins=8)+\n  scale_fill_manual(name=\"Response\",\n                    values=c(\"#555555\",\"#56B4E9\"),\n                    breaks=c(0,1),\n                    labels=c(\"No Treatment\",\"Treatment\"))+\n  ggtitle(\"Acute Myeloid Leukemia Treatment dataset\")\n\n\n\n\n\n\n\n\n\n\n2.2.2 Bar plot\nPreviously, we had used base R for bar plot. Here we use the geom_bar argument in ggplot.\n\nlibrary(ggplot2)\nlibrary(extrafont)\n\nRegistering fonts with R\n\n#this data came from a paper published in Epilespia 2019 on cost of looking \n#after patients with pseudoseizure \n\n##                              Lower  Upper\n##                    percentage 0.95   0.95  \n## Duration_PNES.lmg  0.0974     0.0057 0.2906\n## pseudostatus.lmg   0.2283     0.0687 0.3753\n## Hx_anxiety.lmg     0.0471     0.0057 0.1457\n## Hx_depression.lmg  0.0059     0.0041 0.1082\n## DSP.lmg            0.0582     0.0071 0.1500\n## seizure_burden.lmg 0.0179     0.0041 0.1058\n## sex.lmg            0.0413     0.0030 0.1519\ndf&lt;-data.frame(\"Predictors\"=c(\"Duration_PNES\",\"pseudostatus\",\"Hx_anxiety\",\n\"Hx_depression\",\"DSP\",\"seizure_burden\",\"sex\"),\n    \"Importance\"=c(0.09737,0.22825, 0.047137,0.00487,0.058153,0.01786,0.04131),\n    \"Lower.95\"=c(0.0057,0.0687,0.0057,0.0041,0.0071,0.0041,0.0030),\n    \"Upper.95\"=c(0.2906,0.3753,0.1457,0.1082,0.1500,0.1058,0.1519))\n\n#check dimensions of data frame\ndim(df)\n\n[1] 7 4\n\n#check variables in data frame\ncolnames(df)\n\n[1] \"Predictors\" \"Importance\" \"Lower.95\"   \"Upper.95\"  \n\n#bar plot uses geom_bar \nggplot(df, aes(x=Predictors,y=Importance))+\n  geom_bar(stat=\"identity\")\n\n\n\n\n\n\n\n\nThis bar plot may be considered as untidy as the variables have not been sorted. Reordering the data requires ordering the factors. The colors were requested by the journal Epilesia in order to avoid recognitin of the bar from color blindness.\n\n#reordering the data\ndf3&lt;-df2&lt;-df\ndf3$Predictors&lt;-factor(df2$Predictors, levels=df2[order(df$Importance),\"Predictors\"])\n#adding color\np&lt;-ggplot(df3, aes(x=Predictors,y=Importance,fill=Predictors))+\n  geom_bar(colour=\"black\",\n    stat=\"identity\",\n    fill=\n      c(\"#e4b84b\",\"#ce8080\",\"#511c23\",\"#e37c1d\",\"#ffde75\",\"#abb47d\",\"#a1c5cb\"))\np\n\n\n\n\n\n\n\n\nThis bar plot is now ordered but the labels on the axis seem to run into each other. One solution is to tile the axis title using element_text. Note that the text size can also be specified within this argument.\n\n#rotate legend on x axis label by 45\np+theme(axis.title.y = element_text(face=\"bold\",  size=12),\n        axis.title.x = element_text(face=\"bold\", size=12), \n        axis.text.x  = element_text(angle=45, vjust=0.5, size=10))\n\n\n\n\n\n\n\n\nThe title can be broken up using the backward slash.\n\n#adding break in title\np1&lt;-p+geom_errorbar(aes(ymin=Lower.95,ymax=Upper.95,width=0.2))+\n  labs(y=\"R2 exaplained (%)\")+\n  theme(text=element_text(size=10))+\n  ggtitle(\" Relative Importance of Regressors \\n Cost among patients with non-epileptic seizure\")\np1\n\n\n\n\n\n\n\n\n\n\n2.2.3 Pie chart\nThis example uses the data above on contribution of non-epileptic seizure variables to hospitalised cost.\n\nlibrary(ggplot2)\ndf3$Percentage=round(df3$Importance/sum(df3$Importance)*100,0)\nggplot(df3, aes(x=\"\" ,y=Percentage,fill=Predictors))+\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() \n\n\n\n\n\n\n\n\n\n\n2.2.4 Scatter plot\nThe above data is used here to illustrate scatter plot. We can denote the color difference among the Predictors by adding color argument in the aesthetics.\n\n#color in qplot\nqplot(data=df3, Predictors, Importance,color=Predictors)\n\n\n\n\n\n\n\n\nAdding color in ggplot is the same as in qplot.\n\n#color ggplot\nggplot(df3, aes(x=Predictors,y=Importance,color=Predictors))+\n  geom_point()+\n  theme(axis.title.y = element_text(face=\"bold\",  size=10),\n        axis.title.x = element_text(face=\"bold\", size=10), \n        axis.text.x  = element_text(angle=45, vjust=0.5, size=10))\n\n\n\n\n\n\n\n\nThe size argument within aes can be used like color. In this case, it’s used to denote the importance of the predictors.\n\n#size\nggplot(df3, aes(x=Predictors,y=Importance,color=Predictors,size=Predictors))+\n  geom_point()+\n  theme(axis.title.y = element_text(face=\"bold\",  size=12),\n        axis.title.x = element_text(face=\"bold\", size=12), \n        axis.text.x  = element_text(angle=45, vjust=0.5, size=12))\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nThis is a more complicated example of scatter plot combined with formula of regression line. The paste0 function is used to add the equation to the plot. The data comes from GBD 2016 publication on lifetime risk of stroke. A comparison with plotting from base R is also provided.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload(\"./Data-Use/world_stroke.Rda\")\n#fitting a regression model\nfit&lt;-lm(MeanLifetimeRisk~LifeExpectancy,data=world_sfdf)\nfitsum&lt;-summary(fit)\n#base R scatter plot with fitted line\nx=world_sfdf$LifeExpectancy #define x\ny=world_sfdf$MeanLifetimeRisk #define y\nplot(x,y, data=world_sfdf, main = \"Lifetime Stroke Risk\",\n     xlab = \"Life Expectancy\", ylab = \"Life time Risk\",\n     pch = 19)\nabline(lm(y ~ x, data = world_sfdf), col = \"blue\")\n\n\n\n\n\n\n\n\nThe ggplot version is now provided. Note that the line is fitted in the geom_smooth argument. An interesting aspect of this plot is that the data can be describe as heterosecadic in which the variance changes throughout the plot.\n\n#ggplot2 scatter plot with fitted line\nSR&lt;-ggplot(world_sfdf,  aes(x=LifeExpectancy,y=MeanLifetimeRisk))+\n  geom_smooth(method=\"lm\")+geom_point()+\n  xlab(\"Life Expectancy\")+\n  ggtitle(paste0(\"Life time Risk\", \"=\", \n                 round(fitsum$coefficients[1],2),\"+\",\n                 round(fitsum$coefficients[2],2),\" x \",\"Life Expectancy\"))\nSR\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTo use the name of the country as label, we use geom_text. The world_sfdf data is now partitioned to show only data from Europe. An interesting pattern emerge. There is clumping of the data around Belgium and Portugal. The nudge_x and nudge_y function are used to adjust the labels and the size argument adjust the label.\n\nlibrary(tidyverse)\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\nworld_sfdf %&gt;% filter(Continent==\"EUROPE\") %&gt;%\nggplot( aes(x=LifeExpectancy,y=MeanLifetimeRisk))+\n  geom_smooth(method=\"lm\")+\n  xlab(\"Life Expectancy\")+\n  geom_text(aes(label=Country, nudge_x=.35, nudge_y=.5, avoid_overlap=T), \n            size=2)\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn order to understand the reason for deviations from the fitted line above, it is possible possible to add additional step to explore the relationship for each income group. This graph illustrates that the high income countries have a ceiling in the relationship between lifetime risk and life expectancy from age of 70 onward.\n\nSRIncome&lt;-ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+\n  geom_smooth(method=\"lm\", \n              aes(group=Income, linetype=Income, colour= Income))+ \n  geom_point()+\n  xlab(\"Life Expectancy\")\n\nSRIncome\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n2.2.5 arrange plot in grids\nPlots can be arrange in tabular format for presentation or journal submission.In base R multiple plots can be combined using par function and specify the number of columns by mfrow. The number of columns can be specified with ncol call when using gridExtra library.\n\n#Leukemia data\npar(mfrow=c(1,2)) #row of 1 and 2 columns\n\nx=Leukemia$Age #define x\ny=Leukemia$Blasts #define y\n\nplot(x,y, data=Leukemia, main = \"Leukemia data\",\n     xlab = \"Age\", ylab = \"Blasts\",\n     pch = 19)\nabline(lm(y ~ x, data = Leukemia), col = \"blue\")\n\ny1=Leukemia$Smear\nplot(x,y1, data=Leukemia, main = \"Leukemia data\",\n     xlab = \"Age\", ylab = \"Smear\",\n     pch = 19)\nabline(lm(y1 ~ x, data = Leukemia), col = \"blue\")\n\n\n\n\n\n\n\n\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nSRContinent&lt;-ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+\n  geom_smooth(method=\"lm\", \n              aes(group=Continent, linetype=Continent, colour= Continent))+\n  geom_point()+\n  xlab(\"Life Expectancy\")+\n  ylim(c(0,50))\n\ngrid.arrange(SRIncome, SRContinent, ncol=1) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAn alternative way to display multiple plots is to use patchwork library.\n\nlibrary(patchwork)\nSRIncome/SRContinent\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n2.2.6 Line plot\nThe data below is generated in the section on data simulation. The data were simulated using summary data from recent clot retrieval trials in stroke Campbell et al. (2015)\n\nlibrary(ggplot2)\nlibrary(dplyr)\ndtTime&lt;-read.csv(\"./Data-Use/dtTime_simulated.csv\")\ndtTrial&lt;-read.csv(\"./Data-Use/dtTrial_simulated.csv\")\n\n#summarise data using group_by\ndtTime2&lt;-dtTime %&gt;%\n  group_by(period, T) %&gt;%\n  summarise(meanY=mean(Y),\n            sdY=sd(Y),\n            upperY=meanY+sdY,\n            lowerY=meanY-sdY)\n\n`summarise()` has grouped output by 'period'. You can override using the\n`.groups` argument.\n\n#individual line plot\nggplot(dtTime,aes(x=as.factor(period),y=Y))+\n  geom_line(aes(color=as.factor(T),group=id))+\n  scale_color_manual(values = c(\"#e38e17\", \"#8e17e3\")) + xlab(\"Time\")+ylab(\"NIHSS\")\n\n\n\n\n\n\n\n\nThe line plot can also be represented as boxplot without the connecting lines.\n\n#box plot\ngg&lt;-ggplot(dtTime,aes(x=as.factor(period),y=Y))+\n  geom_boxplot(aes(color=as.factor(T)))+  \n  xlab(\"Time\")+ylab(\"NIHSS\")\ngg+scale_fill_discrete(name=\"Treatment\")\n\n\n\n\n\n\n\n\nTo perform line plot on the grouped data, first fit the regression line to the grouped data.\n\n#linear regression Y1 predict Y2 where Y1 and Y2 are grouped data \n#from the simulated data above.\nfit&lt;-lm(Y1~Y0+T, data=dtTrial)\n\ndtTrial2&lt;-filter(dtTrial, T==1)\n\nfit2&lt;-lm(Y2~Y1, data=dtTrial2)\n\n#line plot by group\npd &lt;- position_dodge2(width = 0.2) # move them .2 to the left and right\ngbase  = ggplot(dtTime2, aes(y=meanY, colour=as.factor(T))) + geom_errorbar(aes(ymin=lowerY, ymax=upperY), width=.3, position=pd)+\ngeom_point(position=pd) \ngline = gbase + geom_line(position=pd) \n\ndtTime2$period=as.numeric(dtTime2$period)\nunique((dtTime2$period))\n\n[1] 0 1 2 3\n\ngline = gline %+% dtTime2\n\nprint(gline + aes(x=period))\n\n\n\n\n\n\n\n\n\n\n2.2.7 Facet wrap\nFacet wrap is a good way to visually explore different aspects pf the data. Using the dtTime data above, the plots are separated by trial assignment.\n\nggplot(dtTime,aes(x=as.factor(period),y=Y))+\n  geom_line(aes(color=as.factor(T),group=id))+\n  scale_color_manual(values = c(\"#e38e17\", \"#8e17e3\"))+ \n  facet_wrap(~T)+ \n  xlab(\"Time\")+ylab(\"NIHSS\")\n\n\n\n\n\n\n\n\n\n\n2.2.8 Polygons\nThe geom_polygon is often used in thematic plot of maps. It can be used to show polygons outside of map. It requires one data frame for coordinate and another for the values.\n\n#simulate data\nids &lt;- factor(c(\"1\", \"2\", \"3\", \"4\",\"5\",\"6\"))\n\nvalues &lt;- data.frame(\n  id = ids,\n  value = c(3, 3.1, 3.2, 3.3,3.4,3.5)\n)\n\na=seq(0,1.5,by=0.5)\nx=c(a,a-.5,a+.5,a+.2, a-.3,a+.1)\n\npositions &lt;- data.frame(\n  id = rep(ids, each = 4),\n  x=c(a,a-.5,a+.5,a+.2, a-.3,a+.1),\n  y=sample(x)\n  )\n\n# Currently we need to manually merge the two together\ncomb &lt;- merge(values, positions, by = c(\"id\"))\n\np &lt;- ggplot(comb, aes(x = x, y = y)) +\n  geom_polygon(aes(fill = value, group = id))\n  \np\n\n\n\n\n\n\n\n\n\n\n2.2.9 Gantt chart\nGantt chart can be used to illustrate project timeline. It needs a minimum of 4 data columns: Activity, Project, a start date and end date. This example below is meant as a template. If you have 6 rows of data then add 2 extra rows of data including colours.\n\nlibrary(tidyverse)\ngantt_df&lt;-data.frame(item=seq(1:4), \n    activity=c(\"Ethics submission\",\"Length\",\"Recruitment\",\"Follow up\"),\n    category=c(\"Ethics\",\"Duration\",\"Recruitment\",\"Follow up\"),\n    Start=c(\"2020-06-01\",\"2021-01\",\"2021-01-01\",\"2022-01-01\"),\n    End=c(\"2021-01-01\",\"2023-01-01\",\"2022-01-01\",\"2023-01-01\"))\n\n#Set factor level to order the activities on the plot\ngantt_df &lt;- mutate(gantt_df,\n      activity=factor(activity,levels=activity[1:nrow(gantt_df)]),\n      category=factor(category,levels=category[1:nrow(gantt_df)]))  \nplot_gantt &lt;- qplot(ymin = Start,\n                    ymax = End,\n                    x = activity,\n                    colour = category,\n                    geom = \"linerange\",\n                    data = gantt_df,\n                    size = I(10)) +    #width of line\n    scale_colour_manual(values = c(\"blue\", \"red\", \"purple\", \"yellow\")) +\n    coord_flip() +\n    xlab(\"\") +\n    ylab(\"\") +\n    ggtitle(\"Project plan\")\nplot_gantt\n\n\n\n\n\n\n\n\n\n\n2.2.10 Heatmap\nThe ggplot2 library can also be used for creating heatmap. This plot uses the geom_tile function.\n\nlibrary(ggplot2)\nlibrary(plyr)\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nlibrary(reshape)\n\n\nAttaching package: 'reshape'\n\n\nThe following objects are masked from 'package:plyr':\n\n    rename, round_any\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n#swiss fertility dataset from 1888\ndata(swiss, package = \"datasets\") \n\nswiss$swiss_canton&lt;-row.names(swiss) #assign column name to row name\nrownames(swiss)&lt;-NULL #remove row name\ndata.m &lt;- melt(swiss)\n\nUsing swiss_canton as id variables\n\n data.m &lt;- ddply(data.m, .(variable), transform, rescale = rescale(value))\n q &lt;- ggplot(data.m, aes(variable, swiss_canton)) + \n         geom_tile(aes(fill = rescale), colour = \"white\")+\n        scale_fill_gradient(low = \"white\", high = \"steelblue\")+\n   ggtitle(\"Swiss Fertility Data 1888\")\n q",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#ggplot2-extra",
    "href": "intro.html#ggplot2-extra",
    "title": "1  Introduction",
    "section": "2.3 ggplot2 extra",
    "text": "2.3 ggplot2 extra\nThe following plots retains the framework of ggplot2. Their uses require installing additional libraries.\n\n2.3.1 Alluvial and Sankey diagram\nThe Sankey flow diagram uses the width of the arrow used to indicate the flow rate. It is often used to indicate energy dissipation in the system. There are several libraries providing Sankey plot such as networkD3 library. Alluvial plot is a subset of Sankey diagram but differs in having a structured workflow. The ggalluvial library is chosen here for demonstration as it forms part of the ggplot2 framework.\n\nlibrary(ggalluvial)\nlibrary(Stat2Data)\ndata(\"Leukemia\") #treatment of leukemia\n#partition Age into 8 ordered factors\nLeukemia$AgeCat&lt;-cut_interval(Leukemia$Age, n=8, ordered_result=TRUE)\n#axis1\nggplot(data=Leukemia, aes (y=Smear,axis1=AgeCat, axis2=Resp,axis3=Status))+\n  geom_alluvium(aes(fill=AgeCat),width = 1/12)+\n  geom_label(stat = \"stratum\", infer.label = TRUE) +\n  geom_label(stat = \"stratum\", infer.label = TRUE)+\n  scale_x_discrete(limits = c(\"AgeCat\",\"Resp\", \"Status\"),label=c(\"Age Category\",\"Treatment Response\",\"Mortality\"), expand = c(.1, .1)) +\n  scale_fill_brewer(type = \"qual\", palette = \"Set1\") +\n  ggtitle(\"Outcome after Leukemia Treatment\")\n\n\n\n\n\n\n\n\n\n\n2.3.2 Survival plot\nThe survminer library extends ggplot2 style to survival plot. It requires several libraries such as survival for survival analysis and lubridate to parse time. A description of survival analysis is provided in the Statistics section.\n\nlibrary(survminer)\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'ggpubr'\n\n\nThe following object is masked from 'package:plyr':\n\n    mutate\n\nlibrary(lubridate)\nlibrary(survival)\n\n\nAttaching package: 'survival'\n\n\nThe following object is masked from 'package:survminer':\n\n    myeloma\n\ndata(cancer, package=\"survival\")\n\n#data from survival package on NCCTG lung cancer trial\n#https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html\n#time in days\n#status cesored=1, dead=2sex:\n#sex:Male=1 Female=2\nsfit&lt;- survfit(Surv(time, status) ~ sex, data = cancer)\nggsurvplot(sfit, data=cancer,\n           surv.median.line = \"hv\",\n           pval=TRUE,\n           pval.size=3, \n           conf.int = TRUE,\n           legend.labs=c(\"Male\",\"Female\"),xlab=\"Time (Days)\", \n           break.time.by=50,\n           font.x=5,\n           font.y=5,\n           ggtheme = theme_bw(),  \n           risk.table=T,\n           risk.table.font=2, #adjust font\n           risk.table.height=.3  #adjust table height\n           )\n\n\n\n\n\n\n\n\n\n\n2.3.3 ggraph and tidygraph\nThe igraph library does the heavy lifting in graph theory analysis. This aspect will be expanded on in the chapter on Graph Theory. However, the plotting function with igraph is still not optimal. The ggraph and tidygraph libraries extend the ggplot2 style to graph.\n\nlibrary(tidygraph)\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:reshape':\n\n    rename\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, mutate, rename\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(ggraph) \n\n#relationship among members of acute stroke team\ntpa&lt;-readr::read_csv(\"./Data-Use/TPA_edge010817.csv\")%&gt;%\n  rename(from=V1, to=V2)\n\nRows: 25 Columns: 4\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): V1, V2\ndbl (2): time.line, time.capacity\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#node by degree centrality\ngraph&lt;-as_tbl_graph(tpa) %&gt;%  mutate(degree = centrality_degree())\nggraph(graph, layout = 'fr') + \n geom_edge_link() + \n  #label size by degree centrality\n  geom_node_point(aes(size=degree))+\n  #label node\n  geom_node_text(aes(label=name),repel=T)+\n  ggtitle(\"Acute Stroke Team Network\")\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\n\n\n\n\n2.3.4 ggparty-decision tree\nDecision tree can be plotted using ggplot2 and ggparty framework. This example uses data from the Breast Cancer dataset. It explores the effect of different histological features on class of breast tissue type.\n\nlibrary(ggparty)\n\nLoading required package: partykit\n\n\nLoading required package: grid\n\n\nLoading required package: libcoin\n\n\nLoading required package: mvtnorm\n\n\n\nAttaching package: 'ggparty'\n\n\nThe following object is masked from 'package:ggraph':\n\n    geom_node_label\n\nlibrary(partykit)\nlibrary(tidyverse)\n\ndata(\"BreastCancer\",package = \"mlbench\")\n\n#check data type\nstr(BreastCancer)\n\n'data.frame':   699 obs. of  11 variables:\n $ Id             : chr  \"1000025\" \"1002945\" \"1015425\" \"1016277\" ...\n $ Cl.thickness   : Ord.factor w/ 10 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 3 6 4 8 1 2 2 4 ...\n $ Cell.size      : Ord.factor w/ 10 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 4 1 8 1 10 1 1 1 2 ...\n $ Cell.shape     : Ord.factor w/ 10 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 4 1 8 1 10 1 2 1 1 ...\n $ Marg.adhesion  : Ord.factor w/ 10 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 5 1 1 3 8 1 1 1 1 ...\n $ Epith.c.size   : Ord.factor w/ 10 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 2 7 2 3 2 7 2 2 2 2 ...\n $ Bare.nuclei    : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 10 2 4 1 10 10 1 1 1 ...\n $ Bl.cromatin    : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 3 3 3 3 3 9 3 3 1 2 ...\n $ Normal.nucleoli: Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 1 7 1 7 1 1 1 1 ...\n $ Mitoses        : Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 5 1 ...\n $ Class          : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n\nBC&lt;- BreastCancer %&gt;% select(-Id)\n\ntreeBC&lt;-ctree(Class~., data=BC, control = ctree_control(testtype = \"Teststatistic\"))\n\n#plot tree using plot from base R\nplot(treeBC)\n\n\n\n\n\n\n\n#plot tree using ggparty\n\nggparty(treeBC) +\n  geom_edge() +\n  geom_edge_label() +\n  geom_node_label(aes(label = splitvar),\n                  ids = \"inner\") +\n  geom_node_label(aes(label = info),\n                  ids = \"terminal\")\n\n\n\n\n\n\n\nggparty(treeBC) +\n  geom_edge() +\n  geom_edge_label() +\n  # map color to level and size to nodesize for all nodes\n  geom_node_splitvar(aes(col = factor(level),\n                         size = nodesize)) +\n  geom_node_info(aes(col = factor(level),\n                     size = nodesize))\n\n\n\n\n\n\n\n\n\n\n2.3.5 Map\nSeveral simple examples are provided here. They illustrate the different plotting methods used according to the type of data. It is important to check the structure of the data using class() function.\n\n2.3.5.1 Thematic map\nThe ggplot2 library can also be used to generate thematic (choropleth) map. Here we use map_data function from ggplot2 to obtain a map of USA. Geocoded data are contained in the long and lat columns. The US map data is in standard dataframe format. In this case, the geom_map function is used for mapping. The USArrests data contains a column for murder, assault, rape and urban population. The assault data presented here is normalised by the population data. This section will be expanded further in the Geospatial Analysis chapter.\n\nlibrary(dplyr)\nlibrary(ggplot2)\narrest&lt;-data(\"USArrests\") \narrest&lt;-USArrests%&gt;% add_rownames(\"region\") %&gt;% \n  mutate(region=tolower(region))\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\nUS &lt;- map_data(\"state\") \nmap_assault&lt;-ggplot()+ \n  geom_map(data=US, map=US,\n                    aes(x=long, y=lat, map_id=region),\n                    fill=\"#ffffff\", color=\"#ffffff\", size=0.15)+\n  #add USArrests data here\n  geom_map(data=arrest, map=US,\n              aes(fill=Assault/UrbanPop, map_id=region),\n                    color=\"#ffffff\", size=0.15)+\n  scale_fill_continuous(low='thistle2', high='darkred', \n                                 guide='colorbar')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_map(data = US, map = US, aes(x = long, y = lat, map_id =\nregion), : Ignoring unknown aesthetics: x and y\n\nmap_assault\n\n\n\n\n\n\n\n\nThis is a more complex example and uses state by state COVID-19 data CDC website. Steps to extract the COVID-10 is shown in the next chapter. The shapefile for USA can also be extracted from tigris library. A challenge with plotting a map of US is that the country extends far North to Alaska and East to pacific islands.\n\nlibrary(ggplot2)\nlibrary(dplyr)\ncovid&lt;-read.csv(\"./Data-Use/Covid_bystate_Table130420.csv\") %&gt;% mutate(region=str_to_lower(Jurisdiction))\n\nmap_covid&lt;-ggplot()+ \n  geom_map(data=US, map=US,\n                    aes(x=long, y=lat, map_id=region),\n                    fill=\"#ffffff\", color=\"#ffffff\", size=0.15)+\n  #add covid data here\n  geom_map(data=covid, map=US,\n              aes(fill=CumulativeIncidence31.03.20, map_id=region),\n                    color=\"#ffffff\", size=0.15)+\n  scale_fill_continuous(low='thistle2', high='darkred', \n                                 guide='colorbar')\n\nWarning in geom_map(data = US, map = US, aes(x = long, y = lat, map_id =\nregion), : Ignoring unknown aesthetics: x and y\n\nmap_covid\n\n\n\n\n\n\n\n\nIn the simple example below we will generate a map of Australian State territories color by size of area. The ggplot2 combines with sf library and uses the shape file data in the geom_sf call.\n\nlibrary(ggplot2)\nlibrary(sf)\n#shape file\nAust&lt;-st_read(\"./Data-Use/GCCSA_2016_AUST.shp\") \n\nReading layer `GCCSA_2016_AUST' from data source \n  `C:\\Users\\phant\\Documents\\Book\\Healthcare-R-Book\\Data-Use\\GCCSA_2016_AUST.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 34 features and 5 fields (with 18 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.81694 ymin: -43.74051 xmax: 167.998 ymax: -9.142176\nGeodetic CRS:  GDA94\n\ncolnames(Aust) #find out column variables\n\n[1] \"GCC_CODE16\" \"GCC_NAME16\" \"STE_CODE16\" \"STE_NAME16\" \"AREASQKM16\"\n[6] \"geometry\"  \n\nggplot() + \n  geom_sf(data=Aust,aes(fill=AREASQKM16))+\n  labs(x=\"Longitude (WGS84)\", y=\"Latitude\", title=\"Map of Australia\") + \n  theme_bw() \n\n\n\n\n\n\n\n\n\n\n2.3.5.2 tmap\nThe tmap library works in conjunction with ggplot2 and sf. The tm_shape function takes in the shape data. The tm_polygon function color the shape file with the column data of interest.\n\nlibrary(tmap)\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\n\nload(\"./Data-Use/world_stroke.Rda\")\n#data from GBD 2016 investigators\ncolnames(world_sfdf) \n\n [1] \"FIPS\"                                \n [2] \"ISO2\"                                \n [3] \"ISO3\"                                \n [4] \"UN\"                                  \n [5] \"NAME\"                                \n [6] \"AREA\"                                \n [7] \"POP2005\"                             \n [8] \"REGION\"                              \n [9] \"SUBREGION\"                           \n[10] \"LON\"                                 \n[11] \"LAT\"                                 \n[12] \"MeanLifetimeRisk\"                    \n[13] \"Country code\"                        \n[14] \"y2015\"                               \n[15] \"Continent\"                           \n[16] \"Region, subregion, country or area *\"\n[17] \"18+\"                                 \n[18] \"Code\"                                \n[19] \"Region\"                              \n[20] \"Income\"                              \n[21] \"LendingCategory\"                     \n[22] \"Other\"                               \n[23] \"LifeExpectancy\"                      \n[24] \"MeanLifetimeRiskH\"                   \n[25] \"MeanLifetimeRiskI\"                   \n[26] \"geometry\"                            \n[27] \"Country\"                             \n\nclass(world_sfdf) #contains simple features\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n#map of country income\nm&lt;-tm_shape(world_sfdf, projection = \"+proj=eck4\")+tm_polygons(\"Income\")\nm\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\n#save object as png\n#tmap_save(m,file=\"world_income.png\"\")\n\n#save as leaflet object\n#tmap_save(m,file=\"world_income.html\"\")\n\nmap of lifetime stroke risk\n\nn&lt;-tm_shape(world_sfdf, projection = \"+proj=eck4\")+tm_polygons(\"MeanLifetimeRisk\")\nn\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\n\n\n\n2.3.5.3 Voronoi\nThe ggplot2 and sf libraries are extended to include drawing of voronoi. Voronoi is a special type of polygon. It can be seen as a mathematical approach to partition regions such that all points within the polygons are closest (depending on distance metrics) to the seed points. Voronoi has been used in disease mapping (John Snow mapping of Broad Street Cholera outbreak) and meteorology (Alfred Thiessen polygon method for measuring rainfall in basin). This is a more complex coding task. It uses the geom_voronoi call from ggvoronoi library. Some libraries have vignettes to help you implement the codes. The vignette in the ggvoronoi library can be called using vignette(“ggvoronoi”). The osmdata library will be used to provide map from OpenStreetMap. A related library is OpenStreetMap. The latter library uses raster file whereas osmdata provides vectorised map data. In the chapter on Geospatial Analysis we will expand on this theme with interactive map. One issue with the use of voronoi is that there are infinite sets and so a boundary needs to set. In the example below, the boundary was set for Greater Melbourne.\n\nlibrary(dplyr)\nlibrary(ggvoronoi)\n\nrgeos version: 0.6-3, (SVN revision 696)\n GEOS runtime version: 3.11.2-CAPI-1.17.2 \n Please note that rgeos will be retired during October 2023,\nplan transition to sf or terra functions using GEOS at your earliest convenience.\nSee https://r-spatial.org/r/2023/05/15/evolution4.html for details.\n GEOS using OverlayNG\n Linking to sp version: 2.0-0 \n Polygon checking: TRUE \n\nlibrary(ggplot2)\nlibrary(sf)\n\n#subset data with dplyr for metropolitan melbourne\nmsclinic&lt;-read.csv(\"./Data-Use/msclinic.csv\") %&gt;% filter(clinic==1, metropolitan==1)\n\nVictoria&lt;-st_read(\"./Data-Use/GCCSA_2016_AUST.shp\") %&gt;% filter(STE_NAME16==\"Victoria\") \n\nReading layer `GCCSA_2016_AUST' from data source \n  `C:\\Users\\phant\\Documents\\Book\\Healthcare-R-Book\\Data-Use\\GCCSA_2016_AUST.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 34 features and 5 fields (with 18 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.81694 ymin: -43.74051 xmax: 167.998 ymax: -9.142176\nGeodetic CRS:  GDA94\n\nm&lt;-ggplot(msclinic)+\n  geom_point(aes(x=lon,y=lat))+ \n  #add hospital location\n  geom_voronoi(aes(x=lon,y=lat,fill=distance),fill=NA, color=\"black\")+ \n  #create voronoi from hospital location\n  geom_sf(data=Victoria,aes(fill=AREASQKM16)) +\n  labs(x=\"Longitude (WGS84)\", y=\"Latitude\", \n       title=\"Voronoi Map of MS Clinics in Melbourne\")\nm  \n\nWarning: `fortify(&lt;SpatialPolygonsDataFrame&gt;)` was deprecated in ggplot2 3.4.4.\nℹ Please migrate to sf.\n\n\n\n\n\n\n\n\n\nThis map is not so useful as the features of Victoria overwhelm the features of Greater Melbourne.\n\n\n\n2.3.6 ggwordcloud\nThe ggwordcloud library extend the ggplot2 family to creating wordcloud. The following is an illustration of wordcloud created from comments on Youtube video “Stroke Heroes Act Fast”.\n\nlibrary(ggwordcloud)\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(tidyverse)\nheroes_df&lt;-read.csv(\"./Data-Use/Feb_01_1_49_59 PM_2018_AEDT_YoutubeData.csv\",stringsAsFactors = FALSE)\n#cleaning data\nkeywords &lt;- heroes_df$Comment \nkeywords &lt;- iconv(keywords, to = 'utf-8')\n#create corpus\nmyCorpus &lt;- VCorpus(VectorSource(keywords))\n#lower case\nmyCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))\n#remove numer\nmyCorpus &lt;- tm_map(myCorpus, removeNumbers)\n#remove punctuation\nmyCorpus &lt;- tm_map(myCorpus, removePunctuation)\n#remove stopwords\nmyCorpus &lt;- tm_map(myCorpus, removeWords, stopwords(\"english\"),lazy=TRUE) \n#remove white space\nmyCorpus &lt;- tm_map(myCorpus, stripWhitespace, lazy=TRUE)\n#term document matrix\ndtm &lt;- DocumentTermMatrix(myCorpus,control = list(wordLengths=c(3, 20)))\n#remove sparse terms\ndtm&lt;-removeSparseTerms(dtm, 0.95)\n#remove words of length &lt;=3\ntdm=TermDocumentMatrix(myCorpus,\n                       control = list(minWordLength=4,maxWordLength=20) )\nm &lt;- as.matrix(tdm)\nv &lt;- sort(rowSums(m),decreasing=TRUE)\n#remove words with frequency &lt;=1\nd &lt;- data.frame(word = names(v),freq=v) %&gt;% filter(freq&gt;1)\n#wordcloud\nggplot(data = d, \n       aes(label = word, size = freq, col = as.character(freq))) + \n  geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,\n                      grid_size = 1, eccentricity = .8)+\n    scale_size_area(max_size = 12)+\n    scale_color_brewer(palette = \"Paired\", direction = -1)+\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n2.3.7 gganimate\nThe library gganimate can be used to generate videos in the form of gif file. The data needs to be collated from wide to long format. For the purpose of this book, the code has been turned off.\n\nlibrary(ggplot2)\nlibrary(gganimate)\n\n#ensure data in long format, Y =NIHSS\nu &lt;- ggplot(dtTime2, aes(period, meanY , color = T, frame = period)) +\n  geom_bar(stat=\"identity\") +\n  geom_text(aes(x = 11.9, label = period), hjust = 0) +   xlim(0,13)+\n  coord_cartesian(clip = 'off') + \n  facet_wrap(~meanY)+\n  labs(title = 'ECR Trials', y = 'Number') +\ntransition_reveal(period)\nu\n#create gif\n#animate(u,fps=15,duration=15)\n#anim_save(u,file=\"./Data-Use/simulated_ECR_trial.gif\", width=800, height=800)\n\n\n\n2.3.8 ggneuro\nThere are several ways of plotting mri scans. The ggneuro library is illustrated here as it relates to the ggplot2 family. The Colin \\(T_1\\) scan is a high resolution scan from MNI.\n\n#devtools::install_github(\"muschellij2/ggneuro\")\nlibrary(ggneuro)\nlibrary(neurobase)\n\nLoading required package: oro.nifti\n\n\noro.nifti 0.11.4\n\n\n\nAttaching package: 'oro.nifti'\n\n\nThe following object is masked from 'package:tidygraph':\n\n    slice\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\ncolin_1mm&lt;-untar(\"./Data-Use/colin_1mm.tgz\")\ncolinIm&lt;-readNIfTI(\"colin_1mm\") \nggortho(colinIm)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#plotly",
    "href": "intro.html#plotly",
    "title": "1  Introduction",
    "section": "2.4 plotly",
    "text": "2.4 plotly\nPlotly has its own API and uses Dash to upload the figure on the web. It has additional ability for interaction as well as create a video. Examples are provided with calling plotly directly or via ggplot2. In the examples below, the plots are performed using ggplot2 and then pass onto plotly using ggplotly function. The example uses the Leukemia dataset from Stat2Data library.\n\n2.4.1 Scatter plot with plotly\nThe plotly syntax uses a ~ after the = symbol to identify a variable for plotting.\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:neurobase':\n\n    colorbar\n\n\nThe following object is masked from 'package:oro.nifti':\n\n    slice\n\n\nThe following object is masked from 'package:reshape':\n\n    rename\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, mutate, rename, summarise\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(Stat2Data)\ndata(\"Leukemia\") #treatment of leukemia\n#scatter plot directly from plotly\nplot_ly(x=~Age,y=~Smear, #percentage of blast\n        color=~as.factor(Status),  #dead or alive\n        symbol=~Resp, \n        symbols=c('circle' ,'square'), #Response to treatment\n        data=Leukemia) \n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n\n\n\n\n\n\n\n\n2.4.2 Bar plot with plotly\nPlotly can uses a ggplot object directly.\n\n#using the epilepsy data in p generated above for bar plot\nggplotly(p)\n\n\n\n\n\n\n\n2.4.3 Heatmap\n\n#using swiss data above to create heatmap\nggplotly(q) \n\n\n\n\n\n\n\n2.4.4 map\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\n\ncovid&lt;-read.csv(\"./Data-Use/Covid_bystate_Table130420.csv\") %&gt;% mutate(region=str_to_lower(Jurisdiction)) #consistent\n\nggplotly(\n  ggplot()+ \n  geom_map(data=US, map=US,\n                    aes(x=long, y=lat, map_id=region),\n                    fill=\"#ffffff\", color=\"#ffffff\", size=0.15)+\n  #add covid data here\n  geom_map(data=covid, map=US,\n              aes(fill=NumberCases31.03.20, map_id=region),\n                    color=\"#ffffff\", size=0.15)+\n  scale_fill_continuous(low='thistle2', high='darkred', \n                                 guide='colorbar')\n)\n\nWarning in geom_map(data = US, map = US, aes(x = long, y = lat, map_id =\nregion), : Ignoring unknown aesthetics: x and y\n\n\n\n\n\n\n\n\n\n\nBerkhemer, O. A., P. S. Fransen, D. Beumer, L. A. van den Berg, H. F. Lingsma, A. J. Yoo, W. J. Schonewille, et al. 2015. “A randomized trial of intraarterial treatment for acute ischemic stroke.” N. Engl. J. Med. 372 (1): 11–20.\n\n\nCampbell, B. C., P. J. Mitchell, T. J. Kleinig, H. M. Dewey, L. Churilov, N. Yassi, B. Yan, et al. 2015. “Endovascular therapy for ischemic stroke with perfusion-imaging selection.” N. Engl. J. Med. 372 (11): 1009–18.\n\n\nSeneviratne, U., Z. M. Low, Z. X. Low, A. Hehir, S. Paramaswaran, M. Foong, H. Ma, and T. G. Phan. 2019. “Medical health care utilization cost of patients presenting with psychogenic nonepileptic seizures.” Epilepsia 60 (2): 349–57.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html",
    "href": "data-wrangling.html",
    "title": "2  Data Wrangling",
    "section": "",
    "text": "2.1 Data\nThis section on atomic vector, arrays, matrix and list is often considered boring and ignored.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#data",
    "href": "data-wrangling.html#data",
    "title": "2  Data Wrangling",
    "section": "",
    "text": "2.1.1 Vector, Arrays, Matrix\n\n2.1.1.1 Vector and list\nAll elements of an atomic vector and arrays are the same. A vector is an array with one dimension . List can contain different types of data. A complex example of a structured list is the json format shown below. In base R, c is a function for creating a vector or list. The function list can also be used to create list. It is best to avoid using c when assigning a name to a dataframe or vector (Wickham 2019).\n\na&lt;-c(1,2,3)\nis.vector(a)\n\n[1] TRUE\n\nclass(a) #numeric vector\n\n[1] \"numeric\"\n\nb&lt;-c(\"apple\",\"orange\",\"banana\")\nis.vector(b) #character vector\n\n[1] TRUE\n\nclass(b)\n\n[1] \"character\"\n\nd&lt;-c(1,2,\"banana\")\nis.list(d) #character vector\n\n[1] FALSE\n\nclass(d) #FALSE\n\n[1] \"character\"\n\ne&lt;-list(a,b,c(TRUE,FALSE,FALSE))\nis.list(e) #TRUE\n\n[1] TRUE\n\n\n\n\n2.1.1.2 Matrix and Arrays\nIn R, an array is a vector organised with attributes such as dimensions. It is of a single data type. It contains a description of the number of dimension. Array can also be accessed by its indexing.\nLater in this chapter, we will illustrate the importance of arrays for manipulating MRI scans. A volumetric mri scan, there are 3 dimenions [,,]. The first column is the sagittal sequence, second column is the coronal sequence and the third column is the axial sequence. In the example below, this knowledge of arrays can be used to reverse the ordering of information on MRI scan (flip on it axis between left and right).\n\n#vector\nvector1&lt;-c(1,2,3)\nvector2&lt;-c(4,5,6,7,8,9)\n\n# 2 dimensions\narray1&lt;-array(c(vector1,vector2),dim = c(3,3,2))\narray1\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# 3 dimensions\narray2&lt;-array(c(vector1,vector2),dim = c(2,2,3))\narray2\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9    2\n[2,]    1    3\n\n#check if array or matrix\nis.matrix(array1)\n\n[1] FALSE\n\nis.array(array1)\n\n[1] TRUE\n\n\nArrays can be accessed by indexing its structure.\n\narray1[,,1]\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nThe second array of array1\n\narray1[,,2]\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nFirst row of array1\n\narray1[1,,]\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    4    4\n[3,]    7    7\n\n\nFirst column of array1\n\narray1[,1,]\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    2    2\n[3,]    3    3\n\n\nA matrix is a rectangular array of a single data type arranged in rows and columns or a 2 dimensional array. Data type can be number or character. Matrix can be considered a linear combination of vector into a linear map. In R, the function is.matrix returns true only if there is a dimension argument in the data. This definition also means that a dat frame is not a matrix.\n\nM1&lt;-matrix(c(1,2,3, 4,5,6),nrow=2, ncol=3, byrow = T,\n           dimnames = list(c(\"row1\", \"row2\"),\n                        c(\"Col1\", \"Col2\", \"Col3\")))\nM1\n\n     Col1 Col2 Col3\nrow1    1    2    3\nrow2    4    5    6\n\n\nA tensor is multidimensional array. Using the analogy before about a matrix being a linear map, a tensor can be viewed as a multilinear map. More on tensor later.\n\n\n2.1.1.3 array operations\nArray operations are performed element wise.\n\narrayA&lt;-array(c(1,2,3),dim=c(1,3)) #1 row 3 columns\narrayA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n\narrayB&lt;-array(c(4,5,6),dim=c(1,3))\narrayB\n\n     [,1] [,2] [,3]\n[1,]    4    5    6\n\n#addition\narrayC=arrayA+arrayB\narrayC\n\n     [,1] [,2] [,3]\n[1,]    5    7    9\n\n\nNow we illustrate multiplication of arrays\n\narrayD=arrayA*arrayB\narrayD\n\n     [,1] [,2] [,3]\n[1,]    4   10   18\n\n\nDivision of arrays\n\narrayE&lt;-arrayA/arrayB\narrayE\n\n     [,1] [,2] [,3]\n[1,] 0.25  0.4  0.5\n\n\n\n\n\n2.1.2 Operators\n\n2.1.2.0.1 Relation operators\nWe test if variable A is same as B by using == operator while we test if A is not equal to B by != operator. A is greater than B by &gt; operator and A is lesser than B by &lt; operator.\n\n\n2.1.2.1 Logical operators\nThe | operator signifies elementwise or relationship and || signifies or relationship. The & operator signifies elementwise and relationship and && signifies and relationship.\n\n\n2.1.2.2 %in%\nThe %in% operator helps to evaluate if an element belong to a vector. The package Hmisc contains an operator %nin% which is the opposite of %in%.\n\n\n2.1.2.3 %&gt;%\nThe %&gt;% operator or pipe function, from magrittr package, is used to pass information to the next function.\n\nDF&lt;-data.frame(Disability=c(\"Good\",\"Moderate\",\"Good\",\"Poor\"),NIHSS=c(2,10,1,20),Sex=c(\"M\",\"F\",\"M\",\"M\" ))\n\n#create new variable outcome\nDF$outcome=ifelse(DF$Disability %in% c(\"Good\",\"Moderate\"), 0,1)\nDF\n\n  Disability NIHSS Sex outcome\n1       Good     2   M       0\n2   Moderate    10   F       0\n3       Good     1   M       0\n4       Poor    20   M       1\n\n\n\n\n\n2.1.3 Simple function\na function is written by creating name of function, calling on function (x) and brackets to define function.\n\n# function sun\nArithSum&lt;-function (x) {\n  sum(x)\n}\n\nvector1&lt;-c(1,2,3)\nArithSum(vector1)\n\n[1] 6\n\n\n\n\n2.1.4 for loop\n\nvector2&lt;-c(4,5,6)\n\nfor (i in vector2){\n  print(i)\n}\n\n[1] 4\n[1] 5\n[1] 6\n\n\nPerform math operation using for loop\n\nfor (i in vector2){\n  m= sum(vector2)/length(vector2)\n}\n\nm\n\n[1] 5\n\n\nThe next command can be used to modify the loop. This simple example removes even number\n\nfor(i in vector2){\n  if(!i %% 2){\n    next\n  }\n  print(i)\n}\n\n[1] 5\n\n\nPerform a math operation along the columns.\n\nA=c(1,2,3)\nB=c(4,5,6)\ndf=data.frame(A,B)\n\noutput_col &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in seq_along(df)) {            # 2. sequence\n  output_col[[i]] &lt;- mean(df[[i]])      # 3. body\n}\noutput_col\n\n[1] 2 5\n\n\nPerform a math operation along the rows.\n\noutput_row &lt;- vector(\"double\", nrow(df))  # 1. output\nfor (i in seq_along(df)) {            # 2. sequence\n  output_row[[i]] &lt;- mean(df[[i]])      # 3. body\n}\noutput_row\n\n[1] 2 5 0\n\n\n\n\n2.1.5 apply, lapply, sapply\n\n2.1.5.1 apply\nThe apply function works on data in array format.\n\n#sum data across rows ie c(1)\napply(array1,c(1),sum)\n\n[1] 24 30 36\n\n#sum data across columns(2)\napply(array1,c(2),sum)\n\n[1] 12 30 48\n\n#sum data across rows and columns c(1,2)\napply(array1,c(1,2),sum)\n\n     [,1] [,2] [,3]\n[1,]    2    8   14\n[2,]    4   10   16\n[3,]    6   12   18\n\n\n\n\n2.1.5.2 lapply\nThe call lapply applies a function to a list. In the section below of medical images the lapply function will be used to creates a list of nifti files and which can be opened painlessly with additional call to readNIfTI. Here a more simple example is used.\n\na&lt;-c(1,2,3,4,5,6,7,8)\nlapply(a, function(x) x^3)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 27\n\n[[4]]\n[1] 64\n\n[[5]]\n[1] 125\n\n[[6]]\n[1] 216\n\n[[7]]\n[1] 343\n\n[[8]]\n[1] 512\n\n\n\n\n2.1.5.3 sapply\nThe call sapply applies a function to a vector, matrix or list. It returns the results in the form of a matrix.\n\na&lt;-c(1,2,3,4)\nsapply(a, function(x) x^2)\n\n[1]  1  4  9 16\n\n\n\n\n2.1.5.4 tapply\nThe tapply function applies a function to a subset of the data.\n\ndat.array1&lt;-as.data.frame.array(array1)\ndat.array1\n\n  V1 V2 V3 V4 V5 V6\n1  1  4  7  1  4  7\n2  2  5  8  2  5  8\n3  3  6  9  3  6  9\n\n\n\ndat.array1$V6&lt;- dat.array1$V6 %% 2\ndat.array1$V6\n\n[1] 1 0 1\n\n\n\ntapply(dat.array1$V1, dat.array1$V6, sum) \n\n0 1 \n2 4 \n\n\nThe rapply function or recursive apply is applied to a list, contingent on the second argument. In this example, the first function is to multiple elements of the list by 2 contingent on the element being numeric.\n\na&lt;-c(1,2,3.4,D,5, \"NA\")\n\nrapply(a, function(x) x*2, class=\"numeric\")\n\n[1]  2.0  4.0  6.8 10.0\n\n\n\n\n\n2.1.6 Functional\nA functional is a function embedded in a function. Later, we will revisit functional to perform analysis on a list of nifti files.\n\nFou&lt;-lapply(df, function(A) mean(A)/sd(A))\n\n#returns a list\nFou\n\n$A\n[1] 2\n\n$B\n[1] 5\n\n\nThe unlist function returns a matrix.\n\nFou2&lt;-unlist(lapply(df, function(A) mean(A)/sd(A)))\n\n#returns a matrix\nFou2\n\nA B \n2 5 \n\n\n\n2.1.6.1 Mapply\nMapply applies the function over elements of the data.\n\nMeanFou&lt;-lapply(df, mean)\n\nMeanFou[]&lt;-Map('/', df, MeanFou)\n\nThe equivalent code with lapply is provided below.\n\nMeanFou2&lt;-lapply(df, function(M) M/mean(M))\n\nMeanFou2\n\n$A\n[1] 0.5 1.0 1.5\n\n$B\n[1] 0.8 1.0 1.2\n\n\n\n\n\n2.1.7 Iteration\nThis is an extension of the discussion on functional programming. Here we will be using purrr library and map function to apply function to multiple input. First, we use split function to partition data.\n\nlibrary(purrr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nss&lt;-read.csv(\"./Data-Use/ss150718.csv\") \nss%&gt;% \n  select(duplicate, PubYear, TP,FP,FN,TN) |&gt; \n  split(ss$duplicate) %&gt;% head()\n\n$no\n   duplicate PubYear TP FP  FN  TN\n1         no    2007 10  3   1  25\n4         no    2009 49 22   6 290\n5         no    2010  7 10   1  41\n6         no    2010 10  9   6  85\n7         no    2010 11  0   4  12\n8         no    2011 23  7   9 100\n9         no    2011 60 16  17 219\n10        no    2011  5 12   8  64\n13        no    2013  5  6   3  57\n14        no    2013  6 13   2  44\n15        no    2013 16 11  16  58\n19        no    2014  7  8   4  55\n20        no    2014 12  1   3  37\n21        no    2014 15 25  29 174\n22        no    2014 25 15  53 194\n24        no    2014 26 21  31 238\n25        no    2014 33 41 123 620\n26        no    2014 35 26  12 114\n30        no    2016 10  7   6 100\n32        no    2016 20  5  12  35\n35        no    2017 13 11  40  69\n\n$yes\n   duplicate PubYear TP FP FN  TN\n2        yes    2007 13 45  1  45\n3        yes    2009 14  7  4  36\n11       yes    2012 33 41 38 279\n12       yes    2012 37 24 36 131\n16       yes    2013 16 15  9  91\n17       yes    2013 17  7 11  77\n18       yes    2013  7  3  3   8\n23       yes    2014 25 19  2  37\n27       yes    2014 37 24 36 131\n28       yes    2014 44 43 45 188\n29       yes    2016  6 10 12  52\n31       yes    2017 16 12  9  78\n33       yes    2016 32 37 20 103\n34       yes    2017 55 66 67 521\n36       yes    2017 19 13 11  86\n37       yes    2017 10 14  4 112\n38       yes    2016 75 95 81 531\n39       yes    2017 NA NA NA  NA\n\n\nFollowing on from the splitting of the data, we perform the regression on the split data with map.\n\nss %&gt;% select(duplicate, PubYear, TP, FP, FN, TN) %&gt;% mutate(Sensitivity=TP/(TP+FN)) |&gt; \n  split(ss$duplicate) |&gt;\n  map(\\(df) lm(Sensitivity ~ PubYear, data = df))|&gt; \n  map(summary) \n\n$no\n\nCall:\nlm(formula = Sensitivity ~ PubYear, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.32962 -0.11367  0.02946  0.13633  0.25884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 99.98638   31.22820   3.202  0.00470 **\nPubYear     -0.04938    0.01552  -3.182  0.00491 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1762 on 19 degrees of freedom\nMultiple R-squared:  0.3477,    Adjusted R-squared:  0.3134 \nF-statistic: 10.13 on 1 and 19 DF,  p-value: 0.004905\n\n\n$yes\n\nCall:\nlm(formula = Sensitivity ~ PubYear, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.225540 -0.104559  0.002324  0.100728  0.314517 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 53.51463   26.03472   2.056   0.0577 .\nPubYear     -0.02627    0.01293  -2.032   0.0603 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1491 on 15 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.2158,    Adjusted R-squared:  0.1636 \nF-statistic: 4.129 on 1 and 15 DF,  p-value: 0.06026\n\n\nWe can extend this function to each element of a vector to get the output for the linear regression on the partition data.\n\nss %&gt;% select(duplicate, PubYear, TP, FP, FN, TN) %&gt;% mutate(Sensitivity=TP/(TP+FN)) |&gt; \n  split(ss$duplicate) |&gt;\n  map(\\(df) lm(Sensitivity ~ PubYear, data = df))|&gt; \n  map(summary) %&gt;%\n  map_dbl(\"r.squared\")\n\n       no       yes \n0.3476832 0.2158460 \n\n\n\n2.1.7.1 map characters\nThe map function can be applied to characters. This can be illustrated using the spot sign data\n\nmap2_chr(ss$Authors, ss$PubYear, ~paste(.x, .y, sep=\": \")) %&gt;% head()\n\n[1] \"Wada: 2007\"           \"Goldstein: 2007\"      \"Delgado Almand: 2009\"\n[4] \"Ederies: 2009\"        \"Hallevi: 2010\"        \"Park: 2010\"          \n\n\nThe characters can be mapped to upper case.\n\nlibrary(stringr)\nmap2_chr(ss$Authors, ss$PubYear, ~paste(.x, .y, sep=\" - \")) %&gt;% \n  map_chr(~str_to_upper(.)) %&gt;%\n  head()\n\n[1] \"WADA - 2007\"           \"GOLDSTEIN - 2007\"      \"DELGADO ALMAND - 2009\"\n[4] \"EDERIES - 2009\"        \"HALLEVI - 2010\"        \"PARK - 2010\"          \n\n\nThe purrr library has function to pluck items from the list or data frame.\n\nmap2_chr(ss$Authors, ss$PubYear, ~paste(.x, .y, sep=\" - \")) %&gt;% \n  pluck(1) \n\n[1] \"Wada - 2007\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#data-storage",
    "href": "data-wrangling.html#data-storage",
    "title": "2  Data Wrangling",
    "section": "2.2 Data storage",
    "text": "2.2 Data storage\nOften one assumes that opening Rstudio is sufficient to locate the file and run the analysis. One way of doing this at the console is to click on Session tab, then Set Working Directory to location of file. Another way of doing this seemlessly is to use the library here. It is easy to find the files in your directory using the list.files() call.\nTo list only some files use pattern matching.\nWe can increase the complexity of pattern matching.\n\n#list files matching pattern\nlist.files(pattern=\".Rmd|*.stan\")\n\ncharacter(0)\n\n\n\n2.2.1 Data frame\nData frame is a convenient way of formatting data in table format. It is worth checking the structure of data. Some libraries prefer to work with data in data frame while others prefer matrix or array structure.\n\na&lt;-c(1,2,3)\nb&lt;-c(\"apple\",\"orange\",\"banana\")\ne&lt;-data.frame(a,b)\nrownames(e)\n\n[1] \"1\" \"2\" \"3\"\n\n\n\n\n2.2.2 Excel data\nExcel data are stored as csv, xls and xlsx. Csv files can be open in base R using read.csv function or using readr library and read_csv function. I would urge you to get used to manipulating data in R as the codes serve as a way to keep track with the change in data. The original xcel data should not be touched outside of R. A problem with excel data is that its autocorrect function change provenance of genomic data eg SEPT1, MARCH1. SEPT1 is now relabeled as SEPTIN1.\n\nA&lt;-read.csv(\"File.csv\")\n\nB&lt;-readr::read_csv (\"File.csv\")\n\nThe readxl library can be used to open files ending in xls and xlsx.\n\nC&lt;-readxl::read_xlsx(\"File.xlsx\",skip=1) #skip first row \n\nD&lt;-readxl::read_xlsx(\"File.xlsx\",sheet=2) #read data from sheet 2\n\n\n2.2.2.1 Date and time\nDate and time can be handle in base R. The library lubridate is useful for parsing date data. It is possible to get an overview of the functions of the library by typing help(package=lubridate). Errors with parsing can occur if there are characters in the column containing date data.\n\nlibrary(dplyr)\n\ndfdate&lt;-data.frame(\"DateofEvent\"=c(\"12/03/2005\",\"12/04/2006\",NA),\n                   \"Result\"=c(4,5,6))\nclass(dfdate$DateofEvent)\n\n[1] \"character\"\n\ndfdate$DateofEvent\n\n[1] \"12/03/2005\" \"12/04/2006\" NA          \n\n\nThe date column appears to be listed as character because of NA. This is easily fixed by filtering NA.\n\ndfdate$`Date.of.Event`&lt;-dfdate$DateofEvent\n#remove NA using filter\ndfdate %&gt;% filter(!is.na(DateofEvent))\n\n  DateofEvent Result Date.of.Event\n1  12/03/2005      4    12/03/2005\n2  12/04/2006      5    12/04/2006\n\n#re assigning date data type\ndfdate$DateofEvent2&lt;-as.POSIXct(dfdate$DateofEvent)\ndfdate$DateofEvent3&lt;-as.POSIXct(dfdate$`Date.of.Event`)\nclass(dfdate$DateofEvent2)\n\n[1] \"POSIXct\" \"POSIXt\" \n\ndfdate$DateofEvent2\n\n[1] \"0012-03-20 LMT\" \"0012-04-20 LMT\" NA              \n\n\nProblem can occur when date and time are located in separate columns. The first issue is that time data is assigned a default date 1899-12-30. This error occurs as the base date in MS Office is 1899-12-30. This issue can be compounded when there are 2 time data eg a patient has stroke onset at 22:00 and is scanned at 02:00. The data become 1899-12-31 22:00 and 1899-12-31 02:00. In this case it implies that scanning occurs before stroke onset. This could have been solved at the data collection stage by having 2 separate date colums. There are several solutions inclusing ifelse but care must be taken with this argument. Note that ifelse argument convert date time data to numeric class. This can be resolved by embedding ifelse statement within as.Date argument. This argument requires origin argument. The logic argument may fail when the default date differ eg 1904-08 02:00. In this case 1904-02-08 02:00 is greater than 1899-12-31 22:00.\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ tibble  3.2.1\n✔ ggplot2 3.4.4     ✔ tidyr   1.3.0\n✔ readr   2.1.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf&lt;-data.frame(\"Onset_date\"=c(\"2014-03-06\",\"2013-06-09\"), \"Onset_Time\"=c(\"1899-12-31 08:03:00\",\"1899-12-31 22:00:00\"), \"Scan_Time\"=c(\"1904-02-08 10:00:00\",\"1899-12-31 02:00:00\")) %&gt;% \n  mutate(\n  #use update argument to reassign year, month and day\n  Scantime=update(ymd_hms(Scan_Time), year=year(ymd(Onset_date)), month=month(ymd(Onset_date)), mday=day(ymd(Onset_date))),\n \n  Onsettime=update(ymd_hms(Onset_Time), year=year(ymd(Onset_date)), month=month(ymd(Onset_date)), mday=day(ymd(Onset_date))),\n  \n  Scan_date=ifelse(Onsettime&gt;Scantime,1,0),\n  Scantime=Scantime+Scan_date*hms(\"24:00:00\"),\n  \n DiffHour=Scantime-Onsettime #minutes\n)\n\ndf %&gt;% select(-Scan_date) %&gt;% gt::gt()\n\n\n\n\n\n\n\nOnset_date\nOnset_Time\nScan_Time\nScantime\nOnsettime\nDiffHour\n\n\n\n\n2014-03-06\n1899-12-31 08:03:00\n1904-02-08 10:00:00\n2014-03-06 10:00:00\n2014-03-06 08:03:00\n1.95\n\n\n2013-06-09\n1899-12-31 22:00:00\n1899-12-31 02:00:00\n2013-06-10 02:00:00\n2013-06-09 22:00:00\n4\n\n\n\n\n\n\n\nOne way of storing data in R format is to save the file as .Rda. This format will ensure that no one can accidentally rewrite or delete a number. For very large data, it’s quicker to save as .Rda file than as csv file.\n\n\n\n2.2.3 Foreign data\nThe foreign library is traditionally use to handle data from SPSS (.sav), Stata (.dta) and SAS (.sas). One should look at the ending in the file to determine the necessary library.\n\nlibrary(foreign)\n#write and read Stata\nwrite.dta(dfdate,file=\"./Data-Use/dfdate_temp.dta\")\na&lt;-read.dta(\"./Data-Use/dfdate_temp.dta\")\na\n\nThe foreign library can handle older SAS files but not the current version. The current version of SAS file requires sas7bdat. The clue is the file ends in .sas7bdat.\n\n\n2.2.4 json format\nJson is short for JavaScript object Notification. These files have a hierarchical structured format. The json file is in text format amd can also be examined using Notepad. These files can be read using the RJSONIO or rjson libraries in R. Geojson is a json format with geographical data.\n\nlibrary(RJSONIO)\nj&lt;-fromJSON(\"./Data-Use/0411.geojson\") #Christionso Island\nj&lt;-lapply(j, function(x) {\n  x[sapply(x,is.null)]&lt;-NA\n  unlist(x)\n})\nk&lt;-as.data.frame(do.call(\"cbind\",j)) #list to data frame\nhead(k)\n\n                                   type       crs\ntype                  FeatureCollection      name\ngeometry.type         FeatureCollection EPSG:4326\ngeometry.coordinates1 FeatureCollection      name\ngeometry.coordinates2 FeatureCollection EPSG:4326\nproperties.id         FeatureCollection      name\nproperties.status     FeatureCollection EPSG:4326\n                                                  features\ntype                                               Feature\ngeometry.type                                        Point\ngeometry.coordinates1                          15.18657358\ngeometry.coordinates2                           55.3200168\nproperties.id         651a5746-735a-4312-b236-3a008e173de9\nproperties.status                                        1\n\n\nThe geojson file can be converted to sf using geojson_sf function from geojsonsf library.\n\nlibrary(geojsonsf)\n\n#Christianso Island\ngeojson_sf(\"./Data-Use/0411.geojson\")  %&gt;% mapview::mapview()\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#tidy-data",
    "href": "data-wrangling.html#tidy-data",
    "title": "2  Data Wrangling",
    "section": "2.3 Tidy data",
    "text": "2.3 Tidy data\nAttention to collection of data is important as it shows the way for performing analysis. In general each row represents on variable and each column represents an attribute of that variables. Sometimes there is a temptation to embed 2 types of attributes into a column.\n\ndf2&lt;-data.frame(Sex=c(\"Male\",\"Female\"), Test=c(\"positive 5 negative 5\",\n                        \" negative 0 negative 10\"))\ndf2\n\n     Sex                    Test\n1   Male   positive 5 negative 5\n2 Female  negative 0 negative 10\n\n\nThe above example should be entered this way. This change allows one to group variables by Test status: ‘positive’ or ‘negative’. One can easily perform a t-test here (not recommend in this case as the data contains only 2 rows).\n\ndf2&lt;-data.frame(Sex=c(\"Male\",\"Female\"), `Test Positive` =c(5,0), `Test Negative`=c(5, 10))\ndf2\n\n     Sex Test.Positive Test.Negative\n1   Male             5             5\n2 Female             0            10\n\n\nThe below example is illustrate how to collapse columns when using base R.\n\ndfa&lt;-data.frame(City=c(\"Melbourne\",\"Sydney\",\"Adelaide\"),\n                State=c(\"Victoria\",\"NSW\",\"South Australia\"))\n#collapsing City and State columns and generate new column address\ndfa$addresses&lt;-paste0(dfa$City,\",\", dfa$State) #separate by comma\ndfa$addresses2&lt;-paste0(dfa$City,\",\", dfa$State,\", Australia\")\ndfa\n\n       City           State                addresses\n1 Melbourne        Victoria       Melbourne,Victoria\n2    Sydney             NSW               Sydney,NSW\n3  Adelaide South Australia Adelaide,South Australia\n                           addresses2\n1       Melbourne,Victoria, Australia\n2               Sydney,NSW, Australia\n3 Adelaide,South Australia, Australia\n\n\nThis example is same as above but uses verbs from tidyr. This is useful for collapsing address for geocoding.\n\nlibrary(tidyr)\ndfa1&lt;-dfa %&gt;% unite (\"new_address\",City:State,sep = \",\")\ndfa1\n\n               new_address                addresses\n1       Melbourne,Victoria       Melbourne,Victoria\n2               Sydney,NSW               Sydney,NSW\n3 Adelaide,South Australia Adelaide,South Australia\n                           addresses2\n1       Melbourne,Victoria, Australia\n2               Sydney,NSW, Australia\n3 Adelaide,South Australia, Australia\n\n\nUsing the data above, let’s split the column address\n\nlibrary(tidyr)\ndfa2&lt;-dfa1 %&gt;% separate(addresses, c(\"City2\", \"State2\"))\n\nWarning: Expected 2 pieces. Additional pieces discarded in 1 rows [3].\n\ndfa2\n\n               new_address     City2   State2\n1       Melbourne,Victoria Melbourne Victoria\n2               Sydney,NSW    Sydney      NSW\n3 Adelaide,South Australia  Adelaide    South\n                           addresses2\n1       Melbourne,Victoria, Australia\n2               Sydney,NSW, Australia\n3 Adelaide,South Australia, Australia\n\n\n\n2.3.1 Factors\nThere are several types of factors in R: ordered and not ordered. It is important to pay attention to how factors are coded. Sometimes, male is represented as 1 and female as 0. Sometimes, female is represented as 2 as integer encoding. Integer encoding assumes a rank ordering. This discussion may seems trivial but several papers have been retracted in high impact factor journal Jama because of miscoding of the trial assignment 1 and 2 rather than the assignment of 0 and 1. This error led to reversing the results with logistic regression when 2 is exchanged for 0 (Aboumatar and Wise 2019). This error led to report that an outpatient management program for chronic obstructive pulmonary disease resulted in fewer admissions. Below is an example which can occur when data is transformed into factor and back to number. Note that the coding goes from 0 and 1 to 2 and 1. It has been suggested that the move away from coding data as 1 and 0 was historical and due to the fear that coding with punch card would treat 0 as a missing number and hence the move to coding binary variable as 1 and 2.\nIn certain analyses, the libraries prefer to use the dependent or outcome variable as binary coding in numeric format such as 1 and 0 eg logistic regression and random forest. The library e1071 for performing support vector machine prefers the outcome variable as factor.\n\nlibrary(Stat2Data)\ndata(\"Leukemia\") #treatment of leukemia\n\nLeukemia %&gt;% dplyr::glimpse()\n\nRows: 51\nColumns: 9\n$ Age    &lt;int&gt; 20, 25, 26, 26, 27, 27, 28, 28, 31, 33, 33, 33, 34, 36, 37, 40,…\n$ Smear  &lt;int&gt; 78, 64, 61, 64, 95, 80, 88, 70, 72, 58, 92, 42, 26, 55, 71, 91,…\n$ Infil  &lt;int&gt; 39, 61, 55, 64, 95, 64, 88, 70, 72, 58, 92, 38, 26, 55, 71, 91,…\n$ Index  &lt;int&gt; 7, 16, 12, 16, 6, 8, 20, 14, 5, 7, 5, 12, 7, 14, 15, 9, 12, 4, …\n$ Blasts &lt;dbl&gt; 0.6, 35.0, 7.5, 21.0, 7.5, 0.6, 4.8, 10.0, 2.3, 5.7, 2.6, 2.5, …\n$ Temp   &lt;int&gt; 990, 1030, 982, 1000, 980, 1010, 986, 1010, 988, 986, 980, 984,…\n$ Resp   &lt;int&gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, …\n$ Time   &lt;int&gt; 18, 31, 31, 31, 36, 1, 9, 39, 20, 4, 45, 36, 12, 8, 1, 15, 24, …\n$ Status &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n\n\nThe variable Resp is now a factor with levels 0 and 1\n\nLeukemia$Resp\n\n [1] 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n[39] 0 1 0 0 0 1 0 0 1 0 1 1 0\n\nLeukemia$Response.factor&lt;-as.factor(Leukemia$Resp)\nhead(Leukemia$Response.factor)\n\n[1] 1 1 1 1 1 0\nLevels: 0 1\n\n\nNote in the conversion back to numeric ‘dummy’ values, the data takes the form 1 and 2. This has changed the dummy values of 0 and 1. It is important to examine the data before running analysis.\n\nLeukemia$Response.numeric&lt;-as.numeric(Leukemia$Response.factor)\nLeukemia$Response.numeric\n\n [1] 2 2 2 2 2 1 2 2 2 1 2 2 1 2 1 2 2 1 2 2 1 1 1 1 2 2 1 1 2 1 2 1 1 1 1 1 1 1\n[39] 1 2 1 1 1 2 1 1 2 1 2 2 1\n\n\nFor variables which are characters but considered as factors, it is necessary to convert to class character before converting to dummy values.\n\ndata(\"BreastCancer\", package=\"mlbench\")\nBreastCancer %&gt;% glimpse()\n\nRows: 699\nColumns: 11\n$ Id              &lt;chr&gt; \"1000025\", \"1002945\", \"1015425\", \"1016277\", \"1017023\",…\n$ Cl.thickness    &lt;ord&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4, …\n$ Cell.size       &lt;ord&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1,…\n$ Cell.shape      &lt;ord&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1,…\n$ Marg.adhesion   &lt;ord&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1,…\n$ Epith.c.size    &lt;ord&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2, …\n$ Bare.nuclei     &lt;fct&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1, …\n$ Bl.cromatin     &lt;fct&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3, …\n$ Normal.nucleoli &lt;fct&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1, …\n$ Mitoses         &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1, …\n$ Class           &lt;fct&gt; benign, benign, benign, benign, benign, malignant, ben…\n\n\nThe steps for conversion are illustrated below. Conversion of multiple columns of factors and ordered factors can be done in one step using lapply function. This will be described much further below.\n\nBreastCancer$Class&lt;-as.character(BreastCancer$Class)\nBreastCancer$Class[BreastCancer$Class==\"benign\"]&lt;-0\nBreastCancer$Class[BreastCancer$Class==\"malignant\"]&lt;-1\nBreastCancer$Class&lt;-as.numeric(BreastCancer$Class)\nhead(BreastCancer$Class)\n\n[1] 0 0 0 0 0 1\n\n\nThis illustration describes conversion of a continuous variable into orderly factors.\n\nlibrary(Stat2Data)\ndata(\"Leukemia\") #treatment of leukemia\n#partition Age into 8 ordered factors\nLeukemia$AgeCat&lt;-ggplot2::cut_interval(Leukemia$Age, n=8, ordered_result=TRUE)\nclass(Leukemia$AgeCat)\n\n[1] \"ordered\" \"factor\" \n\n\n\n\n2.3.2 Multiple files\nMerging of files can be done using dplyr to perform inner_join, outer_join, left_join and right_join. Note that this can also be done in base R or using syntax of data.table. These files can be joined using %&gt;% operator.\n\nDF1&lt;-data.frame(ID=c(1,2,3),Age=c(20,30,40))\nDF2&lt;-data.frame(ID=c(1,2,3),Sex=c(1,0,0))\nDF3&lt;-data.frame(ID=c(1,2,3), Diabetes=c(1,1,0))\n\nDF &lt;-DF1 %&gt;% left_join(DF2, by=\"ID\") %&gt;% left_join(DF3, by=\"ID\")\n\n\n\n2.3.3 Tidy evaluation\nData in a dataframe can be summarised with the help of group_by function in dplyr. The summarise function returns 1 row of data per group. The number of observations in each group can be counted using n() function.\n\n# spot sign dataset\nss&lt;-read.csv(\"./Data-Use/ss150718.csv\")\n\nss %&gt;% group_by(Country) %&gt;% summarise (Number=n())\n\n# A tibble: 11 × 2\n   Country  Number\n   &lt;chr&gt;     &lt;int&gt;\n 1 Brazil        1\n 2 Canada        3\n 3 China         6\n 4 China/US      1\n 5 Denmark       1\n 6 Germany       1\n 7 Japan         5\n 8 Korea         4\n 9 Multiple      5\n10 Spain         1\n11 US           11\n\n\nWe can add more arguments to the group_by function\n\nss %&gt;% group_by(Country, Study.type) %&gt;% summarise (Number=n())\n\n`summarise()` has grouped output by 'Country'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 16 × 3\n# Groups:   Country [11]\n   Country  Study.type  Number\n   &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt;\n 1 Brazil   Prospective      1\n 2 Canada   Prospective      1\n 3 Canada   Retro            2\n 4 China    Prospective      3\n 5 China    Retro            3\n 6 China/US Retro            1\n 7 Denmark  Prospective      1\n 8 Germany  Prospective      1\n 9 Japan    Prospective      1\n10 Japan    Retro            4\n11 Korea    Prospective      1\n12 Korea    Retro            3\n13 Multiple Prospective      5\n14 Spain    Prospective      1\n15 US       Prospective      4\n16 US       Retro            7\n\n\nWe can also apply a function across multiple columns with across. Here we use reframe argument to unlock restriction impose by group_by. Here we use tibble function to return several output columns.\n\nDR&lt;- function (w,x,y,z){\n  tibble(\n   Sensitivity=round(w/(w+y),2),\n   Specificity=round(z/(z+x),2))\n}\n\nss %&gt;% group_by(Country, Study.type) %&gt;% \n  #the output is limted to 6 rows by head argument\n  reframe (across(TP:TN), DR(TP,FP,FN,TN)) %&gt;% head() \n\n# A tibble: 6 × 8\n  Country Study.type     TP    FP    FN    TN Sensitivity Specificity\n  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 Brazil  Prospective     5     6     3    57        0.62        0.9 \n2 Canada  Prospective    10     3     1    25        0.91        0.89\n3 Canada  Retro          49    22     6   290        0.89        0.93\n4 Canada  Retro          11     0     4    12        0.73        1   \n5 China   Prospective    60    16    17   219        0.78        0.93\n6 China   Prospective    17     7    11    77        0.61        0.92\n\n\n\n\n2.3.4 Pivot wide and long\nA variety of different expressions are used to describe data format such as wide and long formats. In some case the distinction between such formats is not clear. The verbs for performing these operations are pivot_wide, pivot_long. Again data.table uses different verbs such as cast and melt. In general, most regression analyses are performed with data in wide format. In this case each row represents a unique ID. Longitudinal analyses are performed with data in long format. In this format, there are several rows with the same ID. In the next Chapter on Statistics, an example of data generated in wide format and coverted to long format using plyr. Here we will demonstrate the use of tidyr to pivot loner or wider.\nThe best way to think about how data should be presented is that data is analyzed according to columns not rows. The data below is extracted from CDC COVID website. Details are given below under Web scraping on how this task was performed.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nusa&lt;-read.csv(\"./Data-Use/Covid_bystate_Table130420.csv\")\n# for demonstration we will select 3 columns of interest\nusa_long &lt;-usa %&gt;% \n  select(Jurisdiction,NumberCases31.03.20,NumberCases07.04.20) %&gt;% pivot_longer(-Jurisdiction,names_to = \"Date\",values_to = \"Number.Cases\")  \nusa_long$Date &lt;- str_replace(usa_long$Date,\"NumberCases\",\"\")\n\n#data in wide format\nhead(usa %&gt;%select(Jurisdiction,NumberCases31.03.20,NumberCases07.04.20),6) \n\n  Jurisdiction NumberCases31.03.20 NumberCases07.04.20\n1      Alabama                 999                2197\n2       Alaska                 133                 213\n3      Arizona                1289                2575\n4     Arkansas                 560                 993\n5   California                8131               15865\n6     Colorado                2966                5429\n\n#data in long format\nhead(usa_long,6) \n\n# A tibble: 6 × 3\n  Jurisdiction Date     Number.Cases\n  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;\n1 Alabama      31.03.20          999\n2 Alabama      07.04.20         2197\n3 Alaska       31.03.20          133\n4 Alaska       07.04.20          213\n5 Arizona      31.03.20         1289\n6 Arizona      07.04.20         2575",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#regular-expressions",
    "href": "data-wrangling.html#regular-expressions",
    "title": "2  Data Wrangling",
    "section": "2.4 Regular Expressions",
    "text": "2.4 Regular Expressions\nHere is a short tutorial on regular expression. We will begin using base R. This section is based on experience trying to clean a data frame containing many words used to describe one disease or one drug.\n\n2.4.1 base R\n\n#create example dataframe\ndf&lt;-data.frame(\n  drug=c(\"valium 1mg\",\"verapamil sr\",\"betaloc zoc\",\"tramadol\",\"valium (diazepam)\"),\n  infection=c(\"pneumonia\",\"aspiration pneumonia\",\"tracheobronchitis\",\"respiratory tract infection\",\"respiratory.tract.infection\"))\ndf\n\n               drug                   infection\n1        valium 1mg                   pneumonia\n2      verapamil sr        aspiration pneumonia\n3       betaloc zoc           tracheobronchitis\n4          tramadol respiratory tract infection\n5 valium (diazepam) respiratory.tract.infection\n\n\nNow that we have a data frame, we can use pattern matching to replace part of phrase. This step can be done simply using gsub command. First create a list so that the computer searches the phrases in the list.\n\n#create list to remove phrase\nredun=c(\"1mg\", \"zoc\", \"sr\")\npat=paste0(\"\\\\b(\",paste0(redun,collapse = \"|\"),\")\\\\b\")\ndf$drug1&lt;-gsub(pat,\"\",df$drug)\ndf$drug1\n\n[1] \"valium \"           \"verapamil \"        \"betaloc \"         \n[4] \"tramadol\"          \"valium (diazepam)\"\n\n#create list to remove phrase\nredunc1=c(\"respiratory tract infection\", \"tracheobronchitis\", \"aspiration\")\npat=paste0(\"\\\\b(\",paste0(redunc1,collapse = \"|\"),\")\\\\b\")\ndf$infection1&lt;-gsub(pat,\"\",df$infection)\ndf$infection1\n\n[1] \"pneumonia\"                   \" pneumonia\"                 \n[3] \"\"                            \"\"                           \n[5] \"respiratory.tract.infection\"\n\n\nThis section deals with meta-characterers. Examples of meta-characters include $ . + * ? ^ () {} []. These meta-characters requires the double back slashes \\.\n\n#create list to remove phrase\nredun=c(\"1mg\", \"zoc\", \"sr\")\npat=paste0(\"\\\\b(\",paste0(redun, collapse = \"|\"),\")\\\\b\")   \ndf$drug2&lt;-gsub(pat,\"\",df$drug)\n\n#[a-z] indicates any letter\n#[a-z]+ indicates any letter and those that follow the intial letter\ndf$drug2&lt;-gsub(\"\\\\(|[a-z]+\\\\)\",\"\",df$drug2)\ndf$drug2\n\n[1] \"valium \"    \"verapamil \" \"betaloc \"   \"tramadol\"   \"valium \"   \n\n\nBack to our data frame df, we want to remove or change the different words accounting for pneumonia.\n\nredunc=c(\"\\\\.\")\nredunc1=c(\"respiratory tract infection\", \"tracheobronchitis\", \"aspiration\")\npat=paste0(\"\\\\b(\",paste0(redunc,collapse = \"|\"),\")\\\\b\")\ndf$infection2&lt;-gsub(pat,\" \",df$infection)\npat=paste0(\"\\\\b(\",paste0(redunc1,collapse = \"|\"),\")\\\\b\")\ndf$infection2&lt;-gsub(pat,\" \",df$infection2)\ndf$infection2\n\n[1] \"pneumonia\"   \"  pneumonia\" \" \"           \" \"           \" \"          \n\n\n\n\n2.4.2 stringr\nThe following examples are taken from excel after conversion from pdf. In the process of conversion errors were introduced in the conversion from pdf to excel. A full list of the options available can be found at https://stringr.tidyverse.org/articles/regular-expressions.html\n\nlibrary(stringr)\n#error introduced by double space \na&lt;-c(\"8396 (7890 to 8920)\",\"6 301 113(6 085 757 to 6 517 308)\",\n     \"4 841 208 (4 533 619 to 5 141 654)\",\n     \"1 407 701 (127 445 922 to 138 273 863)\",\n     \"4 841 208\\n(4 533 619 to\\n5 141 654)\")\n\nb&lt;-str_replace (a, \"\\\\(c.*\\\\)\",\"\")\n\n#this is a complex example to clean and requires several steps. Note that the original data in the list a is now assigned to b. \nb&lt;-str_replace(a,\"\\n\",\"\") %&gt;% \n  #remove (\n  str_replace(\"\\\\(.*\",\"\") %&gt;%\n  str_replace(\"\\n.*\",\"\") %&gt;%\n  #remove )\n  str_replace(\"\\\\)\",\"\") %&gt;%\n  #remove empty space\n  str_replace(\"\\\\s\",\"\") %&gt;%\n  str_replace(\"\\\\s\",\"\")%&gt;% as.numeric()\nb\n\n[1]    8396 6301113 4841208 1407701 4841208\n\n\nAnother example. This time the 2 numbers in the column are separated by a slash sign. Supposed you want to keep the denominator. The first remove the number before the slash sign. The _*_ metacharacter denotes the action occurs at the end.\n\ndf.d&lt;-data.frame(seizure.rate=c(\"59/90\", \"90/100\", \"3/23\"))\ndf.d$seizure.number&lt;-str_replace(df.d$seizure.rate,\"[0-9]*\",\"\") \ndf.d$seizure.number\n\n[1] \"/90\"  \"/100\" \"/23\" \n\n\nNow combine with the next step to remove the slash sign.\n\n#We used [0-9] to denote any number from 0 to 9. For text, one can use [A-Z].\ndf.d$seizure.number&lt;-str_replace(df.d$seizure.rate,\"^[0-9]*\",\"\")%&gt;%\n  str_replace(\"/\",\"\\\\\")\ndf.d$seizure.number\n\n[1] \"90\"  \"100\" \"23\" \n\n\nRemoving the denominator requires a different approach. First remove the last number then the slash sign.\n\ndf.d$case&lt;-str_replace(df.d$seizure.rate,\"/[0-9]*\",\" \")\ndf.d$case\n\n[1] \"59 \" \"90 \" \"3 \" \n\n\nThe example below has several words mixed in numeric vector columns. The words are a mixture of upper and lower cases. Note that “NA” is treated as a word character while NA is treated as Not Available by R. This recognition is important as removing them requires different actions. Character “NA” can be removed by str_replace while NA requires is.na operator.\n\nA&lt;-c(1,2,3,\"NA\",4,\"no COW now\")\nB&lt;-c(1,2,NA,4,\"NA\",\"check\")\nC&lt;-c(1,\"not now\",2,3, NA ,5)\n\nD&lt;-data.frame(A,B,C) \n\n#str_replace on one column\nD$A1&lt;-str_replace(D$A,\"[A-Z]+\",\"\") %&gt;% str_replace(\"[a-z]+\",\"\")\n\n#change to lower case\nD$A2&lt;-str_to_lower(D$A) %&gt;% str_replace(\"[a-z]+\",\"\")\n\n#remove space before replacement\nD$A3&lt;-str_to_lower(D$A) %&gt;% str_replace(\"\\\\s+\",\"\") %&gt;% str_replace(\"[a-z]+\",\"\")\n\n#note that this action does not remove the third word\nD$A4&lt;-str_to_lower(D$A) %&gt;% str_replace(\"\\\\s\",\"\") %&gt;% str_replace(\"[a-z]+\",\"\")\n\n#repeat removal of empty space\nD$A5&lt;-str_to_lower(D$A) %&gt;% str_replace(\"\\\\s\",\"\") %&gt;% \n  str_replace(\"\\\\s\",\"\") %&gt;%  str_replace(\"[a-z]+\",\"\")\n\n#apply str_replace_all rather than repeat\nD$A6&lt;-str_to_lower(D$A) %&gt;% str_replace_all(\"\\\\s\",\"\") %&gt;% \n    str_replace(\"[a-z]+\",\"\")\n\n#now combine into vector. Note the use of c to combine the vector and replace \n#the comma with equal sign\nD$A7&lt;-str_to_lower(D$A) %&gt;%\n str_replace_all(c(\"\\\\s\"=\"\",\"[a-z]+\"=\"\"))\n\nD\n\n           A     B       C    A1       A2   A3   A4 A5 A6 A7\n1          1     1       1     1        1    1    1  1  1  1\n2          2     2 not now     2        2    2    2  2  2  2\n3          3  &lt;NA&gt;       2     3        3    3    3  3  3  3\n4         NA     4       3                                  \n5          4    NA    &lt;NA&gt;     4        4    4    4  4  4  4\n6 no COW now check       5   now  cow now  now  now         \n\n\nThe lessons from above can be combine in when creating data frame. The mutate_if function enable multiple columns to be changed. One problem to handle adding multiple columns which contain NA is the use of rowSums and dplyr::select. These examples are illustrated below.\n\n#use the mutate function \n\nE&lt;-data.frame(A,B,C) %&gt;%\n  mutate (A=str_to_lower(A) %&gt;% str_replace_all(c(\"\\\\s\"=\"\",\"[a-z]+\"=\"\")),\n          B=str_to_lower(B) %&gt;%str_replace_all(c(\"\\\\s\"=\"\",\"[a-z]+\"=\"\")),\n          C=str_to_lower(C) %&gt;%str_replace_all(c(\"\\\\s\"=\"\",\"[a-z]+\"=\"\")))%&gt;%\n  \n  #change character columns to numeric\n  mutate_if(is.character, as.numeric)%&gt;%\n  \n  #add across columns and avoid NA\n  mutate(ABC=rowSums(dplyr::select(.,A:C),na.rm = T))\n\nAnother example of NA creating issues with sum in mutate is provided below. Here, the function rowwise from dplyr can be used to emphasise the operation across rows.\n\ndf&lt;-data.frame(Mon_SBP=c(160,NA,180),Tues_SBP=c(0,0,150),\n               Wed_SBP=c(NA,130,125)) %&gt;%\n  #error from NA\n  rowwise %&gt;%\n  mutate(SBP=ifelse(sum(Mon_SBP&gt;140 & Tues_SBP&gt;120, \n                        Tues_SBP&gt;100 & Wed_SBP&gt;120, \n                        Mon_SBP&gt;100 & Wed_SBP&gt;115, na.rm=T)&gt;1.5,1,0))\n\n\ndf\n\n# A tibble: 3 × 4\n# Rowwise: \n  Mon_SBP Tues_SBP Wed_SBP   SBP\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     160        0      NA     0\n2      NA        0     130     0\n3     180      150     125     1\n\n\nSometimes, you may only want to keep the number and ignore the words in the column. This can be done using the str_extract function.\n\ndf.e&lt;-data.frame(disability=c(\"1 - No significant disability despite symptoms; able to carry out all usual duties and activities\",\"5 - Severe disability, bedridden, incontinent and requiring constant nursing care and attention\",\"\n1 - No significant disability despite symptoms; able to carry out all usual \nduties and activities\"))\ndf.e$disability2&lt;-str_extract(df.e$disability,\"\\\\w\") #extract number",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#pdf-to-xcel",
    "href": "data-wrangling.html#pdf-to-xcel",
    "title": "2  Data Wrangling",
    "section": "2.5 PDF to xcel",
    "text": "2.5 PDF to xcel\nSometimes data from public government sites come in PDF form instead of excel. Conversion from pdf to excel or text can be difficult especially with special character eg Danish. There are several libraries for doing this: pdftables (require API key) and pdftools. The example below uses pdftools. available at https://docs.ropensci.org/pdftools/. The document is the 2018 Danish Stroke Registry report. The tabulizer package is excellent for converting table data. However, tabulizer package depends on rJava and requires deft handling. The PDE libray has user interface for performing data extraction from pdf.\n\nlibrary(pdftools)\n\nUsing poppler version 22.04.0\n\n#Danish stroke registry\ntxt&lt;-pdf_text(\"./Data-Use/4669_dap_aarsrapport-2018_24062019final.pdf\")\ncat(txt[17]) #browse data page 13+4 filler pages\n\n3. Indikatorresultater på lands-, regions- og afdelingsniveau\nIndikator 1a: Andel af patienter med akut apopleksi som indlægges inden for 3 timer\nefter symptomdebut. Standard: ≥ 30%\n\nIndikator 1b: Andel af patienter med akut apopleksi som indlægges inden for 4,5\ntimer efter symptomdebut. Standard: ≥ 40%\n\n                                        Inden for 3 timer\n\n                                                      Uoplyst    Aktuelle år            Tidligere år\n                            Standard     Tæller/       antal         2018          2017            2016\n                             opfyldt     nævner        (%)      %     95% CI     % (95% CI)     % (95% CI)\n\n Danmark                       ja      4730 / 11794    49 (0)   40   (39 - 41)   39 (38-40)     37 (36-38)\n Hovedstaden                   ja       1502 / 3439    49 (1)   44   (42 - 45)   40 (38-42)     40 (39-42)\n Sjælland                      ja        760 / 1917     0 (0)   40   (37 - 42)   39 (36-41)     40 (38-43)\n Syddanmark                    ja        942 / 2433     0 (0)   39   (37 - 41)   39 (37-41)     35 (33-37)\n Midtjylland                   ja        918 / 2590     0 (0)   35   (34 - 37)   36 (34-38)     35 (33-37)\n Nordjylland                   ja        577 / 1341     0 (0)   43   (40 - 46)   41 (39-44)     35 (32-37)\n Bopæl uden for Danmark        ja           31 / 74     0 (0)   42   (31 - 54)   51 (36-66)     39 (26-53)\n\n Hovedstaden                   ja       1502 / 3439    49 (1)   44   (42 - 45)   40 (38-42)     40 (39-42)\n Albertslund                   ja           17 / 52     0 (0)   33   (20 - 47)   30 (18-44)     43 (27-59)\n Allerød                       ja           22 / 53     0 (0)   42   (28 - 56)   32 (20-46)     35 (22-50)\n Ballerup                      ja          43 / 106     0 (0)   41   (31 - 51)   48 (38-58)     40 (31-51)\n Bornholms Regionskommune      ja           38 / 98     0 (0)   39   (29 - 49)   28 (19-38)     32 (22-43)\n Brøndby                       ja          45 / 113     0 (0)   40   (31 - 49)   31 (22-42)     34 (23-47)\n Dragør                        ja           18 / 43     0 (0)   42   (27 - 58)   47 (30-65)     33 (17-54)\n Egedal                        ja           45 / 95     0 (0)   47   (37 - 58)   40 (30-52)     48 (37-59)\n Fredensborg                   ja           36 / 99     0 (0)   36   (27 - 47)   37 (27-47)     43 (33-53)\n Frederiksberg                 ja          74 / 141    13 (8)   52   (44 - 61)   42 (35-50)     52 (44-61)\n Frederikssund                 ja          53 / 141     0 (0)   38   (30 - 46)   39 (31-48)     42 (33-51)\n Furesø                        ja          55 / 110     0 (0)   50   (40 - 60)   56 (44-67)     50 (38-62)\n Gentofte                      ja          65 / 123     1 (1)   53   (44 - 62)   46 (37-56)     38 (30-47)\n Gladsaxe                      ja          68 / 129     0 (0)   53   (44 - 62)   48 (38-57)     36 (27-44)\n Glostrup                      ja           38 / 72     0 (0)   53   (41 - 65)   40 (27-54)     48 (33-63)\n Gribskov                      ja          46 / 129     0 (0)   36   (27 - 45)   35 (26-44)     36 (28-46)\n Halsnæs                       ja           45 / 92     0 (0)   49   (38 - 60)   34 (25-45)     34 (25-44)\n Helsingør                     ja          50 / 129     0 (0)   39   (30 - 48)   32 (24-40)     38 (30-45)\n Herlev                        ja           25 / 50     0 (0)   50   (36 - 64)   52 (38-65)     33 (22-46)\n Hillerød                      ja          47 / 121     0 (0)   39   (30 - 48)   39 (30-49)     40 (30-50)\n Hvidovre                      ja          57 / 135     0 (0)   42   (34 - 51)   38 (29-48)     47 (37-57)\n\n                                                                                                             13\n\nscreenshot13&lt;-\n  pdf_render_page(\"./Data-Use/4669_dap_aarsrapport-2018_24062019final.pdf\", \n                  page =17)\npng::writePNG(screenshot13, \"./Data-Use/Danish-Stroke-page13.png\")\n\nknitr::include_graphics(\"./Data-Use/Danish-Stroke-page13.png\")\n\n\n\n\n\n\n\n\n\n2.5.1 Scanned text or picture\nImporting data from scanned text will require use of Optical Character Recognition (OCR). The tesseract library provides an R interface for OCR. In the example below, a picture is taken from same CDC website containing mortality data (https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/04102020/ nchs-data.html). The screenshot of this website was then cleaned in paint. The data is available in the Data-Use folder.\n\nlibrary(tesseract)\neng &lt;- tesseract(\"eng\") #english\ntext &lt;- tesseract::ocr(\"./Data-Use/Covid_PNG100420.png\", engine = eng)\ncat(text)\n\nNCHS Mortality Surveillance Data\nData as of April 9, 2020\nFor the Week Ending April 4, 2020 (Week 14)\n\nCOVID-19 Deaths Pneumonia Deaths* Influenza Deaths\nYear Week TotalDeaths Number %ofTotal Number %ofTotal Number %of Total\n2019 40 52,452 0 0 2,703 5.15 16 0.03\n2019 4l 52,860 0 0 2,770 5.24 16 0.03\n2019 42 54,129 0 0 2,977 5.50 18 0.03\n2019 43 53,914 0 0 2,985 5.54 30 0.06\n2019 44 53,980 0 0 2,908 5.39 31 0.06\n2019 4S 55,468 0 0 3,063 5.52 31 0.06\n2019 46 55,684 0 0 3,096 5.56 39 0.07\n2019 47 55,986 0 0 2,993 5.35 50 0.09\n2019 48 55,238 0 0 2,976 5.38 65 0.12\n2019 49 56,990 0 0 3,305 5.80 99 0.17\n2019 50 57,276 0 0 3,448 6.02 111 0.19\n2019 51 56,999 0 0 3,345 5.87 125 0.22\n2019 52 57,956 0 0 3,478 5.99 198 0.34\n2020 4 58,961 0 0 3,998 6.77 416 0.71\n2020 2 58,962 0 0 3,995 6.76 450 0.76\n2020 3 57,371 0 0 3,903 6.78 44) 0.77\n2020 4 56,666 0 0 3,742 6.56 468 0.83\n2020 5 56,381 0 0 3,617 6.42 452 0.80\n2020 6 56,713 0 0 3,599 6.35 482 0.85\n2020 7 55,237 0 0 3,577 6.48 487 0.88",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#web-scraping",
    "href": "data-wrangling.html#web-scraping",
    "title": "2  Data Wrangling",
    "section": "2.6 Web scraping",
    "text": "2.6 Web scraping\nThe readers may ask why web scraping for healthcare. A pertinent example related to COVID-19 data is provided below. The library rvest is helpful at scraping data from an internet page. The rvest library assumes that web contents have xml document-tree representation. The different options available for web scraping with rvest are available at the website https://rvest.tidyverse.org/reference/. The user can use CSS selectors to scrape content. The library Rselenium is also useful for web scraping. For dynamic web page, the library CasperJS library does a better job especially if the data contain embedded java script.\nThe library cdccovidview provides access to the CDC website on COVID-19. In the example below, we will try to this manually. Data from CDC website on COVID-19 is downloaded, cleaned and saved in csv format. It is important to pay attention to the data. The first row contains header and is removed. There are several columns with commas. These commas can be removed using the exercises above. Further the data is updated on weekly basis. As such the data needs to be converted into a date time format using lubridate.\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(tidyverse)\n#assign handle to web page accessed 12/4/20\n#cdc&lt;-read_html(\"https://www.cdc.gov/coronavirus/2019-ncov/covid-data/\n#               covidview/04102020/nchs-data.html\")\n# scrape all div tags\n#html_tag &lt;- cdc %&gt;% html_nodes(\"div\")\n# scrape header h1 tags\n#html_list&lt;-html_tag %&gt;% html_nodes(\"h1\") %&gt;% html_text()\n#there is only one table on this web page\n#Table1&lt;- cdc %&gt;% html_node(\"table\") %&gt;% html_table(fill = TRUE)\n#Table1 has a header row\n#Table1&lt;-Table1[-1,]\n#The data in the Total Deaths column has a comma \n#Table1$Total.Deaths&lt;-as.numeric(gsub(\",\",\"\",Table1$`Total Deaths`))\n#now combine the year and week column to Date\n#Table1$Date&lt;-lubridate::parse_date_time(paste(Table1$Year, Table1$Week, 'Mon', sep=\"/\"),'Y/W/a')\n#there are still commas remaining in some columns. This is a useful exercise for the reader. A solution is provided in the next example.  \n#write.csv(Table1,file=\"./Data-Use/Covid_Table100420.csv\")\n\nThe next example is from the CDC COVID-19 website. It poses a different challenges as there are several columns with the same names. In this case we will rename the column by index. There are several columns containing commas. Rather than removing column by column we will write a function with lapply to do it over the table. the apply function returns a matrix whereas lapply returns a dataframe. There is one column containing percentage enclosed in a bracket. This can be removed using the example above on metacharacter ie using doule back slash in front of bracket and again at close of bracket.\n\nlibrary(rvest)\nlibrary(tidyverse)\ncdc&lt;-\nread_html(\"https://www.cdc.gov/mmwr/volumes/69/wr/mm6915e4.htm?s_cid=mm6915e4_w\")\n\n# scrape all div tags\nhtml_tag &lt;- cdc %&gt;% html_nodes(\"div\")\n# scrape header h1 tags\nhtml_list&lt;-html_tag %&gt;% html_nodes(\"h1\") %&gt;% html_text()\n#there is only one table on this web page\nTable2&lt;- cdc %&gt;% html_node(\"table\") %&gt;% html_table(fill = TRUE)\n#first row is header\nnames(Table2) &lt;- as.matrix(Table2[1, ])\nTable2&lt;-Table2[-c(1:2,55),]#rows 1 and 2 are redundant\n#rename the columns by index \nnames(Table2)[2] &lt;-\"NumberCases31.03.20\"\nnames(Table2)[3]&lt;-\"CumulativeIncidence31.03.20\"\nnames(Table2)[4]&lt;-\"NumberCases07.04.20\"\nnames(Table2)[5]&lt;-\"NumberDeath07.04.20\"\nnames(Table2)[6]&lt;-\"CumulativeIncidence07.04.20\"\n#rather than removing column by column we will write a function with lapply to remove commas over the table. the apply function returns a matrix whereas lapply returns a dataframe.\nTable2&lt;-as.data.frame(lapply(Table2, function(y) gsub(\",\", \"\", y))) \nTable2&lt;-as.data.frame(lapply(Table2, function(x)\n  gsub(\"\\\\(|[0-9]+\\\\)\",\"\",x)))\n#write.csv(Table2,file=\"./Data-Use/Covid_bystate_Table130420.csv\")\n\n\n\n\n\nAboumatar, H., and R. A. Wise. 2019. “Notice of Retraction. Aboumatar et al. Effect of a Program Combining Transitional Care and Long-term Self-management Support on Outcomes of Hospitalized Patients With Chronic Obstructive Pulmonary Disease: A Randomized Clinical Trial. JAMA. 2018;320(22):2335-2343.” JAMA 322 (14): 1417–18.\n\n\nWickham, Hadley. 2019. 2nd ed. Chapman & Hall/CRC the r Series. Chapman; Hall/CRC; 2 edition (May 30, 2019).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling-images.html",
    "href": "data-wrangling-images.html",
    "title": "3  Medical Images",
    "section": "",
    "text": "3.0.1 DICOM and nifti format\nR can handle a variety of different data format. Medical images are stored as DICOM files for handling and converted to nifti files for analysis. The workhorses are the oro.dicom and oro.nifti libraries. Nifti is an S4 class object with multiple slots for data type. These slots can be accessed by typing the @ after the handle of the file. The values in an image can be evaluated using range function. Alternately, use cal_max and cal_min to perform the same task. It appears that in the conversion from minc file to nifti file, a scaling factor has been applied and transformed the values.\nlibrary(oro.nifti)\n\noro.nifti 0.11.4\n\nmca&lt;-readNIfTI(\"./Data-Use/mca_notpa.nii.gz\", reorient = FALSE) \n\nrange(mca)\n\n[1] 0.000000000 0.001411765\nThe slots also contain information on whether the data has been scaled. This can be checked by accessing the scl_slope and scl_inter slots. These data on slope and intercept provide a mean of returning an image to its correct value.\nmca@scl_slope\n\n[1] 1\nTo find available slots\n#find available of slots\nslotNames(mca)\n\n [1] \".Data\"          \"sizeof_hdr\"     \"data_type\"      \"db_name\"       \n [5] \"extents\"        \"session_error\"  \"regular\"        \"dim_info\"      \n [9] \"dim_\"           \"intent_p1\"      \"intent_p2\"      \"intent_p3\"     \n[13] \"intent_code\"    \"datatype\"       \"bitpix\"         \"slice_start\"   \n[17] \"pixdim\"         \"vox_offset\"     \"scl_slope\"      \"scl_inter\"     \n[21] \"slice_end\"      \"slice_code\"     \"xyzt_units\"     \"cal_max\"       \n[25] \"cal_min\"        \"slice_duration\" \"toffset\"        \"glmax\"         \n[29] \"glmin\"          \"descrip\"        \"aux_file\"       \"qform_code\"    \n[33] \"sform_code\"     \"quatern_b\"      \"quatern_c\"      \"quatern_d\"     \n[37] \"qoffset_x\"      \"qoffset_y\"      \"qoffset_z\"      \"srow_x\"        \n[41] \"srow_y\"         \"srow_z\"         \"intent_name\"    \"magic\"         \n[45] \"extender\"       \"reoriented\"\nIn this example below we will simulated an image of dimensions 5 by 5 by 5. see [simulation using mand library][PCA with MRI].\nlibrary(oro.nifti)\nset.seed(1234)\ndims = rep(5, 3)\nSimArr = array(rnorm(5*5*5), dim = dims)\nSimIm = oro.nifti::nifti(SimArr)\nprint(SimIm)\n\nNIfTI-1 format\n  Type            : nifti\n  Data Type       : 2 (UINT8)\n  Bits per Pixel  : 8\n  Slice Code      : 0 (Unknown)\n  Intent Code     : 0 (None)\n  Qform Code      : 0 (Unknown)\n  Sform Code      : 0 (Unknown)\n  Dimension       : 5 x 5 x 5\n  Pixel Dimension : 1 x 1 x 1\n  Voxel Units     : Unknown\n  Time Units      : Unknown\nView the simulated image.\nneurobase::ortho2(SimIm)\nThis section provides a brief introduction to viewing nifti files. Data are stored as rows, columns and slices. To view sagital image then assign a number to the row data.\n#plot mca #sagittal\nimage(mca[50,,])\nTo see coronal image, assign a number to the column data.\n#plot mca #coronal\nimage(mca[,70,])\nTo see axial image, assign a number to the slice data.\n#plot mca #axial in third column\nimage(mca[,,35])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Medical Images</span>"
    ]
  },
  {
    "objectID": "data-wrangling-images.html#neurons-and-arteries",
    "href": "data-wrangling-images.html#neurons-and-arteries",
    "title": "3  Medical Images",
    "section": "3.1 Neurons and arteries",
    "text": "3.1 Neurons and arteries\nThe swc format is used for examining morphology of neurons (K. Mehta and Ascoli 2023) or arteries (Decroocq et al. 2023). The file represent neurons or arteries as connected components. Example is provided from the nat library.\n\n3.1.1 Neurons\n\nlibrary(nat)\n\nRegistered S3 method overwritten by 'nat':\n  method             from\n  as.mesh3d.ashape3d rgl \n\n\nSome nat functions depend on a CMTK installation. See ?cmtk and README.md for details.\n\n\n\nAttaching package: 'nat'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    intersect, setdiff, union\n\n\nThe following objects are masked from 'package:dplyr':\n\n    intersect, setdiff, union\n\n\nThe following object is masked from 'package:RNiftyReg':\n\n    xform\n\n\nThe following object is masked from 'package:oro.nifti':\n\n    origin\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nn = Cell07PNs[[1]]\n# inspect its internal structure\nstr(n)\n\nList of 24\n $ CellType     : chr \"DA1\"\n $ NeuronName   : chr \"EBH11R\"\n $ InputFileName: 'AsIs' chr \"/GD/projects/PN2/TransformedTraces/DA1/EBH11R.tasc\"\n $ CreatedAt    : POSIXt[1:1], format: \"2006-01-18 02:21:14\"\n $ NodeName     : Named chr \"jefferis.joh.cam.ac.uk\"\n  ..- attr(*, \"names\")= chr \"nodename\"\n $ InputFileStat:'data.frame':  1 obs. of  10 variables:\n  ..$ size  : num 15379\n  ..$ isdir : logi FALSE\n  ..$ mode  : 'octmode' int 644\n  ..$ mtime : POSIXt[1:1], format: \"2006-01-12 11:52:01\"\n  ..$ ctime : POSIXt[1:1], format: \"2006-01-12 11:52:01\"\n  ..$ atime : POSIXt[1:1], format: \"2006-01-18 02:21:14\"\n  ..$ uid   : int 501\n  ..$ gid   : int 501\n  ..$ uname : chr \"jefferis\"\n  ..$ grname: chr \"jefferis\"\n $ InputFileMD5 : Named chr \"fcacee3f874cbe2c6ad96214e6fee337\"\n  ..- attr(*, \"names\")= 'AsIs' chr \"/GD/projects/PN2/TransformedTraces/DA1/EBH11R.tasc\"\n $ NumPoints    : int 180\n $ StartPoint   : num 1\n $ BranchPoints : num [1:16] 34 48 51 75 78 95 98 99 108 109 ...\n $ EndPoints    : num [1:18] 1 42 59 62 80 85 96 100 102 112 ...\n $ NumSegs      : int 33\n $ SegList      :List of 33\n  ..$ : int [1:34] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ : int [1:9] 34 35 36 37 38 39 40 41 42\n  ..$ : num [1:7] 34 43 44 45 46 47 48\n  ..$ : int [1:4] 48 49 50 51\n  ..$ : int [1:9] 51 52 53 54 55 56 57 58 59\n  ..$ : num [1:4] 51 60 61 62\n  ..$ : num [1:14] 48 63 64 65 66 67 68 69 70 71 ...\n  ..$ : int [1:4] 75 76 77 78\n  ..$ : int [1:3] 78 79 80\n  ..$ : num [1:6] 78 81 82 83 84 85\n  ..$ : num [1:11] 75 86 87 88 89 90 91 92 93 94 ...\n  ..$ : int [1:2] 95 96\n  ..$ : num [1:3] 95 97 98\n  ..$ : int [1:2] 98 99\n  ..$ : int [1:2] 99 100\n  ..$ : num [1:3] 99 101 102\n  ..$ : num [1:7] 98 103 104 105 106 107 108\n  ..$ : int [1:2] 108 109\n  ..$ : int [1:4] 109 110 111 112\n  ..$ : num [1:4] 109 113 114 115\n  ..$ : int [1:3] 115 116 117\n  ..$ : num [1:3] 115 118 119\n  ..$ : int [1:3] 119 120 121\n  ..$ : num [1:14] 119 122 123 124 125 126 127 128 129 130 ...\n  ..$ : num [1:2] 108 135\n  ..$ : int [1:9] 135 136 137 138 139 140 141 142 143\n  ..$ : int [1:6] 143 144 145 146 147 148\n  ..$ : num [1:7] 143 149 150 151 152 153 154\n  ..$ : num [1:7] 135 155 156 157 158 159 160\n  ..$ : int [1:6] 160 161 162 163 164 165\n  ..$ : num [1:5] 160 166 167 168 169\n  ..$ : int [1:4] 169 170 171 172\n  ..$ : num [1:9] 169 173 174 175 176 177 178 179 180\n $ d            :'data.frame':  180 obs. of  7 variables:\n  ..$ PointNo: int [1:180] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ Label  : num [1:180] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ X      : num [1:180] 187 187 188 188 188 ...\n  ..$ Y      : num [1:180] 133 131 130 129 129 ...\n  ..$ Z      : num [1:180] 88.2 90.6 93.1 95 97.5 ...\n  ..$ W      : num [1:180] 1.01 1.27 1.14 1.27 1.27 1.27 1.27 1.27 1.27 1.27 ...\n  ..$ Parent : num [1:180] -1 1 2 3 4 5 6 7 8 9 ...\n $ OrientInfo   :List of 5\n  ..$ AxonOriented: logi TRUE\n  ..$ AVMPoint    : logi NA\n  ..$ AVLPoint    : logi NA\n  ..$ Scl         : num [1:3] 1 1 1\n  ..$ NewAxes     : num [1:3] 1 2 3\n $ SegOrders    : num [1:33] 1 2 2 3 4 4 3 4 5 5 ...\n $ MBPoints     : int [1:2] 34 48\n $ LHBranchPoint: int 75\n $ SegTypes     : num [1:33] 1 3 1 3 3 3 1 2 2 2 ...\n $ AxonSegNos   :List of 3\n  ..$ PreMBAxons : int 1\n  ..$ MBAxons    : int 3\n  ..$ PostMBAxons: int 7\n $ LHSegNos     : num [1:26] 8 9 10 11 12 13 14 15 16 17 ...\n $ MBSegNos     :List of 2\n  ..$ : int 2\n  ..$ : num [1:3] 4 5 6\n $ NumMBBranches: num 2\n $ AxonLHEP     : num 72\n - attr(*, \"class\")= chr [1:2] \"neuron\" \"list\"\n\n# summary of 3D points\nsummary(xyzmatrix(n))\n\n       X               Y                Z        \n Min.   :186.9   Min.   : 90.36   Min.   : 88.2  \n 1st Qu.:225.6   1st Qu.: 97.56   1st Qu.:103.4  \n Median :258.4   Median :102.70   Median :112.9  \n Mean   :249.4   Mean   :104.03   Mean   :120.9  \n 3rd Qu.:277.7   3rd Qu.:109.04   3rd Qu.:139.0  \n Max.   :289.5   Max.   :132.71   Max.   :157.3  \n\n# identify 3d location of endpoints\nxyzmatrix(n)[endpoints(n),]\n\n           X         Y         Z\n1   186.8660 132.70932  88.20393\n42  224.7067 109.86362 153.58749\n59  229.6343  92.30637 157.29700\n62  226.8855  90.36325 147.59602\n80  249.8864  93.59079 136.77795\n85  253.0099  90.94904 137.34921\n96  264.0801  98.53390 121.89463\n100 266.3223  98.73241 116.46558\n102 267.5460  98.59859 112.85608\n112 271.0762 102.67288 100.84955\n117 274.6128 100.92061 103.28772\n121 277.3920 102.19148 102.69473\n134 287.1214 101.43365 103.30740\n148 281.7958  96.63987 101.08075\n154 276.6551  95.09809  99.22113\n165 278.3287 113.75627 104.87700\n172 282.3069 111.93213 105.65492\n180 289.5364 111.96014 109.18281\n\n\nPlotting a neuron\n\nplot(n)\n\n\n\n\n\n\n\n\n\n\n3.1.2 Arteries\n\nlibrary(nat)\na&lt;-read.ngraph.swc(\"./Ext-Data/swc_files/BG0002.CNG.swc\")\nplot(a)\n\n\n\n\n\n\n\n\n\n\n\n\n“Atlas-Based Whole Brain White Matter Analysis Using Large Deformation Diffeomorphic Metric Mapping: Application to Normal Elderly and Alzheimer’s Disease Participants.” 2009. Neuroimage 46 (2): 486–99. https://doi.org/10.1016/j.neuroimage.2009.01.002.\n\n\nDecroocq, Méghane, Carole Frindel, Pierre Rougé, Makoto Ohta, and Guillaume Lavoué. 2023. “Modeling and Hexahedral Meshing of Cerebral Arterial Networks from Centerlines.” Medical Image Analysis 89: 102912. https://doi.org/https://doi.org/10.1016/j.media.2023.102912.\n\n\nDerek B Archer, Stephen A Coombes, David E Vaillancourt. 2018. “A Template and Probabilistic Atlas of the Human Sensorimotor Tracts Using Diffusion MRI.” Cerebral Cortex 28 (5): 1685--1699. https://doi.org/10.1093/cercor/bhx066.\n\n\nK. Mehta, J. Ogden, B. Ljungquist, and G. A. Ascoli. 2023. “Online Conversion of Reconstructed Neural Morphologies into Standardized SWC Format.” Nature Communications 14: 7429. https://doi.org/10.1038/s41467-023-42931-x.\n\n\nMazziotta J, Evans A, Toga A. 2001. “A Probabilistic Atlas and Reference System for the Human Brain: International Consortium for Brain Mapping (ICBM).” Philos Trans R Soc Lond B Biol Sci 356 (1412): 1293–1322. https://doi.org/10.1098/rstb.2001.0915.\n\n\nTzourio-Mazoyer N, Papathanassiou D, Landeau B. 2002. “Automated Anatomical Labeling of Activations in SPM Using a Macroscopic Anatomical Parcellation of the MNI MRI Single-Subject Brain.” Neuroimage 15 (1): 273–89. https://doi.org/10.1006/nimg.2001.0978.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Medical Images</span>"
    ]
  },
  {
    "objectID": "data-wrangling-signal.html",
    "href": "data-wrangling-signal.html",
    "title": "4  Signal processing",
    "section": "",
    "text": "4.1 ECG\nThe ECG records the electrical signal from the heart. In a clinical record it is performed as a 12 lead recording. The ECG contains an initial P wave which originates from the atrium. The P wave is absent in patients with atrial fibrillation. The RR interval is used to measure the heart rate. A false assumption is that the heart rate is constant but measurement shows beat to beat variation. Heart rate variability is a feature of health.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signal processing</span>"
    ]
  },
  {
    "objectID": "data-wrangling-signal.html#ecg",
    "href": "data-wrangling-signal.html#ecg",
    "title": "4  Signal processing",
    "section": "",
    "text": "4.1.0.1 ECG data\nThe ecg dataset of one patient contains 2048 observations collected at a rate of 180 samples per second.\n\nlibrary(ade4)\ndata(\"ecg\") #ts object\n#Time Series:\n#Start = 0.31 \n#End = 11.6822222222222 \n#Frequency = 180\nhead(ecg)\n\n[1] -0.104773 -0.093136 -0.081500 -0.116409 -0.081500 -0.116409\n\n\n\nplot(ecg)\n\n\n\n\n\n\n\n\nLet’s look now at the Hart dataset.\n\nECG&lt;-read.csv(\"../../DataMining/python_journey/Heart/ECG/data.csv\")\n\n#ECG is a dataframe object\n\nplot(as.ts(ECG)) # from base R\n\n\n\n\n\n\n\n\ncreate a ts object by providing starting and end time and frequency. the plot of the data now has a new x limit.\n\nECG1&lt;-ts(ECG$hart, start=c(0.31,13.7), frequency=180)\n\nplot(ECG1)\n\n\n\n\n\n\n\n\n\nEEGECG&lt;-read.csv(\"../../DataMining/python_journey/Heart/ECG/eeg_stroke_ecg.csv\")\n\n#this data has 2 columns time and ECG\n\nplot(as.ts(EEGECG$ECG))\n\n\n\n\n\n\n\nECG2&lt;-ts(EEGECG$ECG, start=c(0.31,6.75), frequency=60)\nplot(ECG2)\n\n\n\n\n\n\n\n\nData from Samsung smart watch series 5, acquired for 30 seconds at 500 Hz\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\necg&lt;-read.csv(\"./Ext-Data/20240226094349.csv\") \necg1&lt;-ts(ecg$Lead, frequency = 500)\nplot(ecg1)\n\n\n\n\n\n\n\n\nHere we illustrate the use of RHRV package to analyse ECG signal (Rodriguez-Linares et al. 2017). The code is provided on the RHRV website https://rhrv.r-forge.r-project.org/documentation.html. The example.beats data was download from this website and stored in the Data-Use folder. Here we make changes regarding the path of the data.\n\nlibrary(RHRV)\n\nLoading required package: waveslim\n\n\n\nwaveslim: Wavelet Method for 1/2/3D Signals (version = 1.8.4)\n\n\n\nAttaching package: 'waveslim'\n\n\nThe following object is masked from 'package:lubridate':\n\n    pm\n\n\nLoading required package: nonlinearTseries\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\nAttaching package: 'nonlinearTseries'\n\n\nThe following object is masked from 'package:grDevices':\n\n    contourLines\n\n\nLoading required package: lomb\n\nhrv.data = CreateHRVData()\nhrv.data = SetVerbose(hrv.data, TRUE)\n\n#setwd(\"C:/RHRV\")\nhrv.data = LoadBeatAscii(hrv.data, \"example.beats.txt\", \n                         RecordPath = \"./Data-Use/\" )\n\n Loading beats positions for record: example.beats.txt \n\n\n Path: ./Data-Use/ \n\n\n Scale: 1 \n\n\n Date:  01 / 01 / 1900 \n  Time:  00 : 00 : 00 \n\n\n Number of beats: 17360 \n\nplot(hrv.data$Beat$Time)\n\n\n\n\n\n\n\nhrv.data = BuildNIHR(hrv.data)\n\n Calculating non-interpolated heart rate \n Number of beats: 17360 \n\nPlotNIHR(hrv.data)\n\n Plotting non-interpolated instantaneous heart rate \n\n\n Number of points: 17360 \n\n\n\n\n\n\n\n\nhrv.data = FilterNIHR(hrv.data)\n\n Filtering non-interpolated Heart Rate \n\n\n Number of original beats: 17360 \n\n\n Number of accepted beats: 17248 \n\nPlotNIHR(hrv.data)\n\n Plotting non-interpolated instantaneous heart rate \n\n\n Number of points: 17248",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signal processing</span>"
    ]
  },
  {
    "objectID": "data-wrangling-signal.html#eeg-signal-processing",
    "href": "data-wrangling-signal.html#eeg-signal-processing",
    "title": "4  Signal processing",
    "section": "4.2 EEG signal processing",
    "text": "4.2 EEG signal processing\nThe eegUtils package has useful methods for plotting EEG. https://craddm.github.io/eegUtils/index.html. According to the site, it has functions for importing data from Biosemi, Brain Vision Analyzer, and EEGLAB.\n\n#devtools::install_github(\"mne-tools/mne-r\")\n#remotes::install_github(\"craddm/eegUtils@develop\")\nlibrary(eegUtils)\n\n\nAttaching package: 'eegUtils'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nplot_butterfly(demo_epochs)\n\nCreating epochs based on combinations of variables: epoch_label participant_id \n\n\n\n\n\n\n\n\n\n\ntopoplot(demo_epochs, \n         time_lim = c(.22, .25 ))\n\nCreating epochs based on combinations of variables: epoch_label participant_id \n\n\nUsing electrode locations from data.\n\n\nPlotting head r 95 mm\n\n\n\n\n\n\n\n\n\nThe following data is from https://datashare.ed.ac.uk/handle/10283/2189\n\nlibrary(eegUtils)\nlibrary(R.matlab)\nlimo_test &lt;- import_set(\"limo_dataset_S1.set\")\nlimo_cont &lt;- R.matlab::readMat(\"continuous_variable.mat\")\nlimo_cat &lt;- readr::read_csv(\"categorical_variable.txt\",\n                            col_names = c(\"cond_lab\"))\n\nThe MNE-R package is an interface to MNE package in Python. It is an excellent package for signal processing.\n\n\n\n\nRodriguez-Linares, Leandro, Xose Vila, Maria Jose Lado, Arturo Mendez, Abraham Otero, and Constantino Antonio Garcia. 2017. Heart Rate Variability Analysis with the r Package RHRV. Use r! Springer. https://doi.org/ 10.1007/978-3-319-65355-6.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signal processing</span>"
    ]
  },
  {
    "objectID": "data-wrangling-python.html",
    "href": "data-wrangling-python.html",
    "title": "5  Python",
    "section": "",
    "text": "Python can be run directly from R markdown file. It requires that python be specified instead of R. The use of python here requires that Miniconda or Anaconda be installed. Installation of [Miniconda][Minconda environment] via reticulate package will be shown below.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#dataset containing locations of ms clinics in Victoria\ndataset = pd.read_csv('./Data-Use/msclinic.csv') #Read data from CSV datafile\ndat=pd.DataFrame(dataset)\n#print 10 rows\nprint(dat.head(10))\n\n          id  public  clinic  ...        lat         lon  metropolitan\n0        mmc       1       1  ... -37.920668  145.123387             1\n1        rmh       1       1  ... -37.778945  144.946894             1\n2        aus       1       1  ... -37.756412  145.060279             1\n3        alf       1       1  ... -37.845960  144.981852             1\n4      stvin       1       1  ... -37.807586  144.975029             1\n5        bhh       1       1  ... -37.813525  145.118510             1\n6  frankston       1       1  ... -38.151211  145.129160             1\n7    geelong       1       1  ... -38.152047  144.364647             0\n8   sunshine       1       1  ... -37.760219  144.815301             1\n9   northern       1       0  ... -37.652886  145.014486             1\n\n[10 rows x 10 columns]\n\nexit\n\nUse exit() or Ctrl-Z plus Return to exit\n\n\nPassing Python object to R and py$ in front of Python object in R.\n\nhead(py$dat)\n\nLet’s open the the Hart dataset in R and pass it to Python.\n\nECG&lt;-read.csv(\"../../DataMining/python_journey/Heart/ECG/data.csv\")\n\n#ECG is a dataframe object\n\nplot(as.ts(ECG)) # from base R\n\n\n\n\n\n\n\n\nTo pass a R object to python then add r. in front of object.\n\n#The ECG data is now passed as r.ECG\nprint(r.ECG)\n\n      hart\n0      530\n1      518\n2      506\n3      494\n4      483\n...    ...\n2478   489\n2479   491\n2480   492\n2481   493\n2482   494\n\n[2483 rows x 1 columns]\n\n\nThe data can now be plotted using matplotlib library from Python.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\n#Matplotlib\nplt.title(\"Heart Rate Signal\") #The title of our plot\n\n#ECG is data frame and hart is the column\nplt.plot(r.ECG.hart) #Draw the plot object\n\n#Display the plot\nplt.show() \nexit\n\n\n5.0.1 Reticulate\nInformation on the use of Python in R is available at https://rstudio.github.io/reticulate/. The package reticulate can import Python function to work directly in R. Note that the chunk code heading here is r.\n\nlibrary(reticulate)\nos &lt;- import(\"os\") #os is operating system package\nos$listdir(\".\")\n\n [1] \".quarto\"                         \".Rhistory\"                      \n [3] \".Rproj.user\"                     \"appendix.qmd\"                   \n [5] \"bayesian-analysis.qmd\"           \"colin_1mm.hdr\"                  \n [7] \"colin_1mm.img\"                   \"colin_1mm.mat\"                  \n [9] \"cover.png\"                       \"Data-Use\"                       \n[11] \"data-wrangling-images.html\"      \"data-wrangling-images.qmd\"      \n[13] \"data-wrangling-images_files\"     \"data-wrangling-python.qmd\"      \n[15] \"data-wrangling-python.rmarkdown\" \"data-wrangling-python_files\"    \n[17] \"data-wrangling-signal.html\"      \"data-wrangling-signal.qmd\"      \n[19] \"data-wrangling-signal_files\"     \"data-wrangling.html\"            \n[21] \"data-wrangling.qmd\"              \"Ext-Data\"                       \n[23] \"geospatial-analysis.qmd\"         \"graph-theory.qmd\"               \n[25] \"Healthcare-R-Book.Rproj\"         \"index.aux\"                      \n[27] \"index.html\"                      \"index.log\"                      \n[29] \"index.qmd\"                       \"index.tex\"                      \n[31] \"index.toc\"                       \"intro.html\"                     \n[33] \"intro.qmd\"                       \"intro_files\"                    \n[35] \"Logistic_GeneticAlgorithm.Rda\"   \"Logistic_SimulatedAnnealing.Rda\"\n[37] \"machine-learning.qmd\"            \"multivariate.qmd\"               \n[39] \"natural-language-processing.qmd\" \"operational-research.qmd\"       \n[41] \"references.bib\"                  \"references.qmd\"                 \n[43] \"site_libs\"                       \"statistics.qmd\"                 \n[45] \"total_journal.Rda\"               \"_book\"                          \n[47] \"_quarto.yml\"                    \n\n\nHere we provide another example on how to use Python in R. Note the change in the way we extract the stats module from scipy Python package.\n\ndata(\"mtcars\") #mtcars data in R\n\nlibrary(reticulate)\nnp&lt;-import(\"numpy\")\npd&lt;-import(\"pandas\")\n\n#equivalent in Python is from scipy import stats\nsc&lt;-import(\"scipy\")\nsc$stats$linregress(mtcars$mpg,mtcars$cyl)\n\nLinregressResult(slope=-0.2525149506667544, intercept=11.260683180739264, rvalue=-0.8521619594266132, pvalue=6.112687142580981e-10, stderr=0.02830980675303087, intercept_stderr=0.5930361857152716)\n\n\n\n\n5.0.2 Minconda\nMiniconda and Anaconda can be installed directly from its website. Here we will illustrate installation of Miniconda from Rstudio. The install_miniconda function from reticulate library download Miniconda from the web.\n\n#library(reticulate)\n\n#this function is turned off as it only needs to be done once\n#install_miniconda(path = miniconda_path(), update = TRUE, force = FALSE)\n\nTo find the libraries install in Miniconda\n\n#conda list\n\n\n\n5.0.3 Python environment\nUnless specified, the default environment is r-reticulate. Setting the Python environment is important to avoid package incompatibility. To set the environment\n\nlibrary(reticulate)\n#virtualenv_create(\"SignalProcessing\")\n\nSome Python libraries such as pycox can be installed in R using install_py… this way.Some python packages have\n\nlibrary(reticulate)\n#library(survivalmodels)\n#install pycox for survivalmodels\n#install_pycox(pip = TRUE, install_torch = TRUE)\n#install_keras(pip = TRUE, install_tensorflow = TRUE)\n\n#install other Python packages\n#this is similar to pip install torch\n#install_torch(method = \"auto\", conda = \"auto\", pip = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "6  Statistics",
    "section": "",
    "text": "6.1 Univariable analyses",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#univariable-analyses",
    "href": "statistics.html#univariable-analyses",
    "title": "6  Statistics",
    "section": "",
    "text": "6.1.1 Parametric tests\nT-test is the workhorse for comparing if 2 datasets are have the same distribution. Performing t-test in R requires data from 2 columns: one containing the variables for comparison and one to label the group. There are different forms of t-test depending on whether the two samples are paired or unpaired. In general, the analysis takes the form of \\(t=\\frac{\\mu_1 - \\mu_2}{variance}\\). It is recommended to check the distribution of the data by using histogram. For this exercise, we will use the simulated data from ECR trials. The grouping variable is the trial assignment.\n\n#comparison of early neurological recovery (ENI) by tral (T)\ndtTrial&lt;-read.csv(\"./Data-Use/dtTrial_simulated.csv\")\nt.test(dtTrial$ENI~dtTrial$T)\n\n\n    Welch Two Sample t-test\n\ndata:  dtTrial$ENI by dtTrial$T\nt = 0.17454, df = 487.36, p-value = 0.8615\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.04569535  0.05460540\nsample estimates:\nmean in group 0 mean in group 1 \n      0.3084644       0.3040094 \n\n\n\n\n6.1.2 Non-parametric tests\nChi-squared and Fisher-exact tests can be done by using the table function for setting up the count data into 2 x 2 contingency table or confusion matrix. The formula for the Chi-squared test takes on a familiar form \\(\\chi^2=\\frac{(observed-expected)^2}{expected}\\). In this example we will use the data above.\n\ntable(dtTrial$HT,dtTrial$T)\n\n   \n      0   1\n  0 112 101\n  1 144 143\n\nchisq.test(dtTrial$HT,dtTrial$T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dtTrial$HT and dtTrial$T\nX-squared = 0.19553, df = 1, p-value = 0.6584\n\n\nThe Wilcoxon rank sum test is performed with continuous data organised in the same way as the t-test. There are several different approaches to performing Wilcoxon rank sum test. The coin package allows handling of ties.\n\nlibrary(coin)\n\nLoading required package: survival\n\nwilcox.test(ENI~T, data=dtTrial)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  ENI by T\nW = 31159, p-value = 0.9642\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#regression",
    "href": "statistics.html#regression",
    "title": "6  Statistics",
    "section": "6.2 Regression",
    "text": "6.2 Regression\nThere are many different form of regression methods. A key principle is that the predictors are independent of each others. This issue will be expand on in the later in the section on collinearity. Special methods are required when the predictors are collinear.\n\n6.2.1 Brief review of matrix\nA vector is has length one. A matrix is an ordered array in 2 dimensions. A tensor is an ordered array in 3 dimensions.\nA matrix in which the columns are linearly related are said to be rank deficient. The rank of a given matrix is an expression of the number of linearly independent columns of that matrix. Given that row rank and column rank are equivalent, rank deficiency of a matrix is expressed as the difference between the lesser of the number of rows and columns, and the rank of the matrix. A matrix with rank of 1 is likely to be linearly related.\n\n\n6.2.2 Linear (least square) regression\nLeast square regression uses the geometric properties of Euclidean geometry to identify the line of best. The sum of squares \\(SSE\\) is \\(\\sum(observed-expected)^2\\). The \\(R^2\\) is a measure of the fit of the model. It is given by \\(1-\\frac{SS_(res)}{SS_(total)}\\). Low \\(R^2\\) indicates a poorly fitted model and high \\(R^2\\) indicates excellent fitting. The assumption here is that the outcome variable is a continuous variable.\n\nlibrary(ggplot2)\nload(\"./Data-Use/world_stroke.Rda\")\n\nggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+\n  geom_smooth(method=\"lm\", aes(Group=Income, linetype=Income))+geom_point()+xlab(\"Life Expectancy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n6.2.3 Logistic regression\nFor outcome that are binary in nature such as yes or no, then least square regression is not appropriate. There are no close form solution for this analysis and a numerical approach using maximum likelihood approach is needed. When examining the results of logistic regression one is often enchanted by the large odds ratio. It is important to look at the metrics of model calibration (discussed below). A clue to a poorly calibrated model is the observation that the width of the confidence interval for odds ratio is wide.\n\n#glm\ndata(\"BreastCancer\",package = \"mlbench\")\n\n#remove id column and column with NA to feed into iml later\nBreastCancer2&lt;-lapply(BreastCancer[,-c(1,7)], as.numeric)\nBreastCancer2&lt;-as.data.frame(BreastCancer2)\n\nDCa&lt;-glm(Class~., data=BreastCancer2)\n\nsummary(DCa)\n\n\nCall:\nglm(formula = Class ~ ., data = BreastCancer2)\n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.723054   0.018613  38.847  &lt; 2e-16 ***\nCl.thickness    0.042754   0.003992  10.709  &lt; 2e-16 ***\nCell.size       0.019263   0.007293   2.641  0.00845 ** \nCell.shape      0.032217   0.007016   4.592 5.22e-06 ***\nMarg.adhesion   0.021463   0.004386   4.893 1.24e-06 ***\nEpith.c.size    0.011637   0.005965   1.951  0.05148 .  \nBl.cromatin     0.035266   0.005650   6.241 7.57e-10 ***\nNormal.nucleoli 0.016928   0.004247   3.986 7.44e-05 ***\nMitoses         0.001086   0.006048   0.180  0.85757    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.0481263)\n\n    Null deviance: 157.908  on 698  degrees of freedom\nResidual deviance:  33.207  on 690  degrees of freedom\nAIC: -126.1\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\n6.2.4 Discrimination and Calibration\nA high _\\(R^2\\) suggests that the linear regression model is well calibrated. This metric is often not displayed but should be sought when interpreting the data.\nThe areas under the receiver operating characteristic curve (AUC) is used to assess how well the models discriminate between those who have the disease and those who do not have the disease of interest. An AUC of 0.5 is classified as no better than by chance; 0.8 to 0.89 provides good (excellent) discrimination, and 0.9 to 1.0 provides outstanding discrimination. This rule of thumb about interpreting AUC when reading the literature is language the authors used to describe the AUC. This test of discrimination is not synonymous with calibration. It is possible to have a model with high discrimination but poor calibration (Diamond 1992). The AUC is similar to Harrell’s c-index but the interpretation of difference in AUC and c-index between models is not straightforward. A difference in 0.1 of AUC correspond to the number of subject rank correctly. The c-index was originally described for survival analysis (Harrell FE Jr 1982). Harrell described the c-index (concordance index) as estimating the probability that, of two randomly chosen patients, the patient with the higher prognostic score will outlive the patient with the lower prognostic score. As such the c-index should be interpreted as the number of concordant pairs relative to the total number of comparable pairs. It has been proposed that the AUC and c-index is not appropriate for survival analysis as they do not account for the dynamic nature of the data(Longato, Vettoretti, and Di Camillo 2020). The integrated Graf score has been proposed to account for difference in the estimated event-free survival probabilities with the actual outcome (Graf E 1999).\nCalibration of logistic regression model is performed using the Hosmer–Lemeshow goodness-of-ﬁt test and the Nagelkerke generalized R2. A model is well calibrated when the Hosmer–Lemeshow goodness-of-ﬁt test shows no difference between observed and expected outcome or P value approaching 1. A high generalized \\(R^2\\) value suggests a well-calibrated regression model. Running regression through the rms or PredictABEL library provide these results. The generalized \\(R^2\\) can be obtained manually from base R by running an intercept only model and again with covariates. It is given by \\(1-\\frac{L1}{L0}\\).\n\n#lrm on logistic regression analysis for Breast Cancer\nlibrary(rms)\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nDCa_rms&lt;-lrm(Class~., data=BreastCancer2)\nDCa_rms\n\nLogistic Regression Model\n\nlrm(formula = Class ~ ., data = BreastCancer2)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           699    LR chi2     759.86      R2       0.915    C       0.993    \n 1            458    d.f.             8      R2(8,699)0.659    Dxy     0.986    \n 2            241    Pr(&gt; chi2) &lt;0.0001    R2(8,473.7)0.795    gamma   0.986    \nmax |deriv| 8e-10                            Brier    0.028    tau-a   0.446    \n\n                Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept       -9.9477 1.0317 -9.64  &lt;0.0001 \nCl.thickness     0.5776 0.1190  4.85  &lt;0.0001 \nCell.size       -0.0115 0.1759 -0.07  0.9479  \nCell.shape       0.5679 0.1913  2.97  0.0030  \nMarg.adhesion    0.3137 0.1004  3.13  0.0018  \nEpith.c.size     0.1306 0.1406  0.93  0.3529  \nBl.cromatin      0.5800 0.1456  3.98  &lt;0.0001 \nNormal.nucleoli  0.1232 0.0987  1.25  0.2120  \nMitoses          0.6092 0.3226  1.89  0.0590  \n\n\n\n6.2.4.1 Measuring Improvement in Regression Models\nThe net reclassification improvement (NRI) and integrated discrimination improvement (IDI) have been proposed as more sensitive metrics of improvement in model discrimination.The NRI can be considered as a percentage reclassiﬁcation for the risk categories and the IDI is the mean difference in predicted probabilities between 2 models (constructed from cases with disease and without disease). The NRI and IDI scores are expressed as fractions and can be converted to percentage by multiplying 100.The continuous NRI and IDI can be performed using PredictABEL [Phan et al. (2017)](Phan et al. 2016).\n\n\n6.2.4.2 Shapley value\nWe can use ideas from game theory relating to fair distribution of proﬁt in coalition games; the coalition (co-operative) game in this case can be interpreted as contribution of the covariates to the model. The Shapley value regression method calculates the marginal contribution of each covariate as the average of all permutations of the coalition of the covariates containing the covariate of interest minus the coalition without the covariate of interest. The advantage of this approach is that it can handle multicollinearity (relatedness) among the covariates.\nThe feature importance is used to assess the impact of the features on the model’s decision\n\n#this section takes the output from logistic regression above.\nlibrary(iml)\nX = BreastCancer2[which(names(BreastCancer2) != \"Class\")]\npredictor = Predictor$new(DCa, data = X, y = BreastCancer2$Class)\nimp = FeatureImp$new(predictor, loss = \"mae\")\nplot(imp)\n\n\n\n\n\n\n\n\nFrom the logistic regression above cell thickness and cromatin have the highest coefficient and lowest p value. This is the same as feature importance. By contrast the Shapley values show that cell shape and marg adhesion make the largest impact on the model when considering the contribution to the model after considering all the contribution by different coalitions.\n\n#explain with game theory\nshapley &lt;- Shapley$new(predictor, x.interest = X[1, ])\nshapley$plot()\n\n\n\n\n\n\n\n\n\n\n6.2.4.3 ICE\nThe individual conditional expectation (ICE) curves is the plot of the expectation fof the predictive value for each observation at the unique value for the feature.\n\n#feature is the covariate of interest\npar(mfrow=c(1,2))\neff_thick &lt;- FeatureEffect$new(predictor, \n                         feature = \"Cl.thickness\", \n                         method = \"ice\",\n                         center.at = 0)\n\nplot(eff_thick)\n\n\n\n\n\n\n\n\n\n\n\n6.2.5 Interaction\nInteractions is plotted here using lollipop bar. The ggalt library can be used to create this type of plot with geom_lollipop. The strength of interaction is measured using Friedman’s H-statistics. The H-statistics ranges from 0 to 1 with 1 indicating the overall interaction strength. In the case with Breast Cancer data, the interaction strength is low.\nWhen describing interaction terms it is recommended that the results be expressed as β coefficients rather than as odds ratio.\n\n#plot interactions\ninteract &lt;- Interaction$new(predictor)\nplot(interact)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#confounder",
    "href": "statistics.html#confounder",
    "title": "6  Statistics",
    "section": "6.3 Confounder",
    "text": "6.3 Confounder\nA confounder is a covariate that serves as a cause of both exposure and outcome and as such confound the analysis. A mediator exist on the causal pathway from exposure to outcome. A common misconception is that the multiple regression adjust for imbalance in covariates in clinical trial. This issue was observed in the pivotal NINDS alteplase trial. The results of the trial has since been accepted with re-analysis of this trial using covariate adjustment (Ingall et al. 2004).\nThere are several methods for covariate adjustment in radomised trials: direct adjustment, standardisation and inverse-probability-of-treatment weighting.\n\n6.3.1 Confounder weighted model\nThe issue asked is whether one should choose to perform confounder analysis or propensity matching.\n\n\n6.3.2 Propensity matching\nPropensity matching is an important technique to adjust for imbalance in covariates between 2 arms. There are concerns with mis-use of this technique such as difference in placebo arms from multiple sclerosis trials (Signori et al. 2020). It is proposed that this technique should be used only if all the confounders are measurable. This situation may not be satisfied if the data were accrued at different period, in different continent etc.\n\n\n6.3.3 E-values\nThe E-values (VanderWeele TJ 2017) has been proposed a measure of unmeasured confounders in observational studies. The E-value is a measure of the extent to which the confounder have on the treatment–outcome association, conditional on the measured covariates. A large E-value is desirable. The E-values is available in Evalue library (Mathur MB 2018).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#causal-inference",
    "href": "statistics.html#causal-inference",
    "title": "6  Statistics",
    "section": "6.4 Causal inference",
    "text": "6.4 Causal inference\nCausation and association are often miscontrued to be the same. However, the finding of correlation (association) does necessarily imply causation. Causal inference evaluates the response of an effect variable in the setting of change in the cause of the effect variable. There are issues with approach to analysis of causal inference. It can be performed using frequentist such as confounder weighted model or Bayesian methods such as [(Baysian additive regression tree)][Bayesian trees method].",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#special-types-of-regression",
    "href": "statistics.html#special-types-of-regression",
    "title": "6  Statistics",
    "section": "6.5 Special types of regression",
    "text": "6.5 Special types of regression\n\n6.5.1 Ordinal regression\nOrdinal regression is appropriate when the outcome variable is in the form of ordered categorical values. For example, the Rankin scale of disability is bounded between the values of 0 and 6. This type of analysis uses the proportional odds model and the requirement for this model is stringent. When examining results of ordinal regression check that the authors provide this metric, the Brant test. The Brant test assesses the parallel regression assumption. Ordinal regression is performed using polr function in MASS library. The Brant test is available in the Brant library.\n\n\n6.5.2 Survival analysis\nSurvival analysis is useful when dealing with time to event data. Time to event data can be left, interval and right censoring. Left censoring exists when events may have already occurred at the start of the study eg purchase of phones. Right censoring exists when events have not happened yet eg cancer trial. Interval censoring exists when an insurance has been purchased but the date of product purchase is not yet known.\nThe Cox model assesses the hazard of outcome between two groups. The assumption of this model is that the hazard between each arm is proportional (Stensrud and Hernan 2020). The proportional hazard model can be tested based on the weighted Schoenfeld residuals(Grambsch and Therneau 1994). There are non-parametric models available when the assumption of the proportional hazard model does not hold.\nIn the next chapter on machine learning, an illustration of [random survival forest with rfrsc library][Random survival forest with rfsrc] and ranger library are provided. In the section on [clinical trial][Interpreting clinical trials] we illustrate how the results can be converted to numbers needed to treat. The median survival corresponding to survival probability of 0.50 can be determined here. Metrics for assessing survival model was described above.\n\nlibrary(survival)\nlibrary(survminer)\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'survminer'\n\n\nThe following object is masked from 'package:survival':\n\n    myeloma\n\n#data from survival package on NCCTG lung cancer trial\n#https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html\ndata(cancer, package=\"survival\")\n\n#time in is available in days\n#status censored=1, dead=2\n#sex:Male=1 Female=2\n\nsurvfit(Surv(time, status) ~ 1, data = cancer)\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = cancer)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\n\nsfit&lt;- coxph(Surv(time, status) ~ age+sex+ph.ecog+ph.karno+wt.loss, data = cancer)\nsummary(sfit)\n\nCall:\ncoxph(formula = Surv(time, status) ~ age + sex + ph.ecog + ph.karno + \n    wt.loss, data = cancer)\n\n  n= 213, number of events= 151 \n   (15 observations deleted due to missingness)\n\n              coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nage       0.015157  1.015273  0.009763  1.553 0.120538    \nsex      -0.631422  0.531835  0.177134 -3.565 0.000364 ***\nph.ecog   0.740204  2.096364  0.191332  3.869 0.000109 ***\nph.karno  0.015251  1.015368  0.009797  1.557 0.119553    \nwt.loss  -0.009298  0.990745  0.006699 -1.388 0.165168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         exp(coef) exp(-coef) lower .95 upper .95\nage         1.0153     0.9850    0.9960    1.0349\nsex         0.5318     1.8803    0.3758    0.7526\nph.ecog     2.0964     0.4770    1.4408    3.0502\nph.karno    1.0154     0.9849    0.9961    1.0351\nwt.loss     0.9907     1.0093    0.9778    1.0038\n\nConcordance= 0.64  (se = 0.026 )\nLikelihood ratio test= 33.53  on 5 df,   p=3e-06\nWald test            = 32.27  on 5 df,   p=5e-06\nScore (logrank) test = 32.83  on 5 df,   p=4e-06\n\n\nTest proportional hazard assumption using weighted residuals (Grambsch and Therneau 1994). The finding below shows that inclusion of covariate ph.karno violate proportional hazard assumption.\n\ncox.zph(sfit)\n\n         chisq df     p\nage      0.186  1 0.666\nsex      2.059  1 0.151\nph.ecog  1.359  1 0.244\nph.karno 4.916  1 0.027\nwt.loss  0.110  1 0.740\nGLOBAL   7.174  5 0.208\n\n\nPlot fit of survival model\n\nggcoxdiagnostics(sfit, type = \"deviance\", \n ox.scale = \"linear.predictions\")\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the survminer package.\n  Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nForest plot of outcome from survival analysis\n\nggforest(sfit)\n\n\n\n\n\n\n\n\nAn alternative way to display the output from Cox regression is to use forestmodel library .\n\nforestmodel::forest_model(sfit)\n\n\n\n\n\n\n\n\nThe pseudoR2 for Cox regression model proposed by Royston can be evaluated\n\nroyston(sfit)\n\n        D     se(D)       R.D      R.KO       R.N      C.GH \n0.8546894 0.1495706 0.1484960 0.1515108 0.1459168 0.6418894 \n\n\n\n\n6.5.3 Quantile regression\nLeast spare regression is appropriate when the data is homoscedascity or the error term remain constant. This can be seen as the data varies around the fitted line. Homoscedascity implies that the data is homogenous. Quantile regression is appropriate when the distribution of the data is non-normal and it is more appropriate to look at the conditional median of the dependent variable. There are several libraries for this task quantreg and Bayesian libraries. In the example below, the life time risk of stroke is regressed against life expectancy using lest square and quantile regression.\n\nlibrary(quantreg)\nlibrary(ggplot2)\n\nload(\"./Data-Use/world_stroke.Rda\")\n\n#quantile regression\nrqfit &lt;- rq( MeanLifetimeRisk~ LifeExpectancy, data = world_sfdf)\nrqfit_sum&lt;-summary(rqfit)\n\n#least square regression\nlsfit&lt;-lm(MeanLifetimeRisk~LifeExpectancy,data=world_sfdf)\nlsfit_sum&lt;-summary(lsfit) \n\n#plot\nggplot(world_sfdf,  aes(x=LifeExpectancy,y=MeanLifetimeRisk))+\n  #add fitted line for least square\n  geom_abline(intercept =lsfit_sum$coefficients[1], slope=lsfit_sum$coefficients[2],color=\"red\")+\n  #add fitted line for quantile regression\n  geom_point()+xlab(\"Life Expectancy\")+\n  geom_abline(intercept =rqfit_sum$coefficients[1], slope=rqfit_sum$coefficients[2],color=\"blue\")\n\n\n\n\n\n\n\n#annotate least square and quantile at position x, y\n#annotate(\"text\",x=60, y=27, label=paste0(\"least square =\",                        round(lsfit_sum$coefficients[1],2) ,\"+\", round(lsfit_sum$coefficients[2],2),\"x \",\"Life Expectancy\"),color=\"red\")+ annotate(\"text\",x=75, y=12,label=paste0(\"quantile =\",round(rqfit_sum$coefficients[1],2), \" + \", round(rqfit_sum$coefficients[2],2),\" x \",\"Life Expectancy\"),color=\"blue\")\n\n\n\n6.5.4 Non-negative regression\nIn certain situations, it is necessary to constrain the analysis so that the regression coefifcients are non-negative. For example, when regressing brain regions against infarct volume, there is no reason believe that a negative coefficient attributable to a brain region is possible(Phan, Donnan, Koga, et al. 2006) . Non-negative regression can be performed in R using nnls.\n\n\n6.5.5 Poisson regression\nPoisson regression is used when dealing with number of event over time or distance such as number of new admissions or new cases of hepatitis or TIA over time. An assumption of the Poisson distribution is that the mean & lambda; and variance λ are the same.\nA special case of Poisson regression is the negative binomial regression. This latter method is used when the variance is greater than the mean pf the data or over-dispersed data. Negative binomial regression can be applied to number of ‘failure’ event over time. Here ‘failure’ has a lose definition and can be stroke recurrence after TIA or cirrhosis after hepatitis C infection.\nZero-inflated data occurs when there is an abundance of zeroes in the data (true and excess zeroes).\n\n\n6.5.6 Conditional logistic regression\nConditional logistical regression model should be used when the aim is to compare pair of objects from the same patient. Examples include left and right arms or left and right carotid arteries. This method is available from clogit in survival.\n\n\n6.5.7 Multinomial modelling\nMultinomial modelling is used when the outcome categorical variables are not ordered. This situation can occur when analysis involves choice outcome (choices of fruit: apple, orange or pear). In this case, the log odds of each of the categorical outcomes are analysed as a linear combination of the predictor variables. The nnet library have functions for performing this analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#sample-size-estimation",
    "href": "statistics.html#sample-size-estimation",
    "title": "6  Statistics",
    "section": "6.6 Sample size estimation",
    "text": "6.6 Sample size estimation\nClinicians are often frustrated about sample size and power estimation for a study, grant or clinical trial. This aspect is scrutinised by ethics committee and in peer review process for journals. Luckily, R provides several packages for sample size amd power estimation: pwr library. Cohen has written reference textbook on this subject (Cohen 1977).\n\n6.6.1 Proportion\n\nlibrary(pwr)\n#ttest-d is effect size \n#d = )mean group1 -mean group2)/variance\npwr.t.test(n=300,d=0.2,sig.level=.05,alternative=\"greater\") \n\n\n     Two-sample t test power calculation \n\n              n = 300\n              d = 0.2\n      sig.level = 0.05\n          power = 0.7886842\n    alternative = greater\n\nNOTE: n is number in *each* group\n\n\nWe provided an example below for generating power of clinical trial. Examples are taken from a paper on sample size estimation for phase II trials (Phan, Donnan, Davis, et al. 2006).\n\nlibrary(pwr)\n#h is effect size. effect size of 0.5 is very large \n#sample size\npwr.2p.test(h=0.5,n=50,sig.level=0.05,alternative=\"two.sided\")\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 50\n      sig.level = 0.05\n          power = 0.705418\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n#medium effect size\npwr.2p.test(h=0.1,n=50,sig.level=0.05,alternative=\"two.sided\")\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1\n              n = 50\n      sig.level = 0.05\n          power = 0.07909753\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n\nThe output of the sample size calculation can be put into a table or plot.\n\nlibrary(pwr)\n#pwr.2p.test(h=0.3,n=80,sig.level=0.05,alternative=\"two.sided\")\nh &lt;- seq(.1,.5,.1) #from 0.1 to 0.3 by 0.05\nnh &lt;- length(h) #5\np &lt;- seq(.3,.9,.1)# power from 0.5 to 0.9 by 0.1\nnp &lt;- length(p) #9\n# create an empty array 9 x 5\nsamplesize &lt;- array(numeric(nh*np), dim=c(nh,np))\nfor (i in 1:np){\n  for (j in 1:nh){\n    result &lt;- pwr.2p.test(n = NULL, h = h[j],\n    #result &lt;- pwr.r.test(n = NULL, h = h[j],\n    sig.level = .05, power = p[i],\n    alternative = \"two.sided\")\n    samplesize[j,i] &lt;- ceiling(result$n)\n  }\n}\nsamplesize\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]  412  583  769  980 1235 1570 2102\n[2,]  103  146  193  245  309  393  526\n[3,]   46   65   86  109  138  175  234\n[4,]   26   37   49   62   78   99  132\n[5,]   17   24   31   40   50   63   85\n\n#graph\nxrange &lt;- range(h)\nyrange &lt;- round(range(samplesize))\ncolors &lt;- rainbow(length(p))\nplot(xrange, yrange, type=\"n\",\n  xlab=\"Effect size (h)\",\n  ylab=\"Sample Size (n)\" )\n# add power curves\nfor (i in 1:np){\n  lines(h, samplesize[,i], type=\"l\", lwd=2, col=colors[i])\n}\n# add annotation (grid lines, title, legend) \nabline(v=0, h=seq(0,yrange[2],50), lty=2, col=\"grey89\")\nabline(h=0, v=seq(xrange[1],xrange[2],.02), lty=2,\n   col=\"grey89\")\ntitle(\"Sample Size Estimation\\n Difference in Proportion\")\nlegend(\"topright\", title=\"Power\", as.character(p),\n   fill=colors)\n\n\n\n\n\n\n\n\n\n6.6.1.1 Non-inferiority\nNon-inferiority trials may offer information in a way that a traditional superiority design do not. The design may be interested in other aspect of the treatment such as cost and lower toxicity (Kaji and Lewis 2015). Examples of non-inferiority trial designs include antibiotics versus surgery for appendicitis (Salminen et al. 2015). There are concerns with reporting of noninferiority trial. Justification for the margin provided in 27.6% (Gopal et al. 2015). The following describes a trial design where it’s expected that drug will result in a certain outcome p1 and the control arm p2 and the ratio of subject in treatment to control arm is k. The difference in outcome is delta. The margin is defined as non-inferior if &lt;0.\n\nlibrary(TrialSize)\nTwoSampleProportion.NIS(alpha=.05, \n                        beta=.8,\n                        p1=.6,\n                        p2=.7,\n                        k=1,\n                        delta = .1,       \n                        margin=-.2\n                        )\n\n[1] 3.225911\n\n\n\n\n\n6.6.2 Logistic regression\n\nlibrary(powerMediation)\nlibrary(ggplot2)\n\n#continuous predictor\n#p1=event rate\n#exp(0.405) =1.5\n\npowerLogisticCon(n=200, p1=0.265, OR=exp(0.014), alpha=0.05)\n\n[1] 0.03056289\n\n# creating a data frame using data from \na=seq(0,05.4,0.05)\ndf_power&lt;-data.frame(`ASPECTS`= a,\n\"Power\"=powerLogisticCon(n=100, p1=a, OR=exp(.695), alpha=0.05)\n)\n\nWarning in sqrt(n * beta.star^2 * p1 * (1 - p1)): NaNs produced\n\nggplot(data=df_power, aes(x=ASPECTS, y=Power))+geom_point()\n\nWarning: Removed 88 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nAn alternative library to perform sample size for logistic regression is WebPower library.\n\nlibrary(WebPower) \n\nLoading required package: lme4\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:modeltools':\n\n    refit\n\n\nThe following object is masked from 'package:nlme':\n\n    lmList\n\n\nThe following object is masked from 'package:MatrixModels':\n\n    mkRespMod\n\n\nThe following object is masked from 'package:generics':\n\n    refit\n\n\nLoading required package: lavaan\n\n\nThis is lavaan 0.6-15\nlavaan is FREE software! Please report any bugs.\n\n\nLoading required package: PearsonDS\n\nwp.logistic(p0=0.007, #Prob (Y=1|X=0) \n            p1=0.012, #Prob (Y=1|X=1)\n            alpha=0.05, \n            power=0.80, \n            alternative=\"two.sided\", \n            family=\"normal\")\n\nPower for logistic regression\n\n       p0    p1     beta0     beta1      n alpha power\n    0.007 0.012 -4.954821 0.5440445 3336.2  0.05   0.8\n\nURL: http://psychstat.org/logistic\n\n\n\n\n6.6.3 Survival studies\nSample size for survival studies can be performed using powerSurvEpi or gsDesign.\n\nlibrary(powerSurvEpi)\n\n#sample size\nssizeEpi.default(power = 0.80, \n                 theta = 2, \n                 p = 0.408 , \n                 psi = 0.15,\n                 rho2 = 0.344^2, \n                 alpha = 0.05)\n\n[1] 512\n\n#power\npowerEpi.default(n = 2691, \n                 theta = 2, \n                 p = 0.408, \n                 psi = 0.250,\n                 rho2 = 0.344^2, \n                 alpha = 0.05)\n\n[1] 1\n\n#Amarenco NEJM 2020 #equal sample size k=1\nssizeCT.default(power = 0.8, k = .8, pE = 0.085, \n                pC = 0.109, \n                RR = 0.78, alpha = 0.05)\n\n  nE   nC \n2417 3021 \n\n\n\n\n6.6.4 Multiple regression\nThe power for general linear model can be calculated using the pwr.f2.test function.\n\nlibrary(pwr)\n#u=degrees of freedom for numerator\n#v=degrees of freedomfor denominator\n#f2=effect size",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#randomised-clinical-trials",
    "href": "statistics.html#randomised-clinical-trials",
    "title": "6  Statistics",
    "section": "6.7 Randomised clinical trials",
    "text": "6.7 Randomised clinical trials\nA common misconception is that the multiple regression adjust for imbalance in covariates in clinical trial. This issue was observed in the pivotal NINDS alteplase trial. The results of the trial has since been accepted with re-analysis of this trial using covariate adjustment(Ingall et al. 2004). There are several methods for covariate adjustment in radomised trials: direct adjustment, standardisation and inverse-probability-of-treatment weighting.\n\n6.7.1 Covariate adjustment in trials\nSpecifically, covariate adjustment refers to adjustment of covariates available at the time of randomisation, i.e. prespecified variables and not variables after randomisation such as pneumonia post stroke trials. The advantage of covariate adjustment is that it results in narrower confidence interval as well as increase the power of the trial up to 7% (Kahan 2014). The increased power is highest when prognostic variables are used but can decrease power when non-prognostic variables are used (Kahan 2014).\n\n\n6.7.2 Subgroup analysis\nSubgroup analysis can be misleading especially if not specified prior to trial analysis (Wang et al. 2007). Furthermore, increasing the number of subgroup analysis will lead to increasing the chance of false positive result or multiplicity. It is important to differentiate between prespecified and posthoc analyses as posthoc analyses may be biased by examination of the data.\n\n\n6.7.3 p value for interaction\nThe p value for interaction describe the influence by a baseline variable treatment effect on outcome in clinical trial (Wang et al. 2007). In a hypothetical trial, a significant p value for interaction between males and females for treatment effect on primary outcome indicates heterogenity of treatment effect.\n\nlibrary(Publish)\n\nWarning: package 'Publish' was built under R version 4.3.2\n\n\nLoading required package: prodlim\n\n\nRegistered S3 method overwritten by 'Publish':\n  method   from\n  print.ci coin\n\nlibrary(survival)\n\n\n\n6.7.4 Interpreting risk reduction in clinical trials\nA key issue in interpreting of clinical trials occurs when the relative risk reduction or relative hazard risk are provided. This issue affect the clinical interpretation of the trial finding and its application in practice. An example is the result of the ACAS asymptomatic carotid artery trial is often quoted as showing 50% risk reduction. In fact, there was 2% annual risk of ipsilateral stroke in the medical and 1% risk in the surgical arm. The absolute risk reduction or ARR was 1% per year. However, the 50% relative risk reduction is often quoted to explain to patients.\n\n\n6.7.5 NNT from ARR\nIn this case, the number needed to treat (NNT) is given by \\(\\frac{1}{ARR}\\) or \\(\\frac{1}{0.01}=100\\) to achieve the trial outcome or 100 patients are needed to be treated to reduce the stroke risk to 1%. The recommendation is that the 95% confidence interval for NNT should be provided.\n\n\n6.7.6 NNT from odds ratio\nCalculation of NNT for odds ratio requires knowledge of the outcome of the placebo group. The NNT is given by \\(\\frac{1}{ACR-\\frac{OR*ACR}{1-(ACR+OR*ACR)}}\\). The ACR represents the assumed control risk. The NNT can be calculated from nnt function in meta library.\n\nlibrary(meta)\n\nWarning: package 'meta' was built under R version 4.3.2\n\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 7.0-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\n#p.c = baseline risk\nnnt(0.73, p.c = 0.3, sm = \"OR\")\n\n    OR p.c      NNT\n1 0.73 0.3 16.20811\n\n\n\n\n6.7.7 NNT from risk ratio\n\n#data from EXTEND-IV trial in NEJM 2019\n#outcome 35.4% in tpa and 29.5% in control\nnnt(1.44, p.c = 0.295, sm = \"RR\")\n\n    RR   p.c      NNT\n1 1.44 0.295 -7.70416\n\n\n\n\n6.7.8 NNT from hazard ratio\nCalculation of NNT for hazard ratio requires knowledge of the outcome of the placebo group and the hazard ratio or HR. The formula using the binomial theorem is \\(p=1-q\\) where q is given by the ratio of outcome and numbers recruited in the placebo group. The formula is taken from (Ludwig, Darmon, and Guerci 2020) . The NNT is given \\(\\frac{1}{[p^{HR}-p}\\) . We illustrated this using data from metaanalysis on aspirin use in stroke in Lancet 2016. There were 45 events among 16053 patients in the control group. The HR was 0.44. The NNT from this formula \\(\\frac{1}{.9971968^.44-.9971968}\\) was 637.\n\n\n6.7.9 NNT from metaananlysis\nThere are concerns with using NNT from the results of metaanalysis as the findings are amalgations of trials with different settings (Marx 2003) (Smeeth 1999). The caution applies when baseline risks or absolute risk differences vary across trials.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#diagnostic-test",
    "href": "statistics.html#diagnostic-test",
    "title": "6  Statistics",
    "section": "6.8 Diagnostic test",
    "text": "6.8 Diagnostic test\n\n6.8.1 Sensitivity, specificity\nThe sensitivity of a diagnostic test is the true positive rate and the specificity is the true negative rate. Example of 2 x 2 table is provided here. As an exercise, consider a paper about a diagnostic test for peripheral vertigo reporting 100% sensitivity and 94.4% specificity. There are 114 patients, 72 patients without stroke have vertigo and positive test findings. Among patients with stroke 7 of 42 have positive test findings. The sensitivity is \\(TP=\\frac{TP}{TP+FN}\\) and the specificity is the \\(TN=\\frac{TN}{TN+FP}\\).\n\n#              Peripheral Vertigo\n#            Disease Positive     Disease Negative\n#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n# HIT Test#                 #                      $\n# Positive# True Positive   # False Positive       $  \n#         #     72          #       7              $\n#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n# HIT Test#                 #                      $\n# Negative# False Negative  # True Negative        $\n#         #     0           #       35             $ \n#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n\n# Peripheral Vertigo\n#Sensitiviy=TP/(TP+FN)=100%\n#Specificity=TN/(TN+FP)=83%\n\n\n\n6.8.2 AUC\nThe area under the receiver operating characteristics (ROC) curve or AUC is a measure of the accuracy of the test. It is recommended that ROC curve is used when there are multiple threshold. It should not be used when the test has only one threshold. Some investigators suggest caution regarding the validity of using receiver operating curve with single threshold diagnostic tests (J 2020). An AUC of 0.5 is classified as no better than by chance; 0.6–0.69 provides poor discrimination; 0.7–0.79 provides acceptable (fair) discrimination, 0.8 to 0.89 provides good (excellent) discrimination, and 0.9 to 1.0 provides outstanding discrimination.\n\n\n6.8.3 Likelihood ratio\nPositive likelihood ratio (PLR) is the ratio of sensitivity to false positive rate (FPR); the negative (NLR) likelihood ratio is the ratio of 1-sensitivity to specificity. A PLR indicates the likelihood that a positive spot sign (test) would be expected in a patient with ICH (target disorder) compared with the likelihood that the same result would be expected in a patient without ICH. Using the recommendation by Jaeschke et al, a high PLR (&gt;5) and low NLR (&lt;0.2) indicate that the test results would make moderate changes in the likelihood of hematoma growth from baseline risk. PLRs of &gt;10 and NLRs of &lt;0.1 would confer very large changes from baseline risk .\n\n6.8.3.1 Fagan’s normogram\nFagan’s normogram can be conceptualised as a sliding ruler to match the disease prevalence and likelihood ratios to evaluate the impact on the post-test probability (TJ 1975). At the current disease prevalence of 23.4% and PLR 4.65, the post-test probability remains low at 0.60.\n\nlibrary(tidyverse)\nsource(\"https://raw.githubusercontent.com/achekroud/nomogrammer/master/nomogrammer.r\")\np&lt;-nomogrammer(Prevalence = .234, Plr = 4.85, Nlr = 0.49)\np+ggtitle(\"Fagan's normogram for Spot Sign and ICH growth\")\n\n#to save the file\n#ggsave(p,file=\"Fagan_SpotSign.png\",width=5.99,height=3.99,units=\"in\")\n\n\n\n6.8.3.2 Likelihood ratio graph\nLikelihood ratio graph is a tool for comparing diagnostic tests (BJ 2000).\n\n#plot likelihood ratio graph\n\nLR_graph&lt;-function (Read,sheet,Sensitivity, Specificity){\n  \n  Read1&lt;-readxl::read_xlsx(Read, sheet = sheet)\n  \n  #binary data\n  #A=True pos %B=False positive   %C=False negative   %D=True negative\n  \n  A=Read1$TP\n  B=Read1$FP\n  C=Read1$FN\n  D=Read1$TN\n  \n  TPR=A/(A+C)\n  FPR=1-(D/(D+B))\n  \n  TPR_DiagnosticTest=Sensitivity\n  FPR_DiagnosticTest=1-Specificity\n  \n  # set plot\n  \n  X=seq(0,1,by=.1)\n  Y=seq(0,1,by=.1)\n\n  plot(X,Y,main=\"Likelihood Ratio graph\", xlab=\"1-Specificity\",ylab=\"Sensitivity\",cex=.25)\n  \n  #pch describe the shape. The value 1 corresponds o\n  points(FPR_DiagnosticTest,TPR_DiagnosticTest,pch=8,col=\"blue\",cex=2)\n  \n  #pch describe the shape. The value 8 corresponds *\n  points(FPR,TPR,pch=1,col=\"red\",cex=2) #add point\n  #abline(coef = c(0,1)) #add diagonal line\n  df1&lt;-data.frame(c1=c(0,TPR_DiagnosticTest),c2=c(0,FPR_DiagnosticTest))\n  reg1&lt;-lm(c1~c2,data=df1)\n  df2&lt;-data.frame(c1=c(TPR_DiagnosticTest,1),c2=c(FPR_DiagnosticTest,1))\n  reg2&lt;-lm(c1~c2,data=df2)\n  abline(reg1)\n  abline(reg2)\n  text(x=FPR_DiagnosticTest,y=TPR_DiagnosticTest+.3,label=\"Superior\",cex=.7)\n  text(x=FPR_DiagnosticTest+.2,y=TPR_DiagnosticTest+.2,label=\"Absence\",cex=.7)\n  text(x=.0125,y=TPR_DiagnosticTest-.1,label=\"Presence\",cex=.7)\n  text(x=FPR_DiagnosticTest+.1,y=TPR_DiagnosticTest,label=\"Inferior\",cex=.7)\n  text(x=.7,y=.2,label=\"Reference = Content Expert\",cex=.7)\n  text(x=.7,y=.15, label=\"Diagnostic Test software\", cex=.7)\n\n}\n\nRunnning the function from above\n\n#Sensitivity=0.623\n#Specificity=1-.927\n\nLR_graph(\"./Data-Use/Diagnostic_test_summary.xlsx\",1,.623,.927)\n\nNew names:\n• `` -&gt; `...11`",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#metaanalysis",
    "href": "statistics.html#metaanalysis",
    "title": "6  Statistics",
    "section": "6.9 Metaanalysis",
    "text": "6.9 Metaanalysis\nDuring journal club, junior doctors are often taught about the importance of metaanalysis. It is worth knowing how to perform a metaanalysis in order to critique the study. This is an important issue as the junior doctor is supervised by someone who a content expert but not necessarily a method expert. Metaanalysis can be performed for clinical trials, cohort studies or diagnostic studies. As an example, it is not well known outside of statistics journal that the bivariate analysis is the preferred method to evaluate diagnostic studies (Reitsma et al. 2005). By contrast, the majority of metaanalysis of diagnostic studies uses the univariate method of Moses and Littenberg (Moses, Shapiro, and Littenberg 1993). This issue will be expanded below.\n\n6.9.1 Quality of study\nAll studies require evaluation of the quality of the individual studies. This can be done with the QUADAS2 tool, available at https://annals.org/aim/fullarticle/474994/quadas-2-revised-tool-quality-assessment-diagnostic-accuracy-studies.\n\n\n6.9.2 PRISMA\nThe PRISMA statement is useful for understanding the search strategy and the papers removed and retained in the metaanalysis. An example of generating the statement is provided below in R. The example given here is from a paper on the use of spot sign to predict enlargment of intracerebral hemorrhage (Phan et al. 2019).\n\nlibrary(PRISMAstatement)\n#example from Spot sign paper. Stroke 2019\nprisma(found = 193,\n       found_other = 27,\n       no_dupes = 141, \n       screened = 141, \n       screen_exclusions = 3, \n       full_text = 138,\n       full_text_exclusions = 112, \n       qualitative = 26, \n       quantitative = 26,\n       width = 800, height = 800)\n\n\n\n\n#https://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html#attributes\nlibrary(DiagrammeR)\ngrViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10]\n\n  # several 'node' statements\n  node [shape = box,\n        fontname = Helvetica]\n  Stroke\n\n  node [shape = oval,\n        fixedsize = false,\n        color=red,\n        width = 0.9] \n  Hypertension; 'No Hypertension'\n  \n  node [shape= circle,\n  fontcolor=red,\n        color=blue,\n        fixedsize=false]\n\n  Hypokalemia; 'No Hypokalemia'        \n\n  # several 'edge' statements\n  edge [arrowhead=diamond]\n  \n  Stroke-&gt;{Hypertension, 'No Hypertension'}\n  Hypertension-&gt;{Hypokalemia, 'No Hypokalemia'}\n  \n}\n\n\")\n\n\n\n\n## alternative\n\ngrViz(\"digraph flowchart {\n      # node definitions with substituted label text\n      node [fontname = Helvetica, shape = rectangle]        \n      tab1 [label = '@@1']\n      tab2 [label = '@@2']\n      tab3 [label = '@@3']\n      tab4 [label = '@@4']\n      tab5 [label = '@@5']\n\n \n# edge definitions with the node IDs\n      tab1 -&gt; tab3 \n      tab1-&gt; tab2 \n      #tab2-&gt;tab3\n      tab2 -&gt; tab4 \n      tab2-&gt; tab5;\n      }\n\n      [1]: 'Stroke n=19'\n      [2]: 'Hypertension n=10'\n      [3]: 'No Hypertension n=9'\n      [4]: 'Hypokalemia n=?'\n      [5]: 'No Hypokalemia n=?'\n      \")\n\n\n\n\n\n\n\n6.9.3 Conversion of median to mean\nOne issue with performing metaanalysis is that one paper may report mean and another report median age. The formula for the mean is given by \\(\\frac{a+2m+b}{4}\\) where m is the median, a is the upper and b is the lower range (Wan 2014). The variance is given by \\(S^2=\\frac{1}{12}(\\frac{(a+2m+b)^2}{4}+{(b-a)^2})\\). This formula requires examination of the data such as the figure to obtain the upper and lower range. These changes are incorporated into meta libray using meatamean function (Balduzzi, Rücker, and Schwarzer 2019). More recently, investigators suggest to also consider the skewness of the data from the 5 number summary data (Shi et al. 2023). The argument method.mean in function metamean is used to specify the method for estimating the mean. In this case we chose the Luo method for illustration Luo (Luo et al. 2018). See the help page by typing question mark before metamean for more options.\n\nlibrary(meta)\n\n#NIHSS data from ANGEL large core trial in NEJM 2023\nmetamean(q1=4,q3=20,median=16,n=230, method.mean = \"Luo\")\n\nNumber of observations: o = 230\n\n    mean             95%-CI\n 13.1932 [11.6506; 14.7358]\n\nDetails:\n- Untransformed (raw) means\n\n\nHere the same data is used for the McGrath method (McGrath et al. 2020).\n\nmetamean(q1=4,q3=20,median=16,n=230, method.mean = \"QE-McGrath\")\n\nNumber of observations: o = 230\n\n    mean             95%-CI\n 13.3333 [11.7907; 14.8759]\n\nDetails:\n- Untransformed (raw) means\n\n\nThe conversion from median to mean in metafor is performed using conv.fivenum function (Viechtbauer 2010). There is an update on the metafor page as well as discussion on alternate approach. The default method of this function is to use the methods by Luo (Luo et al. 2018), Wan (Wan 2014) and Shi (Shi et al. 2023)\n\nlibrary(metafor)\n\nLoading required package: numDeriv\n\n\n\nLoading the 'metafor' package (version 4.2-0). For an\nintroduction to the package please type: help(metafor)\n\n\n\nAttaching package: 'metafor'\n\n\nThe following object is masked from 'package:rms':\n\n    vif\n\n# example data frame\nEstMean &lt;- data.frame(Paper=c(1:4,NA), min=c(1,2,NA,2,NA), q1=c(NA,NA,4,4,NA),\n                  median=c(5,6,6,6,NA), q3=c(NA,NA,10,10,NA),\n                  max=c(12,14,NA,14,NA),\n                  mean=c(NA,NA,NA,NA,7.0), sd=c(NA,NA,NA,NA,4.2),\n                  n=c(30,30,30,30,30))\nEstMean\n\n  Paper min q1 median q3 max mean  sd  n\n1     1   1 NA      5 NA  12   NA  NA 30\n2     2   2 NA      6 NA  14   NA  NA 30\n3     3  NA  4      6 10  NA   NA  NA 30\n4     4   2  4      6 10  14   NA  NA 30\n5    NA  NA NA     NA NA  NA    7 4.2 30\n\nEstMean &lt;- conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=EstMean)\n\nEstMean\n\n  Paper min q1 median q3 max     mean       sd  n\n1     1   1 NA      5 NA  12 5.356748 2.695707 30\n2     2   2 NA      6 NA  14 6.475664 2.940771 30\n3     3  NA  4      6 10  NA 6.713000 4.670521 30\n4     4   2  4      6 10  14 6.882074 3.546379 30\n5    NA  NA NA     NA NA  NA 7.000000 4.200000 30\n\n\nThe estmeansd library uses quantile estimation method with qe.mean.sd function when the data available are the median and quartile ranges (McGrath et al. 2020) (McGrath et al. 2022).The approach here is to use simulation to estimate the parameters.\n\nlibrary(estmeansd)\n\nWarning: package 'estmeansd' was built under R version 4.3.2\n\n#data from ANGEL large core trial in NEJM 2023\nres_qe &lt;- bc.mean.sd(q1.val = 13, med.val = 16, q3.val = 20, n = 230)\nres_qe \n\n$est.mean\n[1] 17.03109\n\n$est.sd\n[1] 5.679235\n\n\nThe standard error from the mean can be estimated using get_SE function.\n\nget_SE(res_qe)\n\n$est.se\n[1] 0.5085423\n\n$boot_means\n   [1] 17.63552 16.87911 17.21087 18.37835 17.06111 16.97324 16.28046 16.55440\n   [9] 16.78779 17.52401 17.40252 16.71926 16.48587 16.71277 16.04041 16.64257\n  [17] 16.58581 16.70409 17.00470 16.77632 16.49513 16.60758 16.53661 16.88875\n  [25] 16.75412 16.83439 16.33413 17.02230 15.73261 16.76107 17.12523 17.79897\n  [33] 16.77011 16.57370 17.60460 16.20365 16.68363 15.87538 16.14768 16.74994\n  [41] 16.19223 16.31440 16.67760 16.92631 16.58194 17.63982 16.98768 16.79799\n  [49] 17.81761 17.37270 16.16284 16.95356 17.16436 16.58675 17.34456 16.67638\n  [57] 16.93804 16.10195 16.21442 17.29764 16.87307 17.45995 17.52932 15.64210\n  [65] 16.96973 16.59971 17.13190 17.26831 16.04833 16.02095 17.44736 17.04858\n  [73] 16.84683 16.64707 16.80557 16.92854 16.77352 17.19616 16.24818 17.31548\n  [81] 16.45396 17.83763 16.82433 16.98244 15.90395 16.66238 16.18822 16.21253\n  [89] 16.51635 16.57454 16.30615 17.09296 16.97888 17.23274 16.73336 15.98700\n  [97] 17.59830 16.92804 16.68139 17.03789 17.80884 16.59474 16.38075 17.07557\n [105] 17.37840 17.41764 16.11332 16.45975 17.08136 17.43782 16.08813 16.68766\n [113] 16.77359 17.01063 16.78582 17.50333 16.22841 17.00464 16.51603 16.57905\n [121] 16.50499 17.53342 16.21646 17.57510 17.78446 16.88442 16.01006 16.87215\n [129] 16.90799 16.34539 16.84681 16.67253 17.68135 17.43595 16.46823 17.21019\n [137] 16.87364 16.38951 17.05552 16.81614 16.98286 15.55177 16.15975 16.63015\n [145] 16.26409 16.06795 16.32198 16.57136 17.03695 17.61712 16.82895 16.78381\n [153] 16.58484 17.03923 16.30591 16.33980 17.32183 16.40775 16.95587 17.10189\n [161] 16.62246 16.72860 16.68711 15.58347 16.18357 16.08777 17.02868 16.69470\n [169] 17.07870 16.63690 17.38398 16.01634 15.95888 16.41388 17.80851 16.38382\n [177] 15.69156 16.77367 16.68318 16.86527 16.10676 16.15706 16.50889 16.36605\n [185] 16.75247 16.01440 17.39575 15.18981 16.35576 17.42447 17.48124 17.56520\n [193] 17.22512 17.02615 16.93811 16.51312 16.67147 17.38068 16.15307 16.49037\n [201] 17.11752 16.90119 16.60430 17.66598 16.53494 17.34310 16.80526 16.69798\n [209] 15.80771 16.49769 17.15487 16.67788 17.00431 15.97433 16.01214 17.23063\n [217] 17.59082 16.84253 16.49650 16.40901 16.10897 16.85729 16.98999 16.09189\n [225] 17.78555 16.04921 16.81920 16.30538 17.01252 17.38448 16.07533 16.77924\n [233] 16.57485 17.01044 16.92117 15.73180 16.29477 16.47791 16.71082 16.98238\n [241] 16.45265 16.46845 16.47887 16.36484 17.28316 16.76334 16.54852 17.04406\n [249] 16.30830 16.65307 16.53896 16.99585 17.28862 16.82874 16.04059 16.27546\n [257] 16.49517 17.37655 17.20229 15.99516 16.29900 17.29727 16.87204 16.96384\n [265] 17.02871 15.78851 16.83398 16.27919 17.04721 17.81183 16.97610 16.81628\n [273] 17.41365 15.67946 16.74241 16.95546 17.55695 16.59957 17.00556 17.86511\n [281] 16.73909 16.79515 17.27563 17.67544 16.21734 16.52301 16.30101 17.30404\n [289] 16.14810 17.32469 17.10859 16.46948 16.64824 16.63864 16.05153 16.85465\n [297] 16.60245 16.61558 16.35863 16.49463 16.43246 16.49985 16.87394 16.46223\n [305] 16.47488 15.89976 17.23880 17.10292 16.65583 16.62990 17.36932 15.88606\n [313] 17.04108 16.60860 17.05643 16.35967 16.78415 17.12709 16.53318 16.78671\n [321] 16.42299 16.55961 16.92835 16.64938 15.99347 17.42448 16.25364 16.06757\n [329] 16.63060 16.66295 17.78741 16.32499 16.40065 16.64827 16.89543 15.97865\n [337] 17.07033 16.34788 16.07255 17.29919 16.21117 15.74121 16.33512 17.11237\n [345] 17.22494 17.41037 16.48544 16.02362 16.44553 16.47716 15.64336 16.63065\n [353] 16.50547 17.01520 16.73845 17.05422 16.39539 18.32118 17.29118 15.93682\n [361] 16.90967 16.61836 17.25013 17.43839 16.97900 16.75668 17.35165 16.63716\n [369] 15.72626 15.68603 16.89282 16.41987 16.52084 16.81461 16.15728 16.19314\n [377] 16.69038 16.16387 15.98129 17.45027 16.70883 16.87516 16.80841 17.10411\n [385] 16.93835 18.00270 16.29081 17.00014 17.94558 16.10689 16.74514 16.67284\n [393] 17.22999 17.34522 17.41020 17.09679 16.28240 16.17050 18.03588 16.25104\n [401] 17.02080 17.12400 17.28266 17.32945 17.46015 17.59566 15.98928 17.20812\n [409] 17.04153 16.94049 15.75573 17.12820 16.71464 16.93647 16.71871 16.63509\n [417] 16.38827 16.82314 16.67598 16.68090 16.78620 15.69594 15.88275 15.99963\n [425] 17.19889 17.72937 16.31546 16.64073 16.92743 16.85051 16.48294 17.12233\n [433] 16.01805 16.49165 16.00047 17.09865 17.39316 17.11191 16.15185 16.43014\n [441] 17.07281 18.03534 16.38008 17.01468 15.46617 16.75776 16.95159 16.16294\n [449] 16.78740 16.11695 16.81745 16.71993 16.62638 16.36986 16.78430 15.77901\n [457] 17.02946 16.84058 16.66577 17.30394 16.87219 16.73180 17.91247 16.35430\n [465] 15.99837 16.55911 16.65988 17.50261 16.91418 16.09610 16.96310 16.90376\n [473] 16.74703 16.88080 16.49425 16.85758 16.94536 16.40761 17.44453 17.08466\n [481] 16.45999 17.00926 17.20709 15.90013 16.29447 16.81261 16.18177 15.88284\n [489] 16.69738 16.81766 17.42055 16.47569 17.70481 16.91884 16.16716 16.78693\n [497] 17.26264 16.58866 15.87236 16.74118 17.21004 17.60702 16.14574 17.45618\n [505] 15.45438 16.83508 17.41440 17.29256 16.99985 16.17754 16.16926 17.16698\n [513] 17.31950 15.85966 16.85566 16.67159 16.84635 15.84262 15.91831 16.74595\n [521] 16.68016 16.70523 16.86365 16.44570 16.94236 16.92425 17.11221 15.89467\n [529] 16.85037 17.49086 16.68441 16.34394 17.21010 16.82981 15.86095 16.85721\n [537] 16.88988 17.12353 17.03401 16.65473 17.05932 17.12827 17.22815 16.67830\n [545] 16.88247 17.44021 16.42174 18.05187 17.01090 16.92066 16.34927 16.64515\n [553] 17.27052 16.75925 16.01724 17.30675 17.21511 16.82497 17.16034 16.73830\n [561] 17.68258 16.23067 17.03457 16.26209 16.57740 16.89472 16.62830 17.36408\n [569] 17.57887 17.08335 17.59276 17.17489 16.24146 16.20648 16.81551 16.61297\n [577] 17.25019 16.88230 17.05644 16.92605 17.36335 16.82407 16.10500 16.00510\n [585] 16.55019 15.99863 17.02721 17.25068 17.28039 16.44537 17.41633 16.52770\n [593] 16.90896 16.51966 16.83033 16.43212 16.98141 16.75303 16.51524 17.04739\n [601] 16.28092 16.30280 16.18539 17.67889 16.18995 17.49105 16.81118 16.77694\n [609] 16.86362 16.32696 17.24665 16.73026 17.26363 16.88233 17.23562 16.21596\n [617] 17.74984 16.71512 17.16604 16.64199 16.09664 16.16906 16.90187 16.97685\n [625] 17.15178 16.54976 16.97483 17.57633 17.63707 17.29575 16.62004 16.26821\n [633] 16.90048 16.46858 16.53488 16.88303 16.33543 16.65519 15.91121 17.37419\n [641] 16.54871 17.20405 15.72540 17.37923 16.66641 17.73343 16.80069 17.23884\n [649] 17.68914 16.54335 17.35171 17.36745 16.55015 16.52218 16.31097 16.54047\n [657] 17.01477 17.47476 16.66523 17.64044 17.43188 16.80426 16.95017 17.25578\n [665] 16.30647 15.92574 17.44893 16.24756 16.63328 16.97350 16.51281 15.91377\n [673] 16.28478 16.51554 16.60869 16.12136 16.95053 16.36684 17.01395 16.58090\n [681] 16.67980 16.79415 17.66977 17.39026 16.60184 16.33658 16.69864 16.61545\n [689] 16.50232 16.53048 16.42172 17.42225 16.87931 17.12594 16.45281 16.86652\n [697] 16.89665 16.48098 16.46897 17.40239 16.89106 15.96213 17.16439 17.52855\n [705] 16.48958 17.06655 17.13329 17.28739 16.61747 16.74063 16.48780 16.72196\n [713] 15.67096 16.96464 17.95920 17.77948 17.20162 16.57102 16.51679 16.73687\n [721] 16.52979 16.73184 17.03052 17.06595 16.60853 16.63777 17.37707 17.10643\n [729] 16.67024 16.22050 16.75423 17.40094 17.31104 16.88108 17.06061 16.51256\n [737] 16.15491 16.32424 16.93654 16.34676 16.30064 16.89027 16.71193 16.72260\n [745] 15.76473 16.82838 17.42521 17.01781 16.10249 16.78741 16.54860 17.32025\n [753] 16.70538 16.45798 16.76205 17.63836 16.46850 16.88435 16.95354 16.76717\n [761] 17.16329 17.19789 17.41806 16.81407 17.26117 16.95764 17.16776 16.45423\n [769] 16.96624 16.68892 16.60894 16.50173 18.01009 17.35865 16.75619 16.77393\n [777] 17.51683 18.10540 17.06483 16.67858 16.77933 17.31615 16.84467 16.45502\n [785] 15.92468 16.31591 17.10925 16.45453 16.29809 16.54940 16.73518 16.59128\n [793] 16.05851 17.33279 16.91966 16.81324 16.73489 16.29195 15.97639 17.20638\n [801] 16.74621 16.84711 16.03217 16.68859 15.60375 17.29094 15.90575 16.92184\n [809] 17.63195 16.44975 16.39110 17.81167 17.95380 16.28888 16.79507 17.20125\n [817] 17.70115 16.42255 16.05503 17.10535 16.94205 16.99315 16.70561 16.76432\n [825] 16.91083 16.75844 16.88353 17.10415 17.02177 17.12964 16.20754 17.30404\n [833] 16.70336 16.76151 16.27224 16.18749 16.40985 16.31122 16.72563 16.72632\n [841] 16.87992 16.04856 16.24297 17.00100 16.48811 16.54128 16.29181 16.58835\n [849] 16.54464 17.06069 16.62673 16.79306 16.11072 16.53187 16.26863 16.75475\n [857] 15.63740 17.36190 17.30887 17.63359 17.31561 16.74851 17.97567 16.15102\n [865] 16.88766 16.35937 17.17841 16.50045 16.46879 17.05660 17.73220 16.68742\n [873] 16.99721 17.45652 16.07807 17.27749 16.67581 17.21957 17.05854 16.91974\n [881] 17.00868 16.84926 16.91927 16.25051 16.19571 17.26171 16.27868 16.95169\n [889] 17.49690 16.43109 17.50804 16.98057 16.69267 16.67760 16.48284 16.94106\n [897] 17.13138 16.92612 16.75502 17.03534 15.51020 16.37590 16.50584 16.66183\n [905] 16.49753 16.82960 15.42539 16.23457 17.28727 15.95610 17.11997 17.22106\n [913] 16.57169 17.78232 16.75053 17.16121 15.96613 16.46361 17.29043 15.83573\n [921] 15.95731 16.76620 16.39871 16.64773 17.43062 16.17167 17.22745 16.95708\n [929] 17.08048 16.99795 16.56256 16.75174 16.60244 16.45106 16.76731 17.62834\n [937] 18.04985 17.53545 16.81717 16.36532 16.27106 17.09588 17.39697 16.96087\n [945] 15.88486 17.37636 17.69110 17.27469 16.01038 16.06225 17.15042 16.47281\n [953] 16.82237 17.01921 16.29794 17.33601 16.02642 16.06303 16.67332 17.37899\n [961] 17.72199 17.34629 16.00766 16.88426 16.05943 17.61250 16.84538 16.93341\n [969] 16.12361 16.80899 17.14116 16.56171 17.19012 16.39437 16.69622 15.84463\n [977] 15.63147 17.22742 16.72179 16.11336 17.88273 17.78646 16.16507 15.73022\n [985] 17.61583 16.63024 16.92645 15.98756 16.89703 16.94030 16.26070 16.47421\n [993] 16.23561 16.86041 16.26407 17.02365 17.27261 17.19724 16.17817 16.38339\n\n$boot_sds\n   [1] 6.056741 5.213747 5.902476 7.169889 4.760362 6.068524 6.081015 5.119545\n   [9] 4.927697 5.569590 5.426484 6.170699 5.639139 4.974583 4.322502 4.957281\n  [17] 5.195258 5.958553 5.447704 5.293951 5.052861 5.267866 5.388314 5.857193\n  [25] 5.256467 6.127236 5.463299 4.911669 4.640675 5.296148 5.590095 6.806176\n  [33] 5.203736 5.412370 6.568382 4.110142 5.195231 4.709093 5.122890 5.417854\n  [41] 4.816023 4.651345 5.241922 5.801091 5.554613 5.978516 6.504411 5.368832\n  [49] 6.170706 5.611463 5.085125 5.874528 5.685271 5.326262 6.444264 6.289633\n  [57] 5.857746 4.981622 4.831596 5.920813 5.973513 5.887680 6.239226 5.043959\n  [65] 6.764114 5.783540 6.305668 5.145124 4.950898 4.766705 5.492023 5.804073\n  [73] 5.796790 4.823568 5.363788 5.172880 5.353465 5.656737 4.853553 5.374593\n  [81] 4.752940 5.865755 4.728040 5.689990 5.119064 5.102319 5.305740 5.699346\n  [89] 5.241966 5.082082 5.250624 5.616849 5.411459 5.948924 5.462404 5.591316\n  [97] 6.932250 5.505007 5.947980 5.977642 6.055582 5.663435 4.965966 5.743761\n [105] 5.681822 5.259006 5.279769 5.546363 5.857552 5.509247 4.577881 5.111263\n [113] 5.417195 5.467720 6.291290 6.449037 5.432694 5.581010 5.211197 5.457356\n [121] 4.925238 5.891856 5.448168 6.465838 6.211603 5.321370 5.316371 5.530511\n [129] 5.455778 5.758156 5.892604 5.845697 5.993668 5.969961 4.786535 6.442191\n [137] 4.890682 4.982662 5.989926 5.322238 5.363692 4.504860 5.361399 6.233237\n [145] 5.635964 4.912339 5.045124 5.188588 5.508130 6.756996 5.443810 5.018430\n [153] 5.753702 6.017025 5.191607 4.806607 6.111525 4.943212 4.915038 6.405865\n [161] 5.007161 5.378051 5.664393 4.380043 4.705554 5.131152 5.599748 5.427096\n [169] 6.357610 5.741957 6.324994 5.046972 5.210314 5.101075 5.793980 4.985792\n [177] 5.656340 5.754991 5.306868 5.697950 5.084930 5.109784 5.463584 5.737425\n [185] 5.356641 4.873075 6.326409 3.589852 5.034065 5.510263 6.529939 5.791265\n [193] 5.047304 5.213920 5.945043 4.266233 5.147504 5.387380 3.757362 5.573362\n [201] 5.788395 5.039780 5.098705 5.775296 5.802627 5.749238 5.551491 5.965704\n [209] 4.937594 4.745325 5.535876 5.601711 5.804621 5.629498 4.799867 6.163589\n [217] 5.565544 5.167937 6.435729 4.974566 5.162915 5.595978 5.415268 5.999200\n [225] 6.467648 6.369713 6.081447 4.858463 5.890976 6.612484 5.146275 5.226877\n [233] 5.142785 6.070448 5.820696 5.109648 5.425036 5.566726 5.725683 6.123589\n [241] 5.183558 5.301968 5.374230 5.714335 5.480546 4.796982 4.942470 5.397422\n [249] 4.904661 5.093931 4.925380 6.355419 6.529766 4.975264 5.326649 5.272093\n [257] 5.400537 6.071203 5.004830 4.448178 5.819002 5.994743 4.951246 5.845142\n [265] 4.781654 5.134423 5.959175 4.354360 4.977988 5.966635 5.654619 5.168192\n [273] 6.314548 4.514891 5.009464 4.636870 5.416897 5.324284 5.811136 5.876990\n [281] 5.230589 6.056841 5.092891 6.334339 5.249366 4.913469 5.320937 6.366168\n [289] 5.656229 5.743542 5.171785 4.901684 4.858967 6.242310 5.126084 6.148678\n [297] 5.842866 5.010516 4.910833 4.879856 4.809715 4.563049 6.006769 6.302921\n [305] 5.081469 4.951517 5.607203 5.361945 6.267967 5.660513 6.249634 4.764647\n [313] 5.201280 5.021209 5.004246 5.781393 5.154447 5.592961 5.572255 4.909260\n [321] 5.581487 4.742172 4.708312 5.301561 5.251651 5.807659 4.907961 4.612651\n [329] 5.101010 5.379095 6.350659 5.302956 5.527672 4.882308 5.310570 5.049866\n [337] 6.506573 5.070533 4.993233 5.849624 5.075301 4.727037 5.064095 6.046110\n [345] 6.067721 5.825397 4.950972 4.855466 5.340220 5.226282 4.858584 5.388112\n [353] 5.598678 4.375292 5.984622 5.458428 5.484467 6.839842 6.319415 4.893833\n [361] 5.940524 5.311512 6.015898 5.118725 5.670243 5.479731 6.268903 4.821310\n [369] 5.511695 5.275749 5.657222 4.788396 5.565805 4.997382 4.477941 4.115370\n [377] 5.095312 4.860635 4.909603 5.770317 5.261597 6.104451 5.748543 5.732272\n [385] 4.978129 5.602711 4.834684 5.427356 5.874491 4.538906 5.065238 5.314658\n [393] 5.796301 5.998860 5.966505 5.149422 5.713563 5.459656 7.004569 4.748827\n [401] 6.073852 5.800947 6.319809 5.401842 5.477487 6.437539 4.433041 6.342293\n [409] 5.712900 6.577496 4.544939 5.700422 5.016599 5.457674 4.801518 5.452891\n [417] 5.130541 4.854769 5.431223 5.727219 5.151599 4.952250 5.184223 4.644023\n [425] 6.122185 5.975738 4.646833 5.654675 5.050837 4.730337 5.064245 6.020249\n [433] 4.339115 4.628110 4.894016 4.945575 6.077845 5.907718 5.438279 5.158604\n [441] 5.693941 6.993993 5.610139 5.933818 4.579090 5.148970 5.472676 4.522705\n [449] 5.076979 4.468243 5.536490 5.277609 6.146037 5.075150 5.166651 4.477236\n [457] 5.450151 6.145710 5.259955 5.713303 5.746360 5.941317 5.389201 4.947068\n [465] 4.343878 6.000846 5.328782 6.484484 5.862195 4.940557 5.055302 6.324059\n [473] 5.860013 5.620876 5.146650 4.779215 5.324630 5.109189 5.343300 5.743748\n [481] 5.495191 5.915198 5.327855 4.702432 5.614464 5.586267 5.280658 4.425061\n [489] 5.668272 5.491833 6.400497 4.830851 6.524642 5.431593 4.959592 4.539267\n [497] 5.786475 5.033967 4.659342 5.204812 5.279754 5.372472 4.919677 5.100160\n [505] 4.714135 5.635332 6.638590 5.537651 5.755955 4.633317 5.506918 5.342096\n [513] 5.307616 4.450802 5.277366 5.145560 5.889413 4.277152 4.964935 5.850624\n [521] 5.996680 5.443877 5.462540 5.145452 5.471913 4.341219 3.970597 4.670697\n [529] 5.184864 5.791498 5.666697 4.945126 5.191879 5.152184 4.981145 5.559733\n [537] 5.046329 5.708033 5.603743 5.097890 5.513832 5.692131 5.808534 5.093389\n [545] 5.416598 5.926895 6.324187 6.380100 5.400291 4.765430 4.031235 5.045223\n [553] 5.917768 5.531825 5.363569 6.185663 6.053648 4.898830 5.010852 5.261617\n [561] 5.075454 5.470097 6.195921 5.363234 5.199629 5.875579 5.125680 6.302583\n [569] 5.679879 4.765348 6.933633 5.481037 5.185202 5.400636 5.159915 5.264336\n [577] 6.193163 5.403461 5.166113 5.939919 5.583905 5.287200 3.981610 4.786695\n [585] 4.852031 5.370548 6.384227 6.122689 5.575492 5.174506 5.223361 4.940634\n [593] 5.569172 4.731739 5.005344 4.864539 5.459836 5.047756 5.971062 5.622524\n [601] 4.840468 5.339127 4.443515 6.355575 5.397552 5.945264 5.543151 5.002016\n [609] 5.547799 5.865403 5.860633 5.408968 6.012204 5.449423 5.618880 5.197064\n [617] 6.062908 5.624336 5.651875 5.292816 4.458503 5.037801 5.419747 5.492930\n [625] 5.625775 5.406194 5.658574 6.450689 6.469434 5.549169 6.101378 4.540134\n [633] 5.259111 5.025476 5.233151 5.127815 4.603570 4.817447 4.495751 5.780170\n [641] 5.451868 5.751063 4.203112 5.427740 6.417331 5.777402 4.364933 5.958015\n [649] 5.746141 5.786433 5.323809 5.979053 4.678767 4.820718 4.981232 4.256734\n [657] 4.849054 5.286219 4.998647 6.124437 4.803946 4.854828 6.558722 5.553511\n [665] 5.629764 4.384870 5.581445 5.591875 5.160486 5.444284 4.943310 5.270554\n [673] 4.990208 4.956123 5.064318 4.806967 5.749104 4.566069 5.398893 4.558318\n [681] 5.598853 4.444345 7.174437 5.701133 5.380573 4.934336 5.407648 5.316219\n [689] 5.382025 5.302965 4.394075 5.354918 6.793008 5.467426 4.717913 5.968466\n [697] 5.603292 4.996312 4.938925 4.573071 5.137026 4.528046 6.045621 5.863035\n [705] 5.218186 5.229258 5.950811 6.283306 5.044920 5.177027 5.277721 4.859208\n [713] 4.351867 5.416365 5.828464 6.619745 5.745728 5.069754 4.745913 4.985595\n [721] 4.456818 5.200976 5.880361 4.980410 4.987664 5.909517 5.617092 5.622905\n [729] 6.075808 5.676389 5.321377 6.332320 6.018670 5.430506 5.749843 4.844204\n [737] 4.642405 4.963982 5.522285 5.925235 4.884240 5.276718 5.512968 5.814385\n [745] 5.126908 5.622638 6.219246 5.548099 5.725805 5.382226 5.710732 5.384670\n [753] 5.209543 5.264657 5.003350 6.239655 4.320836 5.010982 5.728807 5.228848\n [761] 5.351399 5.245250 5.585234 5.869449 5.149486 6.300101 5.315750 4.580842\n [769] 5.589779 5.271780 5.170633 5.098643 6.621591 4.846158 5.125283 5.863942\n [777] 6.316473 6.156023 6.043082 5.157973 5.589664 5.371213 5.399088 5.044246\n [785] 4.576846 4.995992 6.214560 4.891626 4.522202 4.858920 5.347669 5.643482\n [793] 5.384285 5.674924 5.700100 5.413747 3.789611 5.151326 5.292214 6.482386\n [801] 5.759514 4.779323 4.881720 5.982781 4.352779 5.372637 4.762687 5.491111\n [809] 5.468019 4.744731 4.236294 5.691425 5.756013 4.836807 6.221859 5.699388\n [817] 6.776164 4.955605 5.346409 6.181206 5.257216 6.296385 4.887499 5.392892\n [825] 4.709317 6.632168 4.703149 5.542346 5.882456 5.777275 4.621278 5.517518\n [833] 5.366177 5.051838 5.715080 4.932621 5.808006 4.721429 5.405572 5.017073\n [841] 5.310778 4.884083 3.954782 5.254395 4.933062 4.912252 5.266502 5.257018\n [849] 4.801459 4.708130 4.933793 5.676161 5.112535 4.679686 5.125262 5.882474\n [857] 4.653792 4.904053 5.718038 6.363428 5.381595 4.131815 6.069447 4.988944\n [865] 5.665226 4.681931 5.814061 5.454344 4.934491 5.028327 6.143784 5.286777\n [873] 5.340339 5.582643 5.210916 5.688855 5.568147 6.216612 5.355080 5.340671\n [881] 6.219361 4.893092 5.696389 5.719802 4.772792 5.775188 5.004185 5.887932\n [889] 5.473052 5.010622 5.596674 5.770195 4.522743 5.332102 5.210471 4.093167\n [897] 6.022440 5.999420 5.264710 5.802940 5.236029 5.180847 4.595033 4.986894\n [905] 5.335615 4.935902 4.950636 5.259564 5.608624 4.739127 4.756623 6.231335\n [913] 5.101330 6.306926 4.719972 5.349232 4.436680 5.478713 5.979127 4.802901\n [921] 4.720857 5.228361 5.623395 5.610893 5.963917 5.103616 4.946484 5.993472\n [929] 6.758515 6.426608 5.306143 4.375291 5.519104 5.390451 5.298465 5.146285\n [937] 7.083273 6.526111 5.896244 5.102745 5.233789 6.133870 5.659107 5.229765\n [945] 4.660460 6.176343 6.952063 5.613743 4.528696 5.084600 5.969682 5.653292\n [953] 5.301781 5.462190 5.846003 6.018024 4.235123 4.449262 5.393023 6.044202\n [961] 7.193472 5.647921 4.588331 6.301511 4.984883 6.060567 5.837099 5.372416\n [969] 5.848206 5.691978 5.602031 5.998913 5.543951 4.829520 5.591493 5.241383\n [977] 4.454995 5.059309 5.076342 4.527864 7.194828 6.543023 4.741713 4.564505\n [985] 5.405975 5.136176 5.650032 4.734949 4.942184 5.059526 4.852059 5.386645\n [993] 5.199604 5.830754 4.650790 5.485193 5.704031 5.670459 4.689619 4.807152\n\n\nThe estmeansd library uses Box-Cox method for estimating mean and sd when the data available are the median, minimum and maximum values.\n\nlibrary(estmeansd)\n\nres_bc &lt;- bc.mean.sd(min.val = 13, med.val = 16, max.val = 42, n = 230)\nres_bc \n\n$est.mean\n[1] 16.69078\n\n$est.sd\n[1] 3.620106\n\n\nAgain, the standard error from the mean can be estimated using get_SE function.\n\nget_SE(res_bc)\n\n$est.se\n[1] 0.3164941\n\n$boot_means\n   [1] 16.61950 16.60544 16.98685 16.37338 17.22550 16.53991 16.96921 16.42799\n   [9] 16.79853 16.37300 16.46512 16.42107 16.76768 16.65161 16.88464 17.34063\n  [17] 17.02428 16.67472 16.52115 16.49748 16.83511 16.03474 16.50927 16.41899\n  [25] 17.19008 16.62758 16.56849 16.51621 16.59377 16.79799 17.15780 16.64888\n  [33] 16.43999 16.43105 16.84439 17.09933 16.14720 17.10175 16.93661 16.83493\n  [41] 17.19851 16.57157 16.91155 16.78373 16.58965 16.60323 16.46596 16.33361\n  [49] 17.14390 16.92362 16.79613 16.52471 16.76237 17.10626 16.68339 16.74075\n  [57] 16.82590 16.65960 16.89891 16.41728 16.93513 16.40843 16.56640 16.63776\n  [65] 16.69542 16.73260 16.45204 17.02676 16.33247 17.38994 16.21774 16.60234\n  [73] 17.10086 16.83138 16.17054 17.12027 16.96872 16.30576 17.47867 16.88597\n  [81] 16.39533 16.22542 16.08966 16.12750 16.80409 16.50629 17.27337 16.80650\n  [89] 16.65311 16.99611 16.91171 16.61373 16.92425 16.62230 17.06121 17.27548\n  [97] 16.35929 16.54343 16.72730 17.25368 16.67899 16.26994 16.64170 16.08557\n [105] 16.64277 16.82771 16.88511 16.77592 16.95999 16.54840 16.30886 17.33406\n [113] 16.33664 16.68176 16.88799 17.37785 16.82170 17.05243 16.55098 17.07120\n [121] 16.67871 16.43541 16.76094 16.92627 16.56860 17.12882 16.84691 17.02399\n [129] 16.66598 16.58126 16.57442 16.39630 16.73956 16.37509 17.01046 16.55296\n [137] 17.41890 16.96223 17.09172 16.96084 17.04858 16.68308 16.58476 16.64310\n [145] 17.30123 16.85137 16.75863 16.40464 17.51963 17.02571 16.86999 16.38298\n [153] 16.37584 16.61673 16.82036 16.63085 16.68524 17.28534 16.39467 16.47194\n [161] 16.83710 16.40145 16.77101 16.87334 17.05066 16.94943 16.10628 16.20021\n [169] 16.82599 16.33404 17.26177 16.90358 17.21385 16.66521 17.48259 16.86498\n [177] 16.90133 16.56536 16.54127 16.82765 16.50710 16.52007 16.43771 16.56699\n [185] 16.68056 16.01148 16.63919 17.03173 16.53091 16.40270 16.52274 17.20118\n [193] 17.03042 17.24418 16.71294 17.72162 16.73555 17.05308 16.95042 16.66948\n [201] 16.26383 16.86109 16.49322 16.05457 16.43777 17.13530 16.21572 17.11896\n [209] 16.12419 16.58550 16.54040 17.25704 16.67401 16.64811 17.22129 17.00765\n [217] 16.06591 16.73037 16.74179 17.03578 16.39595 16.91102 16.18880 17.42751\n [225] 17.03672 16.83501 16.20934 16.21074 16.86764 16.20755 16.56512 16.68130\n [233] 17.42064 16.55432 16.22931 16.56924 16.88691 16.54002 16.27785 16.48836\n [241] 17.03065 16.59928 16.90509 16.73666 17.01153 16.89132 16.85471 16.83844\n [249] 16.24568 16.78431 16.40237 16.68383 16.71415 16.54202 16.53638 16.97315\n [257] 16.87934 16.89119 16.64040 16.42743 16.85487 16.46166 16.82064 17.06939\n [265] 16.52190 16.83825 16.27107 16.85880 17.42329 16.84253 16.46431 16.92531\n [273] 16.83389 16.50754 16.34825 16.95428 16.11012 16.47015 16.44144 16.69954\n [281] 16.76518 16.32170 16.71646 16.54629 16.55674 16.97551 16.49697 16.70925\n [289] 17.05182 16.44889 16.83577 16.99714 16.25566 16.49456 16.54652 16.80030\n [297] 17.04938 16.56339 17.21699 16.73682 17.08203 17.27258 16.55013 16.82122\n [305] 16.60070 16.67442 16.55545 16.34687 16.91974 16.54617 16.48286 16.60606\n [313] 17.03701 16.84527 16.22599 16.98578 16.85766 17.02041 16.78629 16.49124\n [321] 17.30150 16.72716 16.96854 16.86865 17.25556 17.32631 16.27701 16.74731\n [329] 16.81829 16.69730 16.65410 16.65969 17.08980 16.54420 16.64776 16.57611\n [337] 16.17574 16.30874 16.81722 16.44820 16.38684 16.94403 16.68352 17.39357\n [345] 17.01459 16.66292 17.26149 16.41535 16.29033 16.73009 16.58478 16.57832\n [353] 16.88417 16.46951 16.36591 16.67283 16.87873 17.06480 16.58238 16.46304\n [361] 16.95326 16.37672 16.80326 16.17370 16.30970 16.84445 16.41480 16.82225\n [369] 17.19023 16.72907 17.05970 17.29891 16.50281 16.42930 16.63923 17.07283\n [377] 16.36992 16.41310 16.33938 17.33010 16.48198 16.50842 16.63009 16.60705\n [385] 17.05950 16.97054 16.64788 16.78220 16.93595 17.22851 16.48305 16.38487\n [393] 16.74940 17.55224 16.65947 17.36554 16.21457 16.17941 16.62049 16.49673\n [401] 16.74443 17.01362 16.68746 16.85938 16.80883 16.51906 16.20755 16.69993\n [409] 16.87220 17.06124 16.72650 16.51577 16.74351 16.93603 16.94076 16.81912\n [417] 17.13793 17.09238 16.51380 17.54956 17.16610 16.76158 16.68052 16.75421\n [425] 17.21187 16.43488 17.29382 16.18978 16.68206 16.80612 16.48610 17.01557\n [433] 16.94644 16.62436 16.61528 16.37404 17.23983 16.70706 17.05811 17.14014\n [441] 16.56248 16.40032 16.67330 16.67429 16.47447 16.55634 16.95253 16.32640\n [449] 16.69114 16.95695 16.22040 16.66036 16.48263 16.58726 16.77656 16.69457\n [457] 16.32322 17.03014 16.60256 16.96545 16.59547 16.40034 16.68862 16.03295\n [465] 16.79791 16.75250 16.51765 16.59963 16.60963 17.04489 16.61483 16.29046\n [473] 16.82328 16.14801 16.85790 16.49270 16.80258 16.64292 16.67670 16.61538\n [481] 16.57359 16.14067 16.58160 17.17841 16.65093 16.42826 16.65278 16.65886\n [489] 16.44028 17.31336 16.57838 17.51378 16.79215 16.55075 16.28995 16.52784\n [497] 16.67763 17.50877 16.34263 16.74621 16.51012 16.81807 16.72354 17.08604\n [505] 16.75039 16.55804 16.43936 17.36826 16.62960 16.78908 17.00681 16.53221\n [513] 16.53602 16.47194 16.80416 16.80880 16.41508 17.04593 16.71024 16.39722\n [521] 16.75359 16.39501 16.88696 17.47570 16.85410 16.50587 16.65014 16.63170\n [529] 16.09438 17.04715 16.52366 17.26250 17.47242 17.21561 17.10624 16.89520\n [537] 16.46606 17.47512 16.47407 17.65114 16.96732 16.75003 16.65209 16.58658\n [545] 16.50700 16.44418 16.92918 16.69860 16.48330 16.48420 16.13814 16.75481\n [553] 17.08138 16.91870 16.37350 16.50524 16.66543 16.55565 16.74338 16.66825\n [561] 16.75924 17.23691 16.81857 16.47850 16.82323 16.75203 16.29340 17.30413\n [569] 17.17691 16.48172 16.81652 16.77127 17.12834 17.21529 16.50047 16.67687\n [577] 16.87494 16.41677 16.84109 16.36865 16.51482 16.61041 16.10348 16.77398\n [585] 16.52074 16.97745 17.27354 16.22663 16.84522 16.72585 16.81132 16.59795\n [593] 16.73640 16.81020 16.77819 16.12285 17.25710 17.01036 16.76438 16.87732\n [601] 16.10096 16.38565 17.04781 16.65993 16.50016 16.37622 16.35879 16.51746\n [609] 16.49357 16.60857 16.80879 16.98623 17.43997 16.78724 16.97825 16.50019\n [617] 16.76250 16.78444 16.36914 17.29075 16.97211 16.83616 16.93855 17.20322\n [625] 16.61122 17.06972 16.89402 16.51288 16.71093 17.19952 16.47954 16.75162\n [633] 16.10625 16.90353 17.08770 16.71976 16.53918 17.08954 16.90385 16.91872\n [641] 16.39442 17.52879 16.97059 17.28404 17.12898 16.15378 17.03340 17.17341\n [649] 16.52632 17.16493 16.49777 16.57337 17.11456 16.49804 16.32792 17.28744\n [657] 16.47844 16.35964 16.42806 16.81276 16.60159 16.47683 16.71923 16.19262\n [665] 16.92719 16.51524 16.49262 16.97231 16.54042 16.65453 16.62713 16.99949\n [673] 17.21557 17.09659 16.87726 16.40236 16.67506 16.57430 17.09593 16.51877\n [681] 16.72056 16.16768 16.46834 16.16631 16.73705 16.75070 16.74823 16.37228\n [689] 16.59116 17.18745 16.51885 16.69063 16.67391 16.16896 16.77256 17.19292\n [697] 16.79256 16.24422 16.77800 16.67037 16.55864 16.83749 16.42525 16.59487\n [705] 16.25174 17.07039 16.78256 16.84201 16.33419 17.01221 16.17462 17.17810\n [713] 16.74995 16.82128 16.76198 16.13914 17.11144 16.57436 16.15232 16.37852\n [721] 17.16330 16.44211 16.78330 17.09387 16.86543 16.51587 16.29547 17.08486\n [729] 16.51237 16.31340 16.94488 16.96698 16.49482 16.68760 17.09166 16.60710\n [737] 16.53902 16.42624 16.86328 16.30312 16.77689 16.30885 16.65821 16.47750\n [745] 16.54440 16.94809 17.21016 16.91629 16.54562 16.84132 16.64041 16.72993\n [753] 16.60372 17.00476 16.65480 17.48636 16.51751 16.88196 16.55344 16.98828\n [761] 17.14319 16.08553 16.49946 16.51281 16.37175 16.68992 16.96466 16.80831\n [769] 16.57881 17.22682 16.58003 17.13212 16.11346 16.77800 16.05977 16.83242\n [777] 16.63620 16.63004 16.97802 17.16623 17.11861 16.83805 16.96473 16.40651\n [785] 16.87848 16.57562 16.37510 16.69987 16.94361 17.17232 16.82531 17.09458\n [793] 16.38501 16.08604 17.10672 16.71201 16.67890 16.85420 16.24225 17.18386\n [801] 17.04548 16.79798 17.61376 16.79327 16.82701 16.79519 17.63664 16.62258\n [809] 16.91168 16.90401 16.87316 16.39112 17.06192 16.06099 16.70497 16.37914\n [817] 16.76816 16.59657 16.80608 16.63386 16.82624 16.71995 16.81858 16.19230\n [825] 16.47479 16.99393 16.69931 16.51724 16.83309 16.59971 16.89626 16.81529\n [833] 17.05585 16.77545 16.98455 16.20689 17.31505 16.93538 17.35084 16.71029\n [841] 16.98070 16.60873 16.69574 17.05444 16.51899 17.08946 16.80817 16.18638\n [849] 16.83056 16.58756 16.55804 16.65416 16.16510 16.21905 16.30346 16.79421\n [857] 17.06097 16.77545 16.37005 16.91545 16.95483 17.09106 17.05118 17.03768\n [865] 16.78552 16.11071 16.80426 16.43478 16.41446 16.70347 16.46562 16.76923\n [873] 17.13539 16.79552 16.56757 16.36508 16.60601 16.19475 16.78140 17.16741\n [881] 16.39884 16.92804 17.35341 16.78923 16.37767 16.76492 16.86671 16.51310\n [889] 17.10594 16.72310 16.75434 16.48316 16.34593 16.41818 16.90155 16.98137\n [897] 16.61488 16.47240 16.88310 17.06982 16.60667 16.70527 16.83325 16.66075\n [905] 16.26776 16.83576 16.40616 16.59572 16.83047 16.44360 16.80922 16.74596\n [913] 16.80258 17.17521 16.95385 16.82365 16.51872 17.01162 16.86432 16.93391\n [921] 17.12149 17.06600 16.62506 16.60597 16.83995 16.67435 16.77695 16.63273\n [929] 16.63247 16.83365 16.45602 16.61998 16.19925 16.94297 16.78096 17.44446\n [937] 17.03420 16.74712 16.48054 17.02195 16.23214 16.74617 16.74238 16.93085\n [945] 16.38729 17.42168 16.62369 16.26301 17.42470 16.63969 17.40098 16.11769\n [953] 16.36656 17.17702 17.14599 16.39195 16.27929 16.75580 16.69729 16.44340\n [961] 17.07319 16.76360 16.25003 16.63445 17.37516 16.96659 16.39822 16.46239\n [969] 16.87120 16.95558 16.60808 17.56040 16.78276 16.40824 17.02433 16.40897\n [977] 16.98582 17.18296 16.50015 16.63085 16.99670 16.14841 17.16677 16.58928\n [985] 17.31038 17.01654 16.47989 16.13499 16.95507 16.58247 16.60206 17.05872\n [993] 16.67785 16.63747 16.80655 16.97838 16.70177 16.34688 17.06286 16.46086\n\n$boot_sds\n   [1] 3.359512 3.040868 3.462556 3.104239 3.876423 3.592225 3.386621 3.085985\n   [9] 3.727133 3.720339 3.575315 3.397902 2.982521 3.046177 3.292880 4.114856\n  [17] 3.127077 3.930764 3.441695 3.146310 3.828936 3.402520 3.449742 4.209891\n  [25] 3.849349 3.097083 3.128389 3.663157 3.087440 3.342224 3.999224 3.647124\n  [33] 3.473777 3.583949 3.389349 3.464283 3.133440 4.654132 3.791236 3.586116\n  [41] 4.065361 3.231420 3.650451 4.241677 3.339990 3.513514 4.027122 3.338277\n  [49] 3.081157 4.141428 3.521367 3.606528 3.367824 3.436759 3.836010 3.833287\n  [57] 3.502881 3.303778 3.227869 3.437878 3.471454 3.556926 3.512129 3.543159\n  [65] 3.199027 3.895358 4.028302 3.597079 2.988051 4.673688 3.342388 3.133444\n  [73] 4.439630 3.087020 3.184433 3.436478 3.101436 3.281487 4.317846 3.494574\n  [81] 2.975399 3.432809 3.186014 3.294233 2.982423 3.411344 3.305671 3.247563\n  [89] 3.540794 4.341951 3.121772 3.838589 3.596128 3.030218 3.170706 3.739614\n  [97] 3.376947 3.849478 3.850850 3.557545 4.003960 3.423089 3.926333 3.109470\n [105] 3.518802 3.772823 3.572237 3.412155 3.110599 3.197192 2.758756 3.631654\n [113] 3.418579 3.431561 3.273387 3.809467 3.856216 3.338645 2.918216 4.040096\n [121] 3.538572 3.598832 3.909049 3.427526 3.694060 2.892438 2.880896 3.332710\n [129] 3.267297 3.514022 3.534868 3.317010 3.518988 2.965998 3.223820 3.619296\n [137] 4.032751 3.481499 4.369444 3.252557 3.865549 3.153899 3.773947 2.916339\n [145] 3.363224 3.200528 3.380630 3.464204 4.344313 2.838770 3.534867 3.246998\n [153] 3.564789 3.121635 3.330866 3.509488 3.110390 3.300173 3.703586 3.397200\n [161] 3.864412 3.105346 4.415797 3.059690 3.901487 4.235145 2.574929 2.996730\n [169] 3.765193 3.313775 3.761127 3.172749 3.192972 3.597856 3.736988 3.987327\n [177] 3.913627 3.662701 3.424988 3.271884 3.311052 3.464858 3.536167 3.826471\n [185] 3.457091 2.965435 4.061428 3.922787 3.886334 3.250803 3.085545 3.986150\n [193] 3.583137 3.394111 2.983255 4.608490 3.719781 3.512695 3.606682 3.824318\n [201] 3.480786 3.470998 3.832538 3.488885 2.872989 4.019750 3.459952 3.914302\n [209] 3.625603 3.800077 3.617080 3.815142 2.917614 3.531303 4.221237 4.076579\n [217] 3.208644 3.762085 3.018554 3.675838 3.209927 3.725472 3.324037 3.213128\n [225] 3.450567 3.465805 3.106620 3.993073 3.398860 3.003784 2.930525 3.189928\n [233] 3.523248 3.398149 3.646227 3.768362 3.799771 2.973070 3.111828 3.224097\n [241] 4.637024 3.312187 3.021133 3.491375 3.803699 3.564044 3.066595 4.139760\n [249] 3.681278 3.175941 3.125857 4.190176 3.487544 3.030583 3.427032 3.728431\n [257] 3.231946 3.680510 3.916988 2.959757 3.585928 2.984442 3.661827 3.934464\n [265] 2.873875 3.516144 3.646066 4.147384 3.397698 3.988779 3.878010 3.212551\n [273] 3.655440 3.244604 3.584199 3.762365 3.270253 4.122492 2.897959 3.570981\n [281] 3.331907 3.172335 3.046108 3.844860 3.387932 3.476016 3.322099 3.609761\n [289] 3.173990 3.505029 3.212564 3.570269 2.974546 2.951509 3.951655 2.810755\n [297] 3.393132 3.745622 4.558847 3.493160 3.704375 3.296983 3.130915 3.105294\n [305] 3.250690 3.523195 5.214903 3.878867 3.458924 3.108578 3.043845 3.455394\n [313] 3.717549 3.641197 3.386021 2.972983 3.335146 4.968917 3.872369 3.590170\n [321] 3.926012 3.726851 3.249402 3.331239 4.565762 4.319867 2.889038 3.370722\n [329] 3.734241 3.124420 3.363353 4.576976 3.947433 3.051844 3.032660 3.212016\n [337] 3.328143 3.533983 4.129816 3.650796 3.417366 3.402936 3.154091 4.037794\n [345] 3.780002 3.660537 3.578824 3.873645 3.397929 3.087632 3.854529 3.334795\n [353] 3.602339 3.713030 3.255074 3.007591 3.262524 3.265733 3.723888 3.270590\n [361] 3.278843 3.495700 3.294747 3.185054 3.139003 3.158570 3.169987 3.346556\n [369] 3.371347 4.025090 3.278166 3.515802 3.044396 2.849783 3.901247 4.114475\n [377] 3.206502 4.016281 2.786404 3.904362 3.324364 2.846576 3.486459 3.989800\n [385] 3.752945 3.623547 3.367436 3.646172 4.080464 3.639196 3.970243 3.123117\n [393] 3.578421 4.117916 3.579293 4.135139 3.584619 3.188931 4.273832 2.863964\n [401] 3.678275 3.966714 3.408234 3.634607 3.719216 3.446407 3.642701 3.455120\n [409] 3.315363 3.770770 3.437189 3.421616 3.602692 3.354293 3.747534 3.529308\n [417] 3.297980 4.023264 3.712733 4.352552 3.367661 3.486326 3.118399 3.650840\n [425] 4.414124 3.570202 4.209663 2.969094 3.798286 4.114198 3.260321 3.891911\n [433] 3.232976 2.898695 3.558132 3.041480 3.850465 3.975216 4.070209 3.282120\n [441] 3.409057 3.228269 3.050020 3.001906 3.723391 3.412722 3.558188 3.520703\n [449] 2.909927 3.691195 2.885600 3.947514 2.811257 4.165738 3.484141 3.676974\n [457] 3.045785 3.632209 2.906327 3.235873 3.573705 3.020128 3.325559 3.344297\n [465] 3.404968 3.882903 3.368968 3.648994 3.842286 3.828935 3.112457 3.388627\n [473] 3.284112 2.966155 2.978417 3.253405 2.977951 5.001792 3.524653 3.788371\n [481] 3.206517 3.286890 3.698876 3.721897 3.425936 3.720272 2.844903 3.794871\n [489] 3.363760 4.176133 3.506771 4.835397 3.317035 3.627389 3.994385 3.552659\n [497] 3.683888 4.328764 3.488215 3.755673 3.549893 2.980475 3.285548 3.748840\n [505] 3.477916 3.411501 3.701782 4.324272 3.374821 3.695123 4.190869 3.974619\n [513] 3.662039 4.090460 3.453125 3.368915 3.250412 3.968169 3.886343 3.475103\n [521] 3.439855 2.754528 3.597039 3.542604 3.257713 3.017359 3.188577 3.685025\n [529] 3.272770 3.755863 4.142742 3.336188 3.985684 3.168663 3.866276 4.714214\n [537] 3.136756 4.224437 3.085171 4.064979 4.335398 3.437602 3.136517 3.112040\n [545] 3.617250 3.588404 3.525572 3.420767 4.142380 3.735685 3.676870 3.109483\n [553] 3.728542 3.875168 3.487259 3.757423 4.320250 3.347897 3.403853 3.772583\n [561] 3.277511 3.952613 2.857228 2.997356 4.200886 4.048865 2.857518 4.305748\n [569] 3.359204 2.966107 4.039403 3.212999 3.215278 4.225138 3.661079 3.289208\n [577] 3.119902 3.063754 3.643251 2.915776 3.201957 3.194348 3.487027 2.857098\n [585] 3.316864 3.688776 3.886815 3.757405 3.663038 3.313268 3.365528 3.349293\n [593] 3.799039 3.211985 4.095726 3.491005 4.196924 3.622085 3.725931 3.759641\n [601] 4.077927 3.255892 3.645800 3.446074 3.622835 2.718157 2.955445 3.076012\n [609] 4.034121 3.178412 3.047648 3.323422 3.941636 3.289397 3.268784 3.924884\n [617] 3.180802 3.242838 3.587265 3.869167 3.658008 4.437133 3.895678 2.996989\n [625] 3.404747 3.234257 3.617072 3.799840 3.708462 4.025645 3.054681 4.665679\n [633] 3.543621 3.252473 4.192278 3.387885 3.284834 3.210602 3.543806 3.766633\n [641] 3.301956 4.204492 3.598269 4.167169 4.194233 4.120629 4.244400 3.966151\n [649] 4.240218 3.683100 4.127516 4.039347 3.631923 3.503221 3.618727 3.981776\n [657] 3.126216 3.763566 3.407495 3.301878 3.542653 3.775649 2.845045 3.491737\n [665] 3.335352 3.303095 3.755053 3.308014 3.706475 3.754518 3.204945 3.404913\n [673] 3.726455 3.329033 3.777579 3.482479 3.341445 3.579939 3.900476 3.254276\n [681] 3.314034 2.848357 3.289856 3.511448 3.365672 3.533281 3.371031 3.035289\n [689] 3.257777 4.057384 3.445213 3.624895 3.370395 3.498568 3.294900 4.324975\n [697] 3.154534 3.046266 3.822989 3.454533 3.183511 3.201773 2.994873 2.898273\n [705] 3.262474 3.482685 4.310450 3.527498 3.485117 4.162948 2.870251 3.639222\n [713] 3.346777 4.060885 3.202220 3.412030 3.819132 3.750381 3.130447 2.705698\n [721] 4.203004 3.099716 3.069005 3.605579 3.033768 3.921071 3.670618 3.387079\n [729] 3.675829 3.196812 3.450501 3.615062 3.832763 3.028520 3.148363 3.090168\n [737] 3.906733 3.645993 3.476494 3.685877 3.847078 3.099888 2.801615 3.366746\n [745] 3.770612 3.658273 3.899856 3.196684 2.949192 3.601828 3.515017 3.954447\n [753] 3.385995 3.480730 4.637983 3.893102 3.647796 3.478507 4.161493 3.401141\n [761] 3.160276 3.289747 3.050591 3.288162 3.542769 3.397243 4.112850 3.210029\n [769] 2.880816 3.590616 3.847930 3.428742 3.505731 3.630716 3.584076 3.977790\n [777] 3.304504 3.420578 3.505325 3.801741 3.318873 3.826561 3.504000 3.748521\n [785] 3.593495 3.780877 3.117464 3.788668 3.919304 3.577802 3.491560 4.665466\n [793] 3.138530 3.644990 4.100921 3.392486 2.891639 3.746481 2.837888 3.748914\n [801] 3.056702 2.928692 4.437132 3.981866 4.014172 3.213803 4.247183 4.026047\n [809] 3.835835 3.753461 3.012614 3.460519 4.534813 3.765488 3.142675 3.818762\n [817] 3.084087 3.179751 3.561294 3.333091 3.320137 3.691318 3.585405 3.477585\n [825] 3.702859 3.496404 3.834725 3.182931 3.129443 3.340578 3.651763 3.846303\n [833] 3.172526 3.049635 4.116219 4.522650 3.778376 4.159857 4.000959 3.648635\n [841] 3.378219 3.274609 3.919474 4.096907 4.254369 4.409201 3.164627 3.048871\n [849] 3.640993 2.952619 3.269446 3.757164 3.165673 3.800969 3.501948 3.513101\n [857] 3.285009 3.289032 3.009537 3.135753 3.104035 3.692953 3.533382 3.873435\n [865] 3.371402 3.076619 3.553542 3.347792 3.524867 3.851137 3.573021 3.792690\n [873] 4.726681 3.419966 4.551700 3.718695 3.455996 2.997152 4.060747 3.516549\n [881] 3.057699 4.045652 3.697965 3.307657 3.250917 3.235020 3.255155 3.410850\n [889] 3.603788 3.265801 3.909863 3.305804 3.701584 3.421232 3.340366 4.251237\n [897] 3.932558 3.481889 3.686683 4.382681 3.140562 3.400008 3.077821 3.928243\n [905] 3.100623 3.264433 3.770700 3.308540 3.366591 3.472078 3.530759 3.602315\n [913] 4.105854 4.064646 3.099904 3.434358 3.410026 3.128126 3.779235 3.348105\n [921] 3.729641 3.451806 3.268224 3.540872 4.069521 3.843877 3.858831 3.852462\n [929] 3.876036 3.175229 3.486533 3.226961 3.795258 3.557319 3.874209 4.353848\n [937] 3.555665 3.513705 4.011576 3.842266 2.971988 3.349847 4.532810 3.801281\n [945] 2.914767 3.658050 3.709164 3.406191 3.623236 3.563127 3.581000 2.877455\n [953] 3.159940 4.567433 4.003815 3.424062 3.871608 3.234052 3.568383 3.048157\n [961] 4.235010 3.161186 3.623221 3.233762 4.169818 3.882942 2.771482 4.108877\n [969] 3.388578 3.456044 3.376549 4.903873 3.934038 3.453023 3.626691 3.214224\n [977] 3.423733 3.770653 3.255744 4.156287 3.162564 3.628929 4.409961 3.215500\n [985] 3.677095 4.195121 3.816188 3.143051 3.571218 3.506495 3.442149 3.371968\n [993] 3.099215 3.805297 3.285430 3.882673 3.638019 3.616644 3.582678 3.080668\n\n\n\n\n6.9.4 Inconsistency I2\nThe inconsistency \\(I^2\\) index is the sum of the squared deviations from the overall effect and weighted by the study size. Value &lt;25% is classified as low and greater than 75% as high heterogeneity. This test can be performed using metafor package (Viechtbauer 2010). The presence of high \\(I^2\\) suggests a need to proceed to meta-regression on the data to understand the source of heterogeneity. The fixed component were the covariates which were being tested for their effect on heterogeneity. The random effect components were the sensitivity and FPR. The \\(I^2\\) value for the TIA clinic study is 34.41%. As such meta-regression is not needed for that study. By contrast, the \\(I^2\\) is much higher for the spot sign study, necessitating metaregression.\n\nlibrary(PRISMAstatement)\n\n#example from Spot sign paper. Stroke 2019\nprisma(found = 193,\n       found_other = 27,\n       no_dupes = 141, \n       screened = 141, \n       screen_exclusions = 3, \n       full_text = 138,\n       full_text_exclusions = 112, \n       qualitative = 26, \n       quantitative = 26,\n       width = 800, height = 800)\n\n\n\n\n\n\n\n6.9.5 Metaanalysis of proportion\nThis is an example of metaanalysis of stroke recurrence following management in rapid TIA clinic. A variety of different methods for calculating the 95% confidence interval of the binomial distribution. The mean of the binomial distribution is given by p and the variance by \\(\\frac{p \\times (1-p)}{n}\\). The term \\(z\\) is given by \\(1-\\frac{\\alpha}{2}\\) quantile of normal distribution. A standard way of calculating the confidence interval is the Wald method \\(p\\pm z\\times \\sqrt{\\frac{p \\times(1-p)}{n}}\\). The Freeman-Tukey double arcsine transformation tries to transform the data to a normal distribution. This approach is useful when occurence of event is rare. The exact or Clopper-Pearson method is suggested as the most conservative of the methods for calculating confidence interval for proportion. It is based on cumulative properties of the binomial distribution. The Wilson method has similarities to the Wald method. It has an extra term \\(z^2/n\\). There are many different methods for calculating the confidence interval for proportions. Investigators such as Agresti proposed that approximate methods are better than exact method (Agresti and Coull 1998). Brown and colleagues proposed the use of the Wilson method (Brown, Cai, and DasGupta 2001)\n\nlibrary(metafor) #open software metafor\n#create data frame dat\n#xi is numerator\n#ni is denominator\n\ndat &lt;- data.frame(model=c(\"melbourne\",\"paris\",\"oxford\",\"stanford\",\"ottawa\",\"new zealand\"),\nxi=c(7,7,6,2,31,2), \nni=c(468,296, 281,223,982,172))\n\n#calculate new variable pi base on ratio xi/ni\ndat$pi &lt;- with(dat, xi/ni)\n\n#Freeman-Tukey double arcsine trasformation\ndat &lt;- escalc(measure=\"PFT\", xi=xi, ni=ni, data=dat, add=0) \nres &lt;- rma(yi, vi, method=\"REML\", data=dat, slab=paste(model))\n\n#create forest plot with labels\nmetafor::forest(res, transf=transf.ipft.hm, targs=list(ni=dat$ni), xlim=c(-1,1.4),refline=res$beta[1],\n       cex=.8, ilab=cbind(dat$xi, dat$ni), \n       #position of data on x-axis\n       ilab.xpos=c(-.6,-.4),digits=3)\n\n#par function combine multiple plots into one\nop &lt;- par(cex=.75, font=2)\n#position of column names on x-axis\ntext(-1.0, 7.5, \"model \",pos=4)\ntext(c(-.55,-.2), 7.5, c(\"recurrence\", \" total subjects\"))\ntext(1.4,7.5, \"Proportion [95% CI]\", pos=2)\n\n\n\n\n\n\n\npar(op)\n\nExact 95% confidence interval for proportion is provided below using the TIA data above. This solution was provided on stack overflow. This is performed using the binomial.test function.\n\n#exact confidence interval\nsapply(split(dat, dat$model), function(x) binom.test(x$xi, x$ni)$conf.int)\n\n      melbourne new zealand     ottawa      oxford      paris    stanford\n[1,] 0.00603419 0.001411309 0.02154780 0.007875285 0.00955965 0.001087992\n[2,] 0.03057375 0.041370853 0.04451101 0.045893259 0.04811612 0.032020390\n\n\nThe data needs to be transposed using t function. This generates one column for lower and another for upper CI. Note that sapply returns a matrix or array and the column names have to be assigned.\n\n#the data\ntmp &lt;- t(sapply(split(dat, dat$model), function(x) binom.test(x$xi, x$ni)$conf.int))\ntmp\n\n                   [,1]       [,2]\nmelbourne   0.006034190 0.03057375\nnew zealand 0.001411309 0.04137085\nottawa      0.021547797 0.04451101\noxford      0.007875285 0.04589326\nparis       0.009559650 0.04811612\nstanford    0.001087992 0.03202039\n\n\nThe exact confidence is now put back into the dat data frame.\n\ndat$ci.lb &lt;- tmp[,1] #adding column to data frame dat\ndat$ci.ub &lt;- tmp[,2] #adding column to data frame dat\n\ndat &lt;- escalc(measure=\"PFT\", xi=xi, ni=ni, data=dat, add=0) \nres &lt;- rma.glmm(measure=\"PLO\", xi=xi, ni=ni, data=dat)\n\n\n#insert the exact confidence interval \nwith(dat, metafor::forest(yi, ci.lb=ci.lb, ci.ub=ci.ub, \n  ylim=c(-1.5,8.5), \n  xlim=c(-1.1,1), \n  refline=predict(res, transf=transf.ilogit)$pred,\n  cex=.8, \n  ilab=cbind(dat$xi, dat$ni), \n  ilab.xpos=c(-.6,-.2),digits=3))\n  \nop &lt;- par(cex=.75, font=2)\naddpoly(res, row=-1, transf=transf.ilogit)\nabline(h=0)\n#position of column names on x-axis\ntext(-.9, 7.5, \"Model\", pos=2)\ntext(c(-.55,-.2), 7.5, c(\"recurrence\", \" total subjects\"))\ntext( 1,   7.5, \"Proportion [95% CI]\", pos=2)\n\n\n\n\n\n\n\n\n\n\n6.9.6 Metaanalysis of continuous data\nMetanalysis of continuous outcome data can be performed using standardised mean difference or ratio of means. It is available in the meta library using metacont function (Balduzzi, Rücker, and Schwarzer 2019).\n\n\n6.9.7 Bivariate Metaanalysis\nThe univariate method of Moses-Shapiro-Littenberg combines these measures (sensitivity and specificity) into a single measure of accuracy (diagnostic odds ratio)(Moses, Shapiro, and Littenberg 1993) . This approach has been criticized for losing data on sensitivity and specificity of the test. Similar to the univariate method, the bivariate method employs a random effect to take into account the within study correlation (Reitsma et al. 2005). Additionally, the bivariate method also accounts for the between-study correlation in sensitivity and specificity. Bivariate analysis is performed using mada package. A [Bayesian library][Bayesian Metaanalysis] for bivariate analysis meta4diag is illustrated later.\nThe example below is taken from a metaanalysis of spot sign as predictor expansion of intracerebral hemorrhage (Phan et al. 2019). The data for this analysis is available in the Data-Use sub-folder.\n\nlibrary(mada)\n\nLoading required package: ellipse\n\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\n\nLoading required package: mvmeta\n\n\nThis is mvmeta 1.0.3. For an overview type: help('mvmeta-package').\n\n\n\nAttaching package: 'mvmeta'\n\n\nThe following object is masked from 'package:metafor':\n\n    blup\n\n\nThe following object is masked from 'package:meta':\n\n    blup\n\n\nThe following object is masked from 'package:checkmate':\n\n    qtest\n\n\n\nAttaching package: 'mada'\n\n\nThe following object is masked from 'package:metafor':\n\n    forest\n\n\nThe following object is masked from 'package:meta':\n\n    forest\n\n#spot sign data\nData&lt;-read.csv(\"./Data-Use/ss150718.csv\")\n\n#remove duplicates using subset\n#another way is to use filter from dplyr\nDat&lt;-subset(Data, Data$retain==\"yes\") \n(ss&lt;-reitsma(Dat))\n\nCall:  reitsma.default(data = Dat)\n\nFixed-effects coefficients:\n              tsens     tfpr\n(Intercept)  0.2548  -1.9989\n\n27 studies, 2 fixed and 3 random-effects parameters\n  logLik       AIC       BIC  \n 52.0325  -94.0650  -84.1201  \n\nsummary(ss)\n\nCall:  reitsma.default(data = Dat)\n\nBivariate diagnostic random-effects meta-analysis\nEstimation method: REML\n\nFixed-effects coefficients\n                  Estimate Std. Error       z Pr(&gt;|z|) 95%ci.lb 95%ci.ub    \ntsens.(Intercept)    0.255      0.152   1.676    0.094   -0.043    0.553   .\ntfpr.(Intercept)    -1.999      0.097 -20.664    0.000   -2.189   -1.809 ***\nsensitivity          0.563          -       -        -    0.489    0.635    \nfalse pos. rate      0.119          -       -        -    0.101    0.141    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nVariance components: between-studies Std. Dev and correlation matrix\n      Std. Dev  tsens  tfpr\ntsens    0.692  1.000     .\ntfpr     0.400 -0.003 1.000\n\n logLik     AIC     BIC \n 52.033 -94.065 -84.120 \n\nAUC:  0.858\nPartial AUC (restricted to observed FPRs and normalized):  0.547\n\nI2 estimates \nZhou and Dendukuri approach:  10.2 % \nHolling sample size unadjusted approaches:  64.2 - 79.9 % \nHolling sample size adjusted approaches:  4.7 - 5 %\n\nAUC(reitsma(data = Dat))\n\n$AUC\n[1] 0.8576619\n\n$pAUC\n[1] 0.5474316\n\nattr(,\"sroc.type\")\n[1] \"ruttergatsonis\"\n\nsumss&lt;-SummaryPts(ss,n.iter = 10^3) #bivariate pooled LR\nsummary(sumss)\n\n          Mean Median  2.5%  97.5%\nposLR    4.740  4.720 3.780  5.760\nnegLR    0.496  0.496 0.413  0.582\ninvnegLR 2.030  2.020 1.720  2.420\nDOR      9.700  9.530 6.790 13.500\n\n\n\n\n6.9.8 Metaanalysis of clinical trial.\nFixed effect (Peto or Mantel-Haenszel) approaches assume that the population is the same for all studies and thus each study is the source of error. Random effect (DerSimonian Laird) assumes an additional source of error between studies.The random effect approach results in more conservate estimate of effect size confidence interval. A criticism of DerSimnonian and Laird approach is that it is prone to type I error especially when the number of number of studies is small (n&lt;20) or moderate heterogeneity. It’s estimated that 25% of the significant findings with DerSimonian Laird method may be non-significant with Hartung-Knapp method (BMC Medical Research Methodology 2014, 14:25). The Hartung-Knapp method (Stat Med 2001;20:3875-89) estimate the between studies variance and treat it as fixed. It employs quantile of t-distribution rather than normal distribution. The Hartung-Knapp method is available in meta and metafor package.\nThe data is from Jama Cardiology on Associations of Omega-3 Fatty Acid Supplement Use With Cardiovascular Disease Risks Meta-analysis of 10 Trials Involving 77917 Individuals (Aung T 2018). Subsequently a meta-analysis (Hu 2019) reported that contrary to the earlier meta-analysis, Omega-3 lowers the risk of cardiovascular diseases with effect related to dose. The analysis using DerSimonian Laird and Hartung Knapp method is illustrated below.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%@%()           masks rlang::%@%()\n✖ rlang:::=()            masks data.table:::=()\n✖ dplyr::between()       masks data.table::between()\n✖ readr::col_factor()    masks scales::col_factor()\n✖ dplyr::collapse()      masks nlme::collapse()\n✖ gridExtra::combine()   masks dplyr::combine()\n✖ matrixStats::count()   masks dplyr::count()\n✖ dplyr::data_frame()    masks tibble::data_frame(), vctrs::data_frame()\n✖ dplyr::dim_desc()      masks pillar::dim_desc()\n✖ purrr::discard()       masks scales::discard()\n✖ tidyr::expand()        masks Matrix::expand()\n✖ tidyr::extract()       masks magrittr::extract()\n✖ dplyr::filter()        masks stats::filter()\n✖ dplyr::first()         masks data.table::first()\n✖ purrr::flatten()       masks jsonlite::flatten(), rlang::flatten()\n✖ purrr::flatten_chr()   masks rlang::flatten_chr()\n✖ purrr::flatten_dbl()   masks rlang::flatten_dbl()\n✖ purrr::flatten_int()   masks rlang::flatten_int()\n✖ purrr::flatten_lgl()   masks rlang::flatten_lgl()\n✖ purrr::flatten_raw()   masks rlang::flatten_raw()\n✖ lubridate::hour()      masks data.table::hour()\n✖ purrr::invoke()        masks rlang::invoke()\n✖ lubridate::isoweek()   masks data.table::isoweek()\n✖ dplyr::lag()           masks stats::lag()\n✖ dplyr::last()          masks data.table::last()\n✖ rlang::ll()            masks Metrics::ll()\n✖ rlang::local_options() masks withr::local_options()\n✖ purrr::map()           masks listenv::map()\n✖ lubridate::mday()      masks data.table::mday()\n✖ lubridate::minute()    masks data.table::minute()\n✖ lubridate::month()     masks data.table::month()\n✖ tidyr::pack()          masks Matrix::pack()\n✖ lubridate::quarter()   masks data.table::quarter()\n✖ lubridate::second()    masks data.table::second()\n✖ dplyr::select()        masks MASS::select()\n✖ purrr::set_names()     masks rlang::set_names(), magrittr::set_names()\n✖ readr::spec()          masks mada::spec()\n✖ purrr::splice()        masks rlang::splice()\n✖ dplyr::src()           masks Hmisc::src()\n✖ cli::style_bold()      masks pillar::style_bold()\n✖ dplyr::summarize()     masks Hmisc::summarize()\n✖ purrr::transpose()     masks data.table::transpose()\n✖ cli::tree()            masks xfun::tree()\n✖ jsonlite::unbox()      masks rlang::unbox()\n✖ tidyr::unpack()        masks Matrix::unpack()\n✖ jsonlite::validate()   masks rms::validate()\n✖ lubridate::wday()      masks data.table::wday()\n✖ lubridate::week()      masks data.table::week()\n✖ rlang::with_options()  masks withr::with_options()\n✖ lubridate::yday()      masks data.table::yday()\n✖ lubridate::year()      masks data.table::year()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(metafor)\n\n#Omega 3 data\nYear=c(2010,2014,2010,2007,2010,2010,2013,2008,2012,1999)\nTrials=c(\"DOIT\",\"AREDS-2\",\"SU.FOL.OM3\",\"JELIS\",\"Alpha Omega\",\"OMEGA\",\"R&P\",\"GISSI-HF\",\"ORIGIN\",\"GISSI-P\")\nTreatment=c(29,213,216,262,332,534,733,783,1276,1552)\nTreatment.per=c(10.3,9.9,17.2,2.8,13.8,27.7,11.7,22.4,20.3,27.4)\nControl=c(35,208,211,324,331,541,745,831,1295,1550)\nControl.per=c(12.5,10.1,16.9,3.5,13.6,28.6, 11.9,23.9,20.7,27.3)\n\n#combine into data frame\nrct&lt;-data.frame(Year,Trials,Treatment,Treatment.per,Control,Control.per) %&gt;%\n  mutate(Treatment.number=round(Treatment*100/Treatment.per,0),\n         Control.number=round(Control*100/Control.per,0), group=ifelse(Year&gt;2010,\"wide\",\"narrow\")) %&gt;%\n  rename(ai=Treatment,n1i=Treatment.number,ci=Control,\n         n2i=Control.number,study=Trials) \n\n\n#peto's fixed effect method\nres &lt;- rma.peto(ai=ai, n1i=n1i, ci=ci, n2i=n2i, data=rct)\nprint(res, digits=2)\n\n\nEqual-Effects Model (k = 10)\n\nI^2 (total heterogeneity / total variability):  0.00%\nH^2 (total variability / sampling variability): 0.93\n\nTest for Heterogeneity: \nQ(df = 9) = 8.36, p-val = 0.50\n\nModel Results (log scale):\n\nestimate    se   zval  pval  ci.lb  ci.ub \n   -0.04  0.02  -1.72  0.08  -0.08   0.00 \n\nModel Results (OR scale):\n\nestimate  ci.lb  ci.ub \n    0.97   0.93   1.00 \n\nresult&lt;-predict(res, transf=exp, digits=2)\n#forest plot\nmetafor::forest(res, targs=list(study=rct$study), \nmain=\"RCT of Omega3 fatty acid for cardiovascular disease-Peto\")\n\n\n\n\n\n\n\n\nThe funnel plot is used here to illustrate presence or absence publication bias. In this case the funnel plot reflects the same findings as the low \\(I^2\\) value. There is one outlier. We will explore this further with the GOSH plot below.\n\n# funnel plot \nfunnel(res, refline=0, level=c(90, 95, 99), \n       shade=c(\"white\", \"gray\", \"darkgray\"))\n\n\n\n\n\n\n\n\nRandom effect (DerSimonian Laird) assumes an additional source of error between studies when calculating odds ratio.\n\n#DL\ndatO3 &lt;- escalc(measure = \"OR\",ai=ai, n1i=n1i, ci=ci, n2i=n2i,data=rct)\nres.DL&lt;-rma(yi,vi, method = \"DL\",data=datO3)\nmetafor::forest(res.DL,main=\"RCT of Omega3 fatty acid for cardiovascular disease-DL\")\n\n\n\n\n\n\n\n\nDerSimonian Laird method may not be appropriate for metaanalysis with small number of studies . The recommendation is to use the Hartung Knapp random effect method. This analysis show that inspite of the small sample size the significant findings remain.\n\n#Hartung-ignore error message\nres.HK&lt;-rma.uni(yi,vi=1/vi,method=\"FE\",knha=TRUE,data=datO3)\nmetafor::forest(res.HK,main=\"RCT of Omega3 fatty acid for cardiovascular disease-HK\")\n\n\n\n\n\n\n\n\nSubgroup analysis is one way of exploring the data for group effect. Metaregression is illustrated later.\n\n#datO3$group&lt;-rct3$group\n#plot subgroups\nres.group &lt;- rma(yi, vi, mods = ~ group, data=datO3)\nres.wide&lt;- rma(yi, vi, subset=(group==\"wide\"), data=datO3)\nres.wide\nres.narrow&lt;- rma(yi, vi, subset=(group==\"narrow\"), data=datO3)\nres.narrow\n#https://stackoverflow.com/questions/39392706/using-the-r-forestplot-package-\n#is-there-a-way-to-assign-variable-colors-to-boxe\n\n#confidence interval\nrmeta_conf &lt;- \n  structure(list(\n    mean  = c(NA, NA, exp(-0.0676), exp(-0.0266),  NA, exp(-0.0352)), \n    lower = c(NA, NA, exp(-0.1873), exp(-0.0715),  NA, exp(-0.0753)),\n    upper = c(NA, NA, exp(0.0520), exp(0.0183),  NA, exp(0.0049))),\n    .Names = c(\"mean\", \"lower\", \"upper\"), \n    row.names = c(NA, -6L), \n    class = \"data.frame\")\n\n#table data\ntabletext&lt;-cbind(c(\"\",\"Trials\",\"wide\",\"narrow\",NA,\"Summary\"),\n    c(\"Events\",\"(Drugs)\",sum(filter(datO3,group==\"wide\")$ai),\n      sum(filter(datO3,group==\"narrow\")$ai),NA,NA),\n    c(\"Events\",\"(Control)\",sum(filter(datO3,group==\"wide\")$ci),\n      sum(filter(datO3,group==\"narrow\")$ci),NA,NA),     \n    c(\"\",\"OR\",\"0.934\",\"0.974\",NA,\"0.965\")\n                 )\n#use forestplot as another library to perform forest plot\nlibrary(forestplot)\n forestplot(tabletext, \n       rmeta_conf,new_page = TRUE,\n       is.summary=c(TRUE,TRUE,rep(FALSE,8),TRUE),\n       clip=c(0.1,2.5), \n       xlog=TRUE, \n       col=fpColors(box=\"royalblue\",line=\"darkblue\", summary=\"royalblue\"))\n\nBaujat plot is another method in addition to funnel plot explore heterogeneity\n\n# adjust margins so the space is better used\n\npar(mar=c(5,4,2,2))\n# create Baujat plot to explore source of heterogeneity\nbaujat(res.DL, xlim=c(0,20), ylim=c(0,0.2))\n\n\n\n\n\n\n\n\nGOSH plot explores study heterogeneity using output of fixed effect model for all possible subsets\n\n## fit FE model to all possible subsets\n# uses output from DL analysis\n\nsav &lt;- gosh(res.DL)\n\nFitting 1023 models (based on all possible subsets).\n\n### create GOSH plot\n### red points for subsets that include and blue points\n### for subsets that exclude study 16 (the ISIS-4 trial)\n\nplot(sav, out=dim(rct)[1], breaks=100)\n\n\n\n\n\n\n\n\n\n\n6.9.9 Metaregression\n\n#separate metaregression for tsens and tfpr\n\n#tsens - setting single or multicentre for spot sign metaanalysis\ndat1&lt;-subset(Dat, Dat$result_id==1 )\n(metass&lt;-reitsma(dat1, formula = cbind(tsens,tfpr)~PubYear+Study.type+Setting+Quality.assessment)) \n\nCall:  reitsma.default(data = dat1, formula = cbind(tsens, tfpr) ~ PubYear + \n    Study.type + Setting + Quality.assessment)\n\nFixed-effects coefficients:\n                       tsens       tfpr\n(Intercept)         298.7348  -164.8603\nPubYear              -0.1484     0.0828\nStudy.typeRetro       0.3554    -0.1489\nSettingSingle         0.2283    -0.1301\nQuality.assessment   -0.0040    -0.0834\n\n26 studies, 10 fixed and 3 random-effects parameters\n  logLik       AIC       BIC  \n 48.5386  -71.0771  -45.7109  \n\nsummary(metass)\n\nCall:  reitsma.default(data = dat1, formula = cbind(tsens, tfpr) ~ PubYear + \n    Study.type + Setting + Quality.assessment)\n\nBivariate diagnostic random-effects meta-regression\nEstimation method: REML\n\nFixed-effects coefficients\n                         Estimate Std. Error      z Pr(&gt;|z|) 95%ci.lb 95%ci.ub\ntsens.(Intercept)         298.735    143.547  2.081    0.037   17.387  580.083\ntsens.PubYear              -0.148      0.071 -2.078    0.038   -0.288   -0.008\ntsens.Study.typeRetro       0.355      0.310  1.146    0.252   -0.253    0.963\ntsens.SettingSingle         0.228      0.495  0.462    0.644   -0.741    1.198\ntsens.Quality.assessment   -0.004      0.050 -0.081    0.936   -0.102    0.094\ntfpr.(Intercept)         -164.860     80.574 -2.046    0.041 -322.783   -6.938\ntfpr.PubYear                0.083      0.040  2.066    0.039    0.004    0.161\ntfpr.Study.typeRetro       -0.149      0.172 -0.865    0.387   -0.486    0.188\ntfpr.SettingSingle         -0.130      0.276 -0.471    0.638   -0.672    0.412\ntfpr.Quality.assessment    -0.083      0.026 -3.190    0.001   -0.135   -0.032\n                           \ntsens.(Intercept)         *\ntsens.PubYear             *\ntsens.Study.typeRetro      \ntsens.SettingSingle        \ntsens.Quality.assessment   \ntfpr.(Intercept)          *\ntfpr.PubYear              *\ntfpr.Study.typeRetro       \ntfpr.SettingSingle         \ntfpr.Quality.assessment  **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nVariance components: between-studies Std. Dev and correlation matrix\n      Std. Dev tsens  tfpr\ntsens    0.657 1.000     .\ntfpr     0.297 0.312 1.000\n\n logLik     AIC     BIC \n 48.539 -71.077 -45.711 \n\n\nI2 estimates \nZhou and Dendukuri approach:  0 % \nHolling sample size unadjusted approaches:  66.8 - 80.5 % \nHolling sample size adjusted approaches:  5.4 - 5.8 %\n\n#tfpr - setting single or multicentre for spot sign metaanalysis\ndat1&lt;-subset(Dat, Dat$result_id==1 )\n(metassfull&lt;-reitsma(dat1, formula = cbind(tsens,tfpr)~\n        PubYear+clinical+Study.type+CTA6hrs+Setting+Quality.assessment)) \n\nCall:  reitsma.default(data = dat1, formula = cbind(tsens, tfpr) ~ PubYear + \n    clinical + Study.type + CTA6hrs + Setting + Quality.assessment)\n\nFixed-effects coefficients:\n                       tsens       tfpr\n(Intercept)         297.5416  -144.2754\nPubYear              -0.1480     0.0724\nclinical             -0.4703    -0.1810\nStudy.typeRetro       0.4884    -0.0989\nCTA6hrsyes           -0.2141    -0.2342\nSettingSingle         0.2568    -0.1435\nQuality.assessment    0.0130    -0.0738\n\n26 studies, 14 fixed and 3 random-effects parameters\n  logLik       AIC       BIC  \n 48.4477  -62.8955  -29.7243  \n\nsummary(metassfull)\n\nCall:  reitsma.default(data = dat1, formula = cbind(tsens, tfpr) ~ PubYear + \n    clinical + Study.type + CTA6hrs + Setting + Quality.assessment)\n\nBivariate diagnostic random-effects meta-regression\nEstimation method: REML\n\nFixed-effects coefficients\n                         Estimate Std. Error      z Pr(&gt;|z|) 95%ci.lb 95%ci.ub\ntsens.(Intercept)         297.542    150.543  1.976    0.048    2.482  592.601\ntsens.PubYear              -0.148      0.075 -1.977    0.048   -0.295   -0.001\ntsens.clinical             -0.470      0.333 -1.411    0.158   -1.124    0.183\ntsens.Study.typeRetro       0.488      0.321  1.523    0.128   -0.140    1.117\ntsens.CTA6hrsyes           -0.214      0.322 -0.665    0.506   -0.845    0.417\ntsens.SettingSingle         0.257      0.495  0.519    0.604   -0.714    1.228\ntsens.Quality.assessment    0.013      0.051  0.255    0.799   -0.087    0.113\ntfpr.(Intercept)         -144.275     82.720 -1.744    0.081 -306.403   17.852\ntfpr.PubYear                0.072      0.041  1.761    0.078   -0.008    0.153\ntfpr.clinical              -0.181      0.187 -0.969    0.333   -0.547    0.185\ntfpr.Study.typeRetro       -0.099      0.177 -0.558    0.577   -0.446    0.249\ntfpr.CTA6hrsyes            -0.234      0.177 -1.322    0.186   -0.581    0.113\ntfpr.SettingSingle         -0.144      0.274 -0.524    0.600   -0.680    0.393\ntfpr.Quality.assessment    -0.074      0.027 -2.774    0.006   -0.126   -0.022\n                           \ntsens.(Intercept)         *\ntsens.PubYear             *\ntsens.clinical             \ntsens.Study.typeRetro      \ntsens.CTA6hrsyes           \ntsens.SettingSingle        \ntsens.Quality.assessment   \ntfpr.(Intercept)          .\ntfpr.PubYear              .\ntfpr.clinical              \ntfpr.Study.typeRetro       \ntfpr.CTA6hrsyes            \ntfpr.SettingSingle         \ntfpr.Quality.assessment  **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nVariance components: between-studies Std. Dev and correlation matrix\n      Std. Dev tsens  tfpr\ntsens    0.650 1.000     .\ntfpr     0.285 0.188 1.000\n\n logLik     AIC     BIC \n 48.448 -62.895 -29.724 \n\n\nI2 estimates \nZhou and Dendukuri approach:  0 % \nHolling sample size unadjusted approaches:  66.8 - 80.5 % \nHolling sample size adjusted approaches:  5.4 - 5.8 %\n\n\nPlot year against tsens from metaregression\n\n#using output from spot sign\n\nlibrary(ggplot2)\nlibrary(lubridate)\nssr&lt;-as.data.frame(ss$residuals)\n#convert character to year\nssr$Year&lt;-as.Date(as.character(Dat$PubYear),\"%Y\")\nssr$Quality&lt;-Dat$Quality.assessment\nggplot(ssr, aes(x=ssr$Year,y=ssr$tsens))+geom_point()+\n  scale_x_date()+geom_smooth(method=\"lm\")+\n  ggtitle(\"Relationship between transformed sensitivity and Publication Year\")+\n  labs(x=\"Year\",y=\"transformed sensitivity\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n6.9.9.1 summary Positive and Negative Likelihood Ratio\nPositive likelihood ratio (PLR) is the ratio of sensitivity to false positive rate (FPR); the negative (NLR) likelihood ratio is the ratio of 1-sensitivity to specificity. A PLR indicates the likelihood that a positive spot sign (test) would be expected in a patient with target disorder compared with the likelihood that the same result would be expected in a patient without target disorder. Using the recommendation by Jaeschke et al(Jaeschke, Guyatt, and Sackett 1994) a high PLR (&gt;5) and low NLR (&lt;0.2) indicate that the test results would make moderate changes in the likelihood of hematoma growth from baseline risk. PLRs of &gt;10 and NLRs of &lt;0.1 would confer very large changes from baseline risk. The pooled likelihood ratios were used to calculate post-test odds according to Bayes’ Theorem and post-test probabilities of outcome after a positive test result for a range of possible values of baseline risk.\nData from likelihood ratio can be used to create Fagan’s normogram.\n\n#The code for this is available at\n#\"https://raw.githubusercontent.com/achekroud/nomogrammer/master/nomogrammer.r\"\nlibrary(nomogrammer)\np&lt;-nomogrammer(Prevalence = .234, Plr = 4.85, Nlr = 0.49)\np+ggtitle(\"Fagan's normogram for Spot Sign and ICH growth\")\n#ggsave(p,file=\"Fagan_SpotSign.png\",width=5.99,height=3.99,units=\"in\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#data-simulation",
    "href": "statistics.html#data-simulation",
    "title": "6  Statistics",
    "section": "6.10 Data simulation",
    "text": "6.10 Data simulation\nData simulation is an important aspects of data science. The example below is taken from our experience trying to simulate data from recent clot retrieval trials in stroke (Berkhemer et al. 2015) (Campbell et al. 2015). Simulation is performed using simstudy library.\n\nlibrary (simstudy)\nlibrary(tidyverse)\n\n#T is Trial\ndef &lt;- defData(varname = \"T\", dist = \"binary\", formula = 0.5)\n\n#early neurological improvement (ENI) .37 in TPA and .8 in ECR\n#baseline NIHSS 13 in TPA and 17 in ECR\n\ndef &lt;- defData(def, varname = \"ENI\", dist = \"normal\", formula = .8-.53*T, variance = .1)\n\n#baseline NIHSS 13 in TPA and 17 in ECR\ndef &lt;- defData(def, varname = \"Y1\", dist = \"normal\", formula=13, variance = 1)\n\ndef &lt;- defData(def, varname = \"Y2\", dist = \"normal\", formula = \"Y1*ENI - 5 * T &gt;5\",variance = 1)\n\ndef &lt;- defData(def, varname = \"Y3\", dist = \"normal\", formula = \"Y2- 4*T&gt;2\",variance = 1)\n\ndef &lt;- defData(def, varname = \"Y4\", dist = \"normal\", formula = \"Y3- 2*T&gt;0\", variance = 1)\n\n#male\ndef &lt;- defData(def,varname = \"Male\", dist = \"binary\", formula = 0.49*T)\n\n#diabetes .23 in TPA and .06 in ECR\ndef &lt;- defData(def,varname = \"Diabetes\", dist = \"binary\", formula = .23-.17*T)\n\n#HT .66 TPA vs .6 ECR\ndef &lt;- defData(def,varname = \"HT\", dist = \"binary\", formula = .66-.06*T)\n\n#generate data frame\ndtTrial &lt;- genData(500, def)\n\n#define parameter for mRS\n\n\n#Add conditional column with field name \"mRS\"\n\n\n\ndtTime &lt;- addPeriods(dtTrial, nPeriods = 4, idvars = \"id\", \n            timevars = c(\"Y1\", \"Y2\", \"Y3\",\"Y4\"), timevarName = \"Y\")\ndtTime\n\n       id period T       ENI Male Diabetes HT          Y timeID\n   1:   1      0 1 0.2888235    0        0  0 13.7430043      1\n   2:   1      1 1 0.2888235    0        0  0  1.4892292      2\n   3:   1      2 1 0.2888235    0        0  0 -0.3249935      3\n   4:   1      3 1 0.2888235    0        0  0 -1.0565860      4\n   5:   2      0 1 0.1940514    0        0  0 12.5764834      5\n  ---                                                          \n1996: 499      3 1 0.2227292    0        0  1 -1.4363097   1996\n1997: 500      0 0 0.5754765    1        0  1 14.1556444   1997\n1998: 500      1 0 0.5754765    1        0  1  2.1193367   1998\n1999: 500      2 0 0.5754765    1        0  1  2.7750694   1999\n2000: 500      3 0 0.5754765    1        0  1  1.4107691   2000\n\n#check  that 2 groups are similar at start but not at finish\nt.test(Y1~T,data=dtTrial)\n\n\n    Welch Two Sample t-test\n\ndata:  Y1 by T\nt = 0.01362, df = 461.78, p-value = 0.9891\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.176228  0.178688\nsample estimates:\nmean in group 0 mean in group 1 \n       13.06803        13.06680 \n\nt.test(Y4~T,data=dtTrial)\n\n\n    Welch Two Sample t-test\n\ndata:  Y4 by T\nt = 4.8311, df = 442.66, p-value = 1.875e-06\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.2792413 0.6622504\nsample estimates:\nmean in group 0 mean in group 1 \n     0.45056170     -0.02018415 \n\nt.test(Male~T,data=dtTrial)\n\n\n    Welch Two Sample t-test\n\ndata:  Male by T\nt = -0.43618, df = 484.68, p-value = 0.6629\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.10750681  0.06844721\nsample estimates:\nmean in group 0 mean in group 1 \n      0.4454148       0.4649446 \n\n#putting the 4 time periods together - long format\ndtTime &lt;- addPeriods(dtTrial, nPeriods = 3, idvars = \"id\", timevars = c(\"Y1\", \"Y2\", \"Y3\",\"Y4\"), timevarName = \"Y\")\n\n#summarise data using group_by\ndtTime2&lt;-dtTime %&gt;%\n  group_by(period, T) %&gt;%\n  summarise(meanY=mean(Y),\n            sdY=sd(Y),\n            upperY=meanY+sdY,\n            lowerY=meanY-sdY)\n\n`summarise()` has grouped output by 'period'. You can override using the\n`.groups` argument.\n\n#write.csv(dtTime, file=\"./Data-Use/dtTime_simulated.csv\")\n#write.csv(dtTrial,          file=\"./Data-Use/dtTrial_simulated.csv\")\n\n\n\n\n\nAgresti, Alan, and Brent A. Coull. 1998. “Approximate Is Better Than ‘Exact’ for Interval Estimation of Binomial Proportions.” The American Statistician 52 (2): 119–26. https://doi.org/10.1080/00031305.1998.10480550.\n\n\nAung T, Kromhout D, Halsey J. 2018. “Associations of Omega-3 Fatty Acid Supplement Use with Cardiovascular Disease Risks: Meta-Analysis of 10 Trials Involving 77 917 Individuals.” Jama Cardiol, March. https://doi.org/doi: 10.1001/jamacardio.2017.5205.\n\n\nBalduzzi, Sara, Gerta Rücker, and Guido Schwarzer. 2019. “How to Perform a Meta-Analysis with R: A Practical Tutorial.” Evidence-Based Mental Health, no. 22: 153–60.\n\n\nBerkhemer, O. A., P. S. Fransen, D. Beumer, L. A. van den Berg, H. F. Lingsma, A. J. Yoo, W. J. Schonewille, et al. 2015. “A randomized trial of intraarterial treatment for acute ischemic stroke.” N. Engl. J. Med. 372 (1): 11–20.\n\n\nBJ, Biggerstaff. 2000. “Comparing Diagnostic Tests: A Simple Graphic Using Likelihood Ratios.” Stat Med 19 (March): 649–63. https://doi.org/10.1002/(sici)1097-0258(20000315)19:5&lt;649::aid-sim371&gt;3.0.co;2-h.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001. “Interval Estimation for a Binomial Proportion.” Statist. Sci. 16 (2): 101–33. https://doi.org/10.1214/ss/1009213286.\n\n\nCampbell, B. C., P. J. Mitchell, T. J. Kleinig, H. M. Dewey, L. Churilov, N. Yassi, B. Yan, et al. 2015. “Endovascular therapy for ischemic stroke with perfusion-imaging selection.” N. Engl. J. Med. 372 (11): 1009–18.\n\n\nCohen, Jacob. 1977. “CHAPTER 6 - Differences Between Proportions.” In Statistical Power Analysis for the Behavioral Sciences, edited by Jacob Cohen, 179–213. Academic Press. https://doi.org/https://doi.org/10.1016/B978-0-12-179060-8.50011-6.\n\n\nDiamond, G. A. 1992. “What price perfection? Calibration and discrimination of clinical prediction models.” J Clin Epidemiol 45 (1): 85–89.\n\n\nGopal, A. D., N. R. Desai, T. Tse, and J. S. Ross. 2015. “Reporting of noninferiority trials in ClinicalTrials.gov and corresponding publications.” JAMA 313 (11): 1163–65.\n\n\nGraf E, Sauerbrei W, Schmoor C. 1999. “Assessment and Comparison of Prognostic Classification Schemes for Survival Data.” Stat Med 18 (September): 2529–45. https://doi.org/10.1002/(sici)1097-0258(19990915/30)18:17/18&lt;2529::aid-sim274&gt;3.0.co;2-5.\n\n\nGrambsch, P. M., and T. M. Therneau. 1994. “Proportional hazards tests and diagnostics based on weighted residuals.” Biometrika 81 (3): 515–26. https://doi.org/10.1093/biomet/81.3.515.\n\n\nHarrell FE Jr, Pryor DB, Califf RM. 1982. “Evaluating the Yield of Medical Tests.” JAMA 247 (May): 2543–46.\n\n\nHu, Hu, Y. 2019. “Marine Omega‐3 Supplementation and Cardiovascular Disease: An Updated Meta‐analysis of 13 Randomized Controlled Trials Involving 127 477 Participants.” JAHA, September. https://doi.org/https://doi.org/10.1161/JAHA.119.013543.\n\n\nIngall, Timothy John, William Michael O’Fallon, Kjell Asplund, Lewis Robert Goldfrank, Vicki S. Hertzberg, Thomas Arthur Louis, and Teresa J. Hengy Christianson. 2004. “Findings from the Reanalysis of the NINDS Tissue Plasminogen Activator for Acute Ischemic Stroke Treatment Trial.” Stroke 35 (10): 2418–24. https://doi.org/10.1161/01.STR.0000140891.70547.56.\n\n\nJ, Muschelli. 2020. “ROC and AUC with a Binary Predictor: A Potentially Misleading Metric.” J Classif 37 (October): 696–708. https://doi.org/10.1007/s00357-019-09345-1.\n\n\nJaeschke, R., G. H. Guyatt, and D. L. Sackett. 1994. “Users’ guides to the medical literature. III. How to use an article about a diagnostic test. B. What are the results and will they help me in caring for my patients? The Evidence-Based Medicine Working Group.” JAMA 271 (9): 703–7.\n\n\nKahan, Jairath, B. C. 2014. “The Risks and Rewards of Covariate Adjustment in Randomized Trials: An Assessment of 12 Outcomes from 8 Studies.” Trials 15: 139. https://doi.org/10.1186/1745-6215-15-139.\n\n\nKaji, A. H., and R. J. Lewis. 2015. “Noninferiority Trials: Is a New Treatment Almost as Effective as Another?” JAMA 313 (23): 2371–72.\n\n\nLongato, Enrico, Martina Vettoretti, and Barbara Di Camillo. 2020. “A Practical Perspective on the Concordance Index for the Evaluation and Selection of Prognostic Time-to-Event Models.” Journal of Biomedical Informatics 108: 103496. https://doi.org/https://doi.org/10.1016/j.jbi.2020.103496.\n\n\nLudwig, L., P. Darmon, and B Guerci. 2020. “Computing and interpreting the Number Needed to Treat for Cardiovascular Outcomes Trials:Perspective on GLP-1 RA and SGLT-2i therapies.” Cardiovasc Diabetol, May. https://doi.org/10.1186/s12933-020-01034-3.\n\n\nLuo, Dehui, Xiang Wan, Jiming Liu, and Tiejun Tong. 2018. “Optimally Estimating the Sample Mean from the Sample Size, Median, Mid-Range, and/or Mid-Quartile Range.” Statistical Methods in Medical Research 27 (6): 1785–805. https://doi.org/10.1177/0962280216669183.\n\n\nMarx, Heiner C, Arthur. Bucher. 2003. “Numbers Needed to Treat Derived from Meta-Analysis: A Word of Caution.” BMJ Evidence-Based Medicine 8 (2): 36–37. https://doi.org/10.1136/ebm.8.2.36.\n\n\nMathur MB, Riddell CA, Ding P. 2018. “Web Site and r Package for Computing e-Values.” Epidemiology, e45–47. https://doi.org/10.1097/EDE.0000000000000864.\n\n\nMcGrath, Sean, Stephan Katzenschlager, Alexander J. Zimmer, Alexander Seitel, Russell J. Steele, and Andrea Benedetti. 2022. “Standard Error Estimation in Meta-Analysis of Studies Reporting Medians.” Statistical Methods in Medical Research 32: 373–88. https://doi.org/10.1177/09622802221139233.\n\n\nMcGrath, Sean, XiaoFei Zhao, Russell Steele, Brett D. Thombs, Andrea Benedetti, the DEPRESsion Screening Data (DEPRESSD) Collaboration, Brooke Levis, et al. 2020. “Estimating the Sample Mean and Standard Deviation from Commonly Reported Quantiles in Meta-Analysis.” Statistical Methods in Medical Research 29 (9): 2520–37. https://doi.org/10.1177/0962280219889080.\n\n\nMoses, L. E., D. Shapiro, and B. Littenberg. 1993. “Combining independent studies of a diagnostic test into a summary ROC curve: data-analytic approaches and some additional considerations.” Stat Med 12 (14): 1293–1316.\n\n\nPhan, T. G., B. B. Clissold, H. Ma, J. V. Ly, and V. Srikanth. 2017. “Predicting Disability after Ischemic Stroke Based on Comorbidity Index and Stroke Severity-From the Virtual International Stroke Trials Archive-Acute Collaboration.” Front Neurol 8: 192.\n\n\nPhan, T. G., B. Clissold, J. Ly, H. Ma, C. Moran, V. Srikanth, K. R. Lees, et al. 2016. “Stroke Severity and Comorbidity Index for Prediction of Mortality after Ischemic Stroke from the Virtual International Stroke Trials Archive-Acute Collaboration.” J Stroke Cerebrovasc Dis 25 (4): 835–42.\n\n\nPhan, T. G., G. A. Donnan, S. M. Davis, and G. Byrnes. 2006. “Proof-of-principle phase II MRI studies in stroke: sample size estimates from dichotomous and continuous data.” Stroke 37 (10): 2521–25.\n\n\nPhan, T. G., G. A. Donnan, M. Koga, L. A. Mitchell, M. Molan, G. Fitt, W. Chong, M. Holt, and D. C. Reutens. 2006. “The ASPECTS template is weighted in favor of the striatocapsular region.” Neuroimage 31 (2): 477–81.\n\n\nPhan, T. G., N. Krishnadas, V. W. Y. Lai, M. Batt, L. A. Slater, R. V. Chandra, V. Srikanth, and H. Ma. 2019. “Meta-Analysis of Accuracy of the Spot Sign for Predicting Hematoma Growth and Clinical Outcomes.” Stroke 50 (8): 2030–36.\n\n\nReitsma, J. B., A. S. Glas, A. W. Rutjes, R. J. Scholten, P. M. Bossuyt, and A. H. Zwinderman. 2005. “Bivariate analysis of sensitivity and specificity produces informative summary measures in diagnostic reviews.” J Clin Epidemiol 58 (10): 982–90.\n\n\nSalminen, P., H. Paajanen, T. Rautio, P. Nordstrom, M. Aarnio, T. Rantanen, R. Tuominen, et al. 2015. “Antibiotic Therapy vs Appendectomy for Treatment of Uncomplicated Acute Appendicitis: The APPAC Randomized Clinical Trial.” JAMA 313 (23): 2340–48.\n\n\nShi, Jiandong, Dehui Luo, Xiang Wan, Yue Liu, Jiming Liu, Zhaoxiang Bian, and Tiejun Tong. 2023. “Detecting the Skewness of Data from the Five-Number Summary and Its Application in Meta-Analysis.” Statistical Methods in Medical Research 32 (7): 1338–60. https://doi.org/10.1177/09622802231172043.\n\n\nSignori, Alessio, Fabio Pellegrini, Francesca Bovis, Luca Carmisciano, Carl de Moor, and Maria Pia Sormani. 2020. “Comparison of Placebos and Propensity Score Adjustment in Multiple Sclerosis Nonrandomized Studies.” JAMA Neurology, April. https://doi.org/10.1001/jamaneurol.2020.0678.\n\n\nSmeeth, A. Ebrahim, L. Haines. 1999. “Numbers needed to treat derived from meta-analyses–sometimes informative, usually misleading.” BMJ 318: 1548–51. https://doi.org/10.1136/bmj.318.7197.1548.\n\n\nStensrud, M. J., and M. A. Hernan. 2020. “Why Test for Proportional Hazards?” JAMA, March.\n\n\nTJ, Fagan. 1975. “Nomogram for Bayes’s Theorem.” N Engl J Med 293 (July): 257. https://doi.org/10.1056/NEJM197507312930513.\n\n\nVanderWeele TJ, Ding P. 2017. “Sensitivity Analysis in Observational Research: Introducing the e-Value.” Ann Intern Med 167 (August): 268–74. https://doi.org/10.7326/M16-2607.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in r with the Metafor Package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03.\n\n\nWan, Wang, X. 2014. “Estimating the Sample Mean and Standard Deviation from the Sample Size, Median, Range and/or Interquartile Range.” BMC Med Res Methodol, December. https://doi.org/https://doi.org/10.1186/1471-2288-14-135.\n\n\nWang, Rui, Stephen W. Lagakos, James H. Ware, David J. Hunter, and Jeffrey M. Drazen. 2007. “Statistics in Medicine — Reporting of Subgroup Analyses in Clinical Trials.” New England Journal of Medicine 357 (21): 2189–94. https://doi.org/10.1056/NEJMsr077003.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "multivariate.html",
    "href": "multivariate.html",
    "title": "7  Multivariate Analysis",
    "section": "",
    "text": "7.1 Multivariate regression\nMultivariable and multivariate regression are often used interchangeably. Some use the term multivariate when there are more than one dependent variables. Multivariable regression refers to linear, logistic or survival curve analysis in the previous chapter. Multivariate regression refers to nested models or longitudinal models or more complex type of analyses described below.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multivariate Analysis</span>"
    ]
  },
  {
    "objectID": "multivariate.html#multivariate-regression",
    "href": "multivariate.html#multivariate-regression",
    "title": "7  Multivariate Analysis",
    "section": "",
    "text": "7.1.1 Penalised regression\nPenalized regression is a method used to overcome collinearity in the columns of the predictor variables. In the presence of collinearity among predictor variables, the inverse solution can be found by introducing a bias term to the correlation matrix. The effect of this bias term is that it leads to restriction in the size of the variance for the parameter estimate. The bias term can be in the form of L1 or L2 penalisation. Penalized regression can be used for linear or logistic regression.\nWe used penalised logistic regression (PLR) to assess the relationship between the ASPECTS regions and stroke disability (binary outcome) (Phan et al. 2013). PLR can be conceptualized as a modification of logistic regression. In logistic regression, there is no algebraic solution to determine the parameter estimate (β coefficient) and a numerical method (trial and error approach) such as maximum likelihood estimate is used to determine the parameter estimate. In certain situations overfitting of the model may occur with the maximum likelihood method. This situation occurs when there is collinearity (relatedness) of the data. To circumvent this, a bias factor is introduced into the calculation to prevent overfitting of the model. The tuning (regularization) parameter for the bias factor is chosen from the quadratic of the norms of the parameter estimate. This method is known as PLR. This method also allows handling of a large number of interaction terms in the model. We employed a forward and backward stepwise PLR that used all the ASPECTS regions in the analysis, calling on the penalized function in R programming environment. This program automatically assessed the interaction of factors in the regression model in the following manner. The choice of factors to be added/deleted to the stepwise regression was based on the cost complexity statistic. The asymmetric hierarchy principle was used to determine the choice of interaction of factors. In this case, any factor retained in the model can form interactions with others that are already in the model and those that are not yet in the model. In this analysis, we have specified a maximum of 5 terms to be added to the selection procedure. The significance of the interactions was plotted using a previously described method. We regressed the dichotomized mRS score against ASPECTS regions, demographic variables (such as age and sex), physiological variables (such as blood pressure and serum glucose level) and treatment (rt-PA). The results are expressed as β coefficients rather than as odds ratio for consistency due to the presence of interaction terms.\nPenalised L1 regression can also be used for feature selection as the L1 penalisation will turn downweight the coefficient of redundant feature towards zeroes.\n\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\ndata(\"BreastCancer\",package = \"mlbench\")\ncolnames(BreastCancer)\n\n [1] \"Id\"              \"Cl.thickness\"    \"Cell.size\"       \"Cell.shape\"     \n [5] \"Marg.adhesion\"   \"Epith.c.size\"    \"Bare.nuclei\"     \"Bl.cromatin\"    \n [9] \"Normal.nucleoli\" \"Mitoses\"         \"Class\"          \n\n#check for duplicates\nsum(duplicated(BreastCancer))\n\n[1] 8\n\n#remove duplicates\n#keep Id to avoid creation of new duplicates\n#BreastCancer1&lt;-unique(BreastCancer) #reduce 699 to 691 rows\n\n#impute missing data\n#m is number of multiple imputation, default is 5\n#output is a list\nimputed_Data &lt;- mice(BreastCancer, m=5, maxit = 5, method = 'pmm', seed = 500)\n\n\n iter imp variable\n  1   1  Bare.nuclei\n  1   2  Bare.nuclei\n  1   3  Bare.nuclei\n  1   4  Bare.nuclei\n  1   5  Bare.nuclei\n  2   1  Bare.nuclei\n  2   2  Bare.nuclei\n  2   3  Bare.nuclei\n  2   4  Bare.nuclei\n  2   5  Bare.nuclei\n  3   1  Bare.nuclei\n  3   2  Bare.nuclei\n  3   3  Bare.nuclei\n  3   4  Bare.nuclei\n  3   5  Bare.nuclei\n  4   1  Bare.nuclei\n  4   2  Bare.nuclei\n  4   3  Bare.nuclei\n  4   4  Bare.nuclei\n  4   5  Bare.nuclei\n  5   1  Bare.nuclei\n  5   2  Bare.nuclei\n  5   3  Bare.nuclei\n  5   4  Bare.nuclei\n  5   5  Bare.nuclei\n\n#choose among the 5 imputed dataset\ncompleteData &lt;- complete(imputed_Data,2)\n\n#convert multiple columns to numeric\n#lapply output a list\nBreastCancer2&lt;-lapply(completeData[,-c(11)], as.numeric) #list\nBreastCancer2&lt;-as.data.frame(BreastCancer2)\nBreastCancer2$Class&lt;-BreastCancer$Class\n\n#convert factor to numeric for calculatin of vif\nBreastCancer2$Class&lt;-as.character(BreastCancer2$Class)\nBreastCancer2$Class[BreastCancer2$Class==\"benign\"]&lt;-0\nBreastCancer2$Class[BreastCancer2$Class==\"malignant\"]&lt;-1\nBreastCancer2$Class&lt;-as.numeric(BreastCancer2$Class)\n\nBC &lt;- unique(BreastCancer2) # Remove duplicates\n\n#check correlation\nlibrary(ggcorrplot)\n\nLoading required package: ggplot2\n\nggcorrplot(cor(BC),\n    p.mat=cor_pmat(BC),hc.order=T, type=\"lower\", colors=c(\"red\",\"white\",\"blue\"),tl.cex = 8)\n\n\n\n\n\n\n\n\n\n\n7.1.2 MARS\nMultivariate adaptive regression spline (MARS) is a non-linear regression method that fits a set of splines (hinge functions) to each of the predictor variables i.e. different hinge function for different variables (Friedman and Roosen 1995). As such, the method can be used to plot the relationship between each variable and outcome. Use in this way, the presence of any threshold effect on the predictors can be graphically visualized. The MARS method is implemented in R programming environment in the earth package.\n\nlibrary(earth)\n\nLoading required package: Formula\n\n\nLoading required package: plotmo\n\n\nLoading required package: plotrix\n\n\nLoading required package: TeachingDemos\n\nBC&lt;-BC[-1]\n\nFit&lt;-earth(Class ~.,data= BC,\n           nfold=10,ncross=30, varmod.method = \"none\",\n           glm=list(family=binomial))\n\nplotmo(Fit)\n\n plotmo grid:    Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size\n                            4         1          1             1            2\n Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses\n           1           3               1       1\n\n\n\n\n\n\n\n\nsummary(Fit)\n\nCall: earth(formula=Class~., data=BC, glm=list(family=binomial), nfold=10,\n            ncross=30, varmod.method=\"none\")\n\nGLM coefficients\n                          Class\n(Intercept)          -3.0367094\nh(Cl.thickness-4)     0.7610206\nh(5-Cell.size)       -0.4035770\nh(5-Cell.shape)      -0.6270418\nh(6-Bare.nuclei)     -0.5975594\nh(Bl.cromatin-3)     -0.6211132\nh(5-Bl.cromatin)     -0.8983521\nh(Bl.cromatin-5)      0.9870000\nh(Normal.nucleoli-2)  5.5785046\nh(4-Normal.nucleoli)  2.9930778\nh(Normal.nucleoli-4) -6.7265575\nh(Normal.nucleoli-8)  3.9273365\nh(3-Mitoses)         -1.1559421\n\nGLM (family binomial, link logit):\n nulldev  df       dev  df   devratio     AIC iters converged\n 889.065 689   101.133 677      0.886   127.1     8         1\n\nEarth selected 13 of 18 terms, and 7 of 9 predictors\nTermination condition: Reached nk 21\nImportance: Cell.size, Bare.nuclei, Cl.thickness, Normal.nucleoli, ...\nNumber of terms at each degree of interaction: 1 12 (additive model)\nEarth GCV 0.03174149  RSS 20.3433  GRSq 0.8599283  RSq 0.8695166  CVRSq 0.8481011\n\nNote: the cross-validation sd's below are standard deviations across folds\n\nCross validation:   nterms 12.38 sd 0.75    nvars 7.22 sd 0.50\n\n CVRSq   sd ClassRate   sd MaxErr    sd   AUC    sd MeanDev    sd CalibInt   sd\n 0.848 0.05     0.963 0.02     -1 0.859 0.991 0.008   0.252 0.162     6.01 46.7\n CalibSlope   sd\n       13.6 57.5\n\n\n\n\n7.1.3 Mixed modelling\nIn a standard regression analysis, the data is assumed to be random. Mixed models assume that there are more than one source of random variability in the data. This is expressed in terms of fixed and random effects. Mixed modeling is a useful technique for handling multilevel or group data. The intraclass correlation (ICC) is used to determine if a multilevel analysis is necessary ie if the infarct volume varies among the surgeon or not. ICC is the between group variance to the total variance. If the ICC approaches zero then a simple regression model would suffice.\nThere are several R packages for performing mixed modeling such as lme4. Mixed modeling in meta-regression is illustrated in the section on Metaanalysis. An example of mixed model using Bayesian approach with INLA is provided in the [Bayesian section][INLA, Stan and BUGS]\n\n7.1.3.1 Random intercept model\nIn a random intercept or fixed slope multilevel model the slope or gradient of the fitted lines are assumed to be parallel to each other and the intercept varies for different groups. This can be the case of same treatment effect on animal experiments performed by different technician or same treatment in different clusters of hospitals. There are several approached to performing analysis with random intercept model. The choice of the model depends on the reason for performing the analysis. For example, the maximum likelihood estimation (MLE) method is better than restricted maximum likelihood (RMLE) in that it generates estimates for fixed effects and model comparison. RMLE is preferrred if there are outliers.\n\n\n7.1.3.2 Random slope model\nIn a random slope model, the slopes are not paralleled\n\n\n\n7.1.4 Generalized estimating equation (GEE)\nGEE is used for analysis of longitudinal or clustered data. GEE is preferred when the idea is to discover the group effect or population average (marginal) log odds (Hubbard AE 2010). This is contrast with the mixed model approach to evaluate the average subject via maximum likelihood estimation. The fitting for mixed model is complex compare to GEE and can breakdown. The library for performing GEE is gee or geepack.\n\nlibrary(tidyverse)\nlibrary(gee)\n\n#open simulated data from previous chapter\ndtTime&lt;-read.csv(\"./Data-Use/dtTime_simulated.csv\") %&gt;%\n  rename(NIHSS=Y) %&gt;% mutate (NIHSS=abs(NIHSS))\n  \n\n(fit&lt;-gee(ENI~T+Diabetes+NIHSS,\n         id=id, \n         corstr = \"unstructured\",\n         tol = 0.001, maxiter = 25,\n         #data=dtTrial_long)\n          data=dtTime))\n  summary(fit)\n\n\n\n7.1.5 Group-based Trajectory modelling\nTrajectory analysis attempts to group the behaviour of the subject of interest over time. There are several different approaches to trajectory analysis: data in raw form or after orthonal transformation of the data in principal component analysis. Trajectory analysis is different from mixed modelling in that it examines group behaviour. The output of trajectory analysis is only the beginning of the modeling analysis. For example, the analysis may identify that there are 3 groups. These groups are labelled as group A, B and C. The next step would be to use the results in a modelling analysis of your choice.\n\n7.1.5.1 Akmdoids\nA useful library for performing trajectory analysis is akmedoids. This library anchored the analysis around the median value. The analysis requires the data in long format. This library has been removed from CRAN but an archived version is available for download. It requires installation of other libraries such as kml and clusterCrit. An issue with trajectory analysis is a way to identify the number of groups within the data.\n\nlibrary (simstudy)\nlibrary (ggplot2)\nlibrary (dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary (akmedoids)\n\nset.seed(1234)\n##trial NIHSS outcomes at timepoints\n\n###MRCLEAN\n#Control base NIHSS 18 (14-22)\n#Control 24h NIHSS 16 (12-21)\n#Control 5-7 day NIHSS 8 14 (7-18)\n\n#Intervention base NIHSS17 (14-21)\n#Intervention 24h NIHSS 13 (6-20)\n#Intervention d5-7 NIHSS 8 (2-17)\n\n##NIHSS connected between YO to Y1, Y1 to Y2\n##NIHSS using means, not medians\n\ndef &lt;- defData (varname = \"T\", dist = \"binary\", formula = 0.466)\ndef &lt;- defData (def, varname = \"Y0\", dist = \"normal\", formula = \"18 - 1*T\", variance = \"4 - 0.5*T\")\ndef &lt;- defData(def, varname = \"Y1\", dist = \"normal\", formula = \"Y0 - 1.5 - 2 * T\", variance = \"4.5 + 2.5*T\")\ndef &lt;- defData(def, varname = \"Y2\", dist = \"normal\", formula = \"Y1 - 3.5\", variance = \"6+ 1.5 * T\")\n\n#generate data frame\ndtTrial &lt;- genData(500, def)\n\ndtTrial [dtTrial &lt; 0] &lt;- 0\n\n#putting the 4 time periods together\nMRCLEAN &lt;- addPeriods(dtTrial, nPeriods = 3, idvars = \"id\", timevars = c(\"Y0\", \"Y1\", \"Y2\"), timevarName = \"Y\")\n\n\nggplot(MRCLEAN,aes(x=as.factor(period),y=Y))+geom_line(aes(color=as.factor(T),group=id))+scale_color_manual(values = c(\"#e38e17\", \"#8e17e3\")) +  xlab(\"Time\")+ylab(\"NIHSS\")\n\n\n\n\n\n\n\n###akmedoids MRCLEAN\ndtTrial2 &lt;- dtTrial [,c(1,3,4,5)] \n\n##cluster output\ncluster_output &lt;- akclustr(dtTrial2, id_field = TRUE, method = \"linear\", k = c(3,8))\n\n[1] \"Processing....\"\n[1] \"..............\"\n[1] \"solution of k = 3 determined!\"\n[1] \"solution of k = 4 determined!\"\n[1] \"solution of k = 5 determined!\"\n[1] \"solution of k = 6 determined!\"\n[1] \"solution of k = 7 determined!\"\n[1] \"solution of k = 8 determined!\"\n\n\nPlot\n\n#print cluster solution\ncluster_output$qltyplot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nDetermine the optimal number of groups.\n\n#print cluster solution\ncluster_output$optimal_k\n\n[1] 5.000001\n\n\nPlot trajectory of clusters\n\n#assigning cluster membership to a variable\nclustr &lt;- as.vector(cluster_output$optimal_k) \n\nplot_akstats(cluster_output, k=round(cluster_output$optimal_k,0), type=\"lines\")\n\n$cluster_plot\n\n\n\n\n\n\n\n\n\nShow the data\n\n#return the cluster to the original dataset for further analysis\ndtTrial3&lt;-cbind(dtTrial,clustr)\nhead(dtTrial3)\n\n   id T       Y0        Y1        Y2   clustr\n1:  1 0 18.87386 17.163907 17.792171 5.000001\n2:  2 1 18.98331 15.757665 11.874736 5.000001\n3:  3 1 17.84597 17.543321 14.375073 5.000001\n4:  4 1 18.24073 14.988760 11.199345 5.000001\n5:  5 1 14.87404  9.383915  8.201616 5.000001\n6:  6 1 16.30686 13.851734  8.247250 5.000001\n\n\n\n\n7.1.5.2 Traj\nThe traj library is similar to the one in Stata. It uses several steps including factor and k-mean cluster analyses to identify group structures (Leffondre K 2004).\n\nlibrary(traj)\n\ntraj_out&lt;-Step1Measures(dtTrial2)\n\nUse factor analysis\n\ntraj_out1&lt;-Step2Selection(traj_out)\n\nscree plot\n\ntraj_V&lt;-as.data.frame(t(traj_out1$PC$Vaccounted))\n\nplot(traj_V$`Proportion Var`)\n\n\n\n\n\n\n\n\nThe scree plot shows the optimal number of factors were 4\n\ntraj_out2&lt;-Step3Clusters(traj_out1, nclusters = 4)\n\nhead(traj_out2$data)\n\n     ID                               \n[1,]  1 1 18.87386 17.163907 17.792171\n[2,]  2 2 18.98331 15.757665 11.874736\n[3,]  3 3 17.84597 17.543321 14.375073\n[4,]  4 4 18.24073 14.988760 11.199345\n[5,]  5 5 14.87404  9.383915  8.201616\n[6,]  6 6 16.30686 13.851734  8.247250\n\n\n\n\n\n7.1.6 Group based multivariate trajectory\nIn the section above we discuss handling one type of a repeated collected data over time. Here the focus is on grouping multiple types of repeated collected data based on their trajectory pattern over time. The gbmt library conceptualises the mutiple data types as a latent growth curve and uses an Expectation-Maximization (EM) algorithm (D. S. Nagin and Tremblay 2018).\n\nlibrary(gbmt)\n\nLoading required package: Matrix",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multivariate Analysis</span>"
    ]
  },
  {
    "objectID": "multivariate.html#principal-component-analysis",
    "href": "multivariate.html#principal-component-analysis",
    "title": "7  Multivariate Analysis",
    "section": "7.2 Principal component analysis",
    "text": "7.2 Principal component analysis\nPrincipal component analysis (PCA) is a data dimension reduction method which can be applied to a large dataset to determine the latent variables (principal components) which best represent that set of data. A brief description of the method is described here and a more detailed description of the method can be found in review (Friston et al. 2000). The usual approach to PCA involves eigen analysis of a covariance matrix or singular value decomposition.\nPCA estimates an orthogonal transformation (variance maximising) to convert a set of observations of correlated variables into a set of values of uncorrelated (orthogonal) variables called principal components. The first extracted principal component aligns in the direction that contains most of the variance of observed variables. The next principal component is orthogonal to the first principle component and contains the second most of spread of variance. The next component contains the third most of spread, and so on. The latter principal components are likely to represent noise and are discarded. Expressing this in terms of our imaging data, each component yields a linear combination of ‘ischemic’ voxels that covary with each other. These components can be interpreted as patterns of ischemic injury. The unit of measurement in PCA images is the covariance of the data.\nIn the case of MR images, each voxel is a variable, leading to tens of thousands of variables with relatively small numbers of samples. Specialised methods are required to compute principal components. There are situations in which PCA may not work well if there is non-linear relationship in the data.\nBased on cosine rule, principal components from different data are similar if values approach 1 and dissimilar if values approach 0 (Singhal et al. 2012).\n\n7.2.1 PCA with MRI\nHere we illustrate multivariate analysis with mand library.\n\n#modified commands from mand vignette\nlibrary(mand)\n\nLoading required package: msma\n\ndata(\"atlas\")\ndata(\"atlasdatasets\")\ndata(\"template\")\n\natlasdataset=atlasdatasets$aal\ntmpatlas=atlas$aal\n\n#overlay images\ncoat(template, tmpatlas, regionplot=TRUE, \n atlasdataset=atlasdataset, ROIids = c(1:2, 41:44), regionlegend=TRUE)\n\n\n\n\n\n\n\n\nSimulate a dataset using mand.\n\ndata(\"diffimg\") #mand\ndata(\"baseimg\")\ndata(\"mask\")\ndata(\"sdevimg\")\n\ndiffimg2 = diffimg * (tmpatlas %in% 41:44)\n\nimg1 = simbrain(baseimg = baseimg, diffimg = diffimg2, \nsdevimg=sdevimg, mask=mask, n0=20, c1=0.01, sd1=0.05)\n\n#dim(img1$S) #40 6422\n\n#mand takes matrix array as argument\nfit=msma(img1$S,comp=2)\nplot(fit, v=\"score\", axes = 1:2, plottype=\"scatter\")\n\n\n\n\n\n\n\nmidx = 1 ## the index for the modality\nvidx = 1 ## the index for the component\nQ = fit$wbX[[midx]][,vidx]\noutstat1 = rec(Q, img1$imagedim, mask=img1$brainpos)\ncoat(template, outstat1)\n\n\n\n\n\n\n\n#https://rdrr.io/cran/mand/f/vignettes/a_overview.Rmd\n#https://rdrr.io/cran/caret/man/plsda.html\n\nLet’s apply mand on imaging data. First we use pattern matching to extract the relevant nii files into a list.\n\n#remotes::install_github(\"neuroconductor/MNITemplate\")\nlibrary(MNITemplate)\n\n#MNI choose resolution\nMNI = readMNI(res = \"2mm\")\n\n#create a list using pattern matching ending in .nii\n#regular expression | indicates matching for ica.nii or mca_blur.nii\nmca.list&lt;-list.files(path=\"./Ext-Data/\",\n                     pattern = \"*ica.nii|*mca_blur.nii\", \n                     full.names = TRUE)\n\nWe use the imgdatamat function to read in the files with patients as rows and imaging voxels in columns.\n\n#use imgdatamat to organise data as rows of patients and columns of voxels\n#simscale reduces the size of the voxel to quarter of its size\nm.list.dat&lt;-imgdatamat(mca.list, simscale=1/4)\n\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\nMalformed NIfTI - not reading NIfTI extension, use at own risk!\n\n#check that the dimension is correct\ndim(m.list.dat$S) #42 902629\n\n[1]     29 902629\n\nfit1=msma(m.list.dat$S,comp=2)\nplot(fit1, v=\"score\", axes = 1:2, plottype=\"scatter\")\n\n\n\n\n\n\n\nmidx = 1 ## the index for the modality\nvidx = 1 ## the index for the component\nQ = fit1$wbX[[midx]][,vidx]\noutstat_fit1 = rec(Q, m.list.dat$imagedim, mask=m.list.dat$brainpos)\ncoat(MNI, outstat_fit1)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multivariate Analysis</span>"
    ]
  },
  {
    "objectID": "multivariate.html#independent-component-analysis",
    "href": "multivariate.html#independent-component-analysis",
    "title": "7  Multivariate Analysis",
    "section": "7.3 Independent component analysis",
    "text": "7.3 Independent component analysis\nIndependent component analysis is different from PCA in that it seeks components which are statistically independent.It separates signal from a multivariate distribution into additive components which as statistically independent. It is used in separating components of noise signal or blind source localisation.\n\nlibrary(fastICA)\na &lt;- fastICA(img1$S, 2, alg.typ = \"deflation\", fun = \"logcosh\", alpha = 1,\n             method = \"R\", row.norm = FALSE, maxit = 200, \n             tol = 0.0001, verbose = TRUE)\n\nCentering\n\n\nWhitening\n\n\nDeflation FastICA using logcosh approx. to neg-entropy function\n\n\nComponent 1\n\n\nIteration 1 tol = 0.002084111\n\n\nIteration 2 tol = 4.374718e-05\n\n\nComponent 2\n\n\nIteration 1 tol = 0",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multivariate Analysis</span>"
    ]
  },
  {
    "objectID": "multivariate.html#partial-least-squares",
    "href": "multivariate.html#partial-least-squares",
    "title": "7  Multivariate Analysis",
    "section": "7.4 Partial least squares",
    "text": "7.4 Partial least squares\nThere are several versions of partial least squares (PLS). A detailed mathematical exposition of the PLS-PLR technique used here can be found in the paper by Fort and Lambert-Lacroix (Fort and Lambert-Lacroix 2005). PLS is a multiple regression method that is suited to datasets comprising large sets of independent predictor variables (voxels in an image) and smaller sets of dependent variables (neurological outcome scores). Each voxel can take on a value of 1 (representing involvement by infarction) or 0 (representing absence of involvement) in the MR image of each patient. PLS employs a data reduction method which generates latent variables, linear combinations of independent and dependent variables which explain as much of their covariance as possible.\nLinear least squares regression of the latent variables produces coefﬁcients or beta weights for the latent variables at each voxel location in the brain in stereotaxic coordinate space.(Phan et al. 2010)\nThe colon dataset containing microarray data comes with the plsgenomics library (Durif et al. 2018). The analysis involves partitioning the data into training and test set. The classification data is in the Y column. This example is provided by the plsgenomics library\n\nlibrary(plsgenomics)\n\nFor any news related to the 'plsgenomics' package (update, corrected bugs), please check http://thoth.inrialpes.fr/people/gdurif/\n\n\nC++ based sparse PLS routines will soon be available on the CRAN in the new 'fastPLS' package.\n\ndata(\"Colon\")\nclass(Colon) #list\n\n[1] \"list\"\n\n#62 samples 2000 genes\n#Outcome is in Y column as 1 and 2. 62 rows\n#2000 gene names\n\ndim(Colon$X) \n\n[1]   62 2000\n\n#heatmap\nmatrix.heatmap(cbind(Colon$X,Colon$y))\n\n\n\n\n\n\n\n#\nIndexLearn &lt;- c(sample(which(Colon$Y==2),12),sample(which(Colon$Y==1),8))\nXtrain &lt;- Colon$X[IndexLearn,] \nYtrain &lt;- Colon$Y[IndexLearn] \nXtest &lt;- Colon$X[-IndexLearn,]\n\n# preprocess data \nresP &lt;- preprocess(Xtrain= Xtrain, Xtest=Xtest,Threshold = c(100,16000),Filtering=c(5,500), log10.scale=TRUE,row.stand=TRUE)\n\n# Determine optimum h and lambda\nhlam &lt;- gsim.cv(Xtrain=resP$pXtrain,Ytrain=Ytrain,hARange=c(7,20),\n              LambdaRange=c(0.1,1),hB=NULL)\n\n# perform prediction by GSIM \n# lambda is the ridge regularization parameter from the cross validation\nres &lt;- gsim(Xtrain=resP$pXtrain, \n            Ytrain= Ytrain,Xtest=resP$pXtest,\n            Lambda=hlam$Lambda,hA=hlam$hA,hB=NULL)\nres$Cvg \n\n[1] 1\n\n#difference between predicted and observed\nsum(res$Ytest!=Colon$Y[-IndexLearn])\n\n[1] 6\n\n\n\n\n\n\nD. S. Nagin, V. L. Passos, B. L. Jones, and R. E. Tremblay. 2018. “Group-Based Multi-Trajectory Modeling.” Statistical Methods in Medical Research 27: 2015–23. https://doi.org/10.1177/0962280216673085.\n\n\nDurif, G., L. Modolo, J. Michaelsson, J. E. Mold, S. Lambert-Lacroix, and F. Picard. 2018. “High dimensional classification with combined adaptive sparse PLS and logistic regression.” Bioinformatics 34 (3): 485–93.\n\n\nFort, G., and S. Lambert-Lacroix. 2005. “Classification using partial least squares with penalized logistic regression.” Bioinformatics 21 (7): 1104–11.\n\n\nFriedman, J. H., and C. B. Roosen. 1995. “An introduction to multivariate adaptive regression splines.” Stat Methods Med Res 4 (3): 197–217.\n\n\nFriston, K., J. Phillips, D. Chawla, and C. Buchel. 2000. “Nonlinear PCA: characterizing interactions between modes of brain activity.” Philos. Trans. R. Soc. Lond., B, Biol. Sci. 355 (1393): 135–46.\n\n\nHubbard AE, Fleischer NL, Ahern J. 2010. “To GEE or Not to GEE: Comparing Population Average and Mixed Models for Estimating the Associations Between Neighborhood Risk Factors and Health.” Epidemiology 21: 467–74. https://doi.org/10.1097/EDE.0b013e3181caeb9.\n\n\nLeffondre K, Regeasse A, Abrahamowicz M. 2004. “Statistical Measures Were Proposed for Identifying Longitudinal Patterns of Change in Quantitative Health Indicators.” J Clin Epidemiol 57: 1049–62. https://doi.org/10.1016/j.jclinepi.2004.02.012.\n\n\nPhan, T. G., J. Chen, G. Donnan, V. Srikanth, A. Wood, and D. C. Reutens. 2010. “Development of a new tool to correlate stroke outcome with infarct topography: a proof-of-concept study.” Neuroimage 49 (1): 127–33.\n\n\nPhan, T. G., A. Demchuk, V. Srikanth, B. Silver, S. C. Patel, P. A. Barber, S. R. Levine, and M. D. Hill. 2013. “Proof of concept study: relating infarct location to stroke disability in the NINDS rt-PA trial.” Cerebrovasc. Dis. 35 (6): 560–65.\n\n\nSinghal, Shaloo, Jian Chen, Richard Beare, Henry Ma, John Ly, and Thanh G. Phan. 2012. “Application of Principal Component Analysis to Study Topography of Hypoxic–Ischemic Brain Injury.” NeuroImage 62 (1): 300–306. https://doi.org/https://doi.org/10.1016/j.neuroimage.2012.04.025.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multivariate Analysis</span>"
    ]
  },
  {
    "objectID": "machine-learning.html",
    "href": "machine-learning.html",
    "title": "8  Machine learning",
    "section": "",
    "text": "8.1 Feature selection\nThere are several approaches available for feature selection. These methods include non-deterministic methods such as genetic algorithm and simulated annealing, [LASSO][Penalised regression]. Filter methods for feature selection include the use of correlation matrix to identified correlated data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#decision-tree-analysis",
    "href": "machine-learning.html#decision-tree-analysis",
    "title": "8  Machine learning",
    "section": "8.2 Decision tree analysis",
    "text": "8.2 Decision tree analysis\nDecision tree method generates a logical flow diagram that resembles a tree. This triangulated diagram, with repeated partitioning of the original data into smaller groups (nodes) on a yes or no basis, resembles clinical reasoning. By way of contrast, regression methods generate significant predictors but it’s not clear how those predictors enter the sequential nature of clinical reasoning. Regression models assume that all of the variables are required at once to formulate an accurate prediction. This would make some of the elements of any model from regression analysis superfluous.\nThere are several different approaches to performing decision tree analyses. The most famous method CART is implemented in R as rpart. The second approaches uses chi-square test to partition the tree, available from the party library. Decision tree may also reveal complex intreactions (relationship) among the predictors in a way that regression analyses do not easily reveal.\n\n8.2.1 Information theory driven\nThe tree is grown using a “divide and conquer” strategy, with repeated partitioning of the original data into smaller groups (nodes) on a yes or no basis. The method uses a splitting rule built around the notion of “purity.” A node in the tree is defined as pure when all the elements belong to one class. When there is impurity in the node, a split occurs to maximize reduction in “impurity.” In some cases, the split may be biased toward attributes that contain many different ordinal levels or scales. Thus, the selection of an attribute as the root node may vary according to the splitting rule and the scaling of the attribute. The decision tree package rpart does tolerate certain degree of missing number because the data are split using the available data for that attribute to calculate the Gini index (rather than the entire cohort). One major advantage of rpart is the presentation of the classification rules in the easily interpretable form of a tree. The hierarchical nature of the decision tree is similar to many decision processes (Phan et al. 2018). A criticism of decision tree is that it’s prone to overfitting and or preference for variable with many levels. Decision tree do not handle collinearity issues well.\n\nlibrary(rpart)\nlibrary(rattle)\n\nLoading required package: tibble\n\n\nLoading required package: bitops\n\n\nRattle: A free graphical interface for data science with R.\nVersion 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.\nType 'rattle()' to shake, rattle, and roll your data.\n\nlibrary(rpart.plot)\ndata(\"Leukemia\", package = \"Stat2Data\")\ncolnames(Leukemia)\n\n[1] \"Age\"    \"Smear\"  \"Infil\"  \"Index\"  \"Blasts\" \"Temp\"   \"Resp\"   \"Time\"  \n[9] \"Status\"\n\n#decision tree model for AML treatment\ntreLeukemia&lt;-rpart(Status~., data=Leukemia)\nfancyRpartPlot(treLeukemia)\n\n\n\n\n\n\n\n\n\n\n8.2.2 Conditional decision tree\nThe conditional decision tree approach has been proposed to be superior to CART method because that method uses information criterion for partitioning and which can lead to overﬁtting.The scenario of overﬁtting describes model which works well on training data but less so with new data.The conditional approach by party is less prone to overﬁtting as it includes signiﬁcance testing (Phan et al. 2019).\n\nlibrary(party)\n\nLoading required package: grid\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: modeltools\n\n\nLoading required package: stats4\n\n\nLoading required package: strucchange\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\ndata(\"aSAH\", package = \"pROC\")\ncolnames(aSAH)\n\n[1] \"gos6\"    \"outcome\" \"gender\"  \"age\"     \"wfns\"    \"s100b\"   \"ndka\"   \n\n#decision tree model\ntreeSAH&lt;-ctree(outcome~., data=aSAH, control = ctree_control(mincriterion=0.95, minsplit=50))\nplot(treeSAH,type = \"simple\",main = \"Conditional Inference for aSAH\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#ensemble-tree-methods",
    "href": "machine-learning.html#ensemble-tree-methods",
    "title": "8  Machine learning",
    "section": "8.3 Ensemble tree methods",
    "text": "8.3 Ensemble tree methods\n\n8.3.1 Bagging trees\nBoth gradient boost machine and random forest are examples of tree-based method with the former based on boosting of the residuals of the model and the latter based on bagging with random selection (rows and columns) of multiple subsets of the data. As such random forest regression ensembles the model from multiple decision trees. The trees are created by obtaining multiple subset of the data without replacement (random selection of data by rows and columns). Decision tree comes at certain disadvantage such as overfitting. Random forest avoids the problems of single decision tree analyses by aggregating the results of multiple trees obtained by performing analysis on random subsets of the original data. This method is different from the bootstrapping procedure in which the data is subsetted with replacement. Theoretically, decision tree can look very similar as the data structure is not significantly changed. There is a theoretical risk of overfitting with random forest and underfitting with boosting tree methods.\n\n8.3.1.1 Random Forest\nRandom forest is available as randomForest or ranger or via caret. A major drawback to random forest is that the hierarchical nature of the trees is lost. As such this method is seen as a black box tool and is less commonly embraced in the medical literature. One way us to use an interpretable machine learning tool iml (Molnar, Bischl, and Casalicchio 2018) (Shapley values) tool to aid interpretation of the model. This method uses ideas from coalition game theory to fairly distribute the contribution of the coalition of covariates to the random forest model.\nThe machine learning models are tuned using caret library.\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\ndata(\"BreastCancer\",package = \"mlbench\")\n#The Breast Cancer data contains NA as well as factors\n#note Class is benign or malignant of class factor\n#column Bare.nuclei removed due to NA\nBreastCancer&lt;-BreastCancer[,-c(1,7)]\n\n#split data using caTools. \n#The next example will use createDataPartition from caret\nset.seed(123)\nsplit = caTools::sample.split(BreastCancer$Class, SplitRatio = 0.75)\nTrain = subset(BreastCancer, split == TRUE)\nTest = subset(BreastCancer, split == FALSE)\n\n\n# specify that resampling method is \nrf_control &lt;- trainControl(## 10-fold CV\n                           method = \"cv\",\n                           number = 10)\n\n#scaling data is performed here under preProcess\n#note that ranger handles the outcome variable as factor\nrf &lt;- caret::train(Class ~ ., \n                    data = Train, \n                  method = \"ranger\",\n                 trControl=rf_control,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneLength = 10, verbose=F)\n\n\nsummary(rf)\n\n                          Length Class         Mode     \npredictions               525    factor        numeric  \nnum.trees                   1    -none-        numeric  \nnum.independent.variables   1    -none-        numeric  \nmtry                        1    -none-        numeric  \nmin.node.size               1    -none-        numeric  \nprediction.error            1    -none-        numeric  \nforest                      9    ranger.forest list     \nconfusion.matrix            4    table         numeric  \nsplitrule                   1    -none-        character\nnum.random.splits           1    -none-        numeric  \ntreetype                    1    -none-        character\ncall                        9    -none-        call     \nimportance.mode             1    -none-        character\nnum.samples                 1    -none-        numeric  \nreplace                     1    -none-        logical  \nxNames                     71    -none-        character\nproblemType                 1    -none-        character\ntuneValue                   3    data.frame    list     \nobsLevels                   2    -none-        character\nparam                       1    -none-        list     \n\npred_rf&lt;-predict(rf,BreastCancer)\nconfusionMatrix(pred_rf, BreastCancer$Class)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       455         6\n  malignant      3       235\n                                          \n               Accuracy : 0.9871          \n                 95% CI : (0.9757, 0.9941)\n    No Information Rate : 0.6552          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9714          \n                                          \n Mcnemar's Test P-Value : 0.505           \n                                          \n            Sensitivity : 0.9934          \n            Specificity : 0.9751          \n         Pos Pred Value : 0.9870          \n         Neg Pred Value : 0.9874          \n             Prevalence : 0.6552          \n         Detection Rate : 0.6509          \n   Detection Prevalence : 0.6595          \n      Balanced Accuracy : 0.9843          \n                                          \n       'Positive' Class : benign          \n                                          \n\nroc_rf&lt;-pROC::roc(BreastCancer$Class, as.numeric(pred_rf))\n\nSetting levels: control = benign, case = malignant\n\n\nSetting direction: controls &lt; cases\n\nroc_rf\n\n\nCall:\nroc.default(response = BreastCancer$Class, predictor = as.numeric(pred_rf))\n\nData: as.numeric(pred_rf) in 458 controls (BreastCancer$Class benign) &lt; 241 cases (BreastCancer$Class malignant).\nArea under the curve: 0.9843\n\n\n\n\n8.3.1.2 Random survival forest with rfsrc\nRandom survival forest example is provided below using rfsrc library. The survex library is used for explanation on the model. This library is also available as a learner in the mlr3verse.\n\nlibrary(survival)\n\n\nAttaching package: 'survival'\n\n\nThe following object is masked from 'package:caret':\n\n    cluster\n\nlibrary(survminer)\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'survminer'\n\n\nThe following object is masked from 'package:survival':\n\n    myeloma\n\nlibrary(randomForestSRC)\n\n\n randomForestSRC 3.2.2 \n \n Type rfsrc.news() to see new features, changes, and bug fixes. \n \n\nlibrary(survex)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:survex':\n\n    explain\n\n\nThe following object is masked from 'package:party':\n\n    where\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#time in days\n#status censored=1, dead=2\n#sex:Male=1 Female=2\n\ncancer2&lt;- cancer %&gt;% mutate(\n  status=ifelse(status==1,0,1)) %&gt;%\n  rename(Dead=status, Days=time)\n\ntime=cancer2$Days\nstatus=cancer2$Dead\n\nRF&lt;- rfsrc(Surv(Days, Dead) ~ age+sex+ph.ecog+ph.karno+wt.loss, data = cancer2)\n\n#specify library to avoid confusion with dplyr\nexplainer&lt;-survex::explain(RF)\n\nPreparation of a new explainer is initiated \n  -&gt; model label       :  rfsrc ( \u001b[33m default \u001b[39m ) \n  -&gt; data              :  213  rows  5  cols (  extracted from the model  ) \n  -&gt; target variable   :  213  values ( 151 events and 62 censored , censoring rate = 0.291 ) (  extracted from the model  ) \n  -&gt; times             :  50 unique time points , min = 5 , mean = 304.1224 , median = 263.16 , max = 835.44 \n  -&gt; times             :  (  generated from y as 50 time points being consecutive quantiles (0.00, 0.02, ..., 0.98)  ) \n  -&gt; predict function  :  sum over the predict_cumulative_hazard_function will be used ( \u001b[33m default \u001b[39m ) \n  -&gt; predict survival function  :  stepfun based on predict.rfsrc()$survival will be used ( \u001b[33m default \u001b[39m ) \n  -&gt; predict cumulative hazard function  :  stepfun based on predict.rfsrc()$chf will be used ( \u001b[33m default \u001b[39m ) \n  -&gt; model_info        :  package randomForestSRC , ver. 3.2.2 , task survival ( \u001b[33m default \u001b[39m ) \n  A new explainer has been created!  \n\n\nPlot a single tree from the random survival forest model.\n\nplot(get.tree(RF,4))\n\n\n\n\n\nDynamic AUC\n\ny &lt;- explainer$y\ntimes &lt;- explainer$times\n\nsurv &lt;- explainer$predict_survival_function(RF, explainer$data, times)\n\ncd_auc(y, surv = surv, times = times)\n\n [1] 1.0000000 0.9307568 0.9112200 0.9096154 0.8829532 0.8786292 0.8560335\n [8] 0.8517304 0.8459069 0.8241969 0.8231190 0.8146465 0.7873396 0.7604641\n[15] 0.7722174 0.7696099 0.7667343 0.7605675 0.7514324 0.7500935 0.7638787\n[22] 0.7621149 0.7673878 0.7606034 0.7352058 0.7093474 0.7040276 0.7112689\n[29] 0.7135705 0.7084011 0.7085640 0.7135241 0.7028266 0.6921722 0.6955375\n[36] 0.7029153 0.6907407 0.6919927 0.7015251 0.7019995 0.7273598 0.7425582\n[43] 0.7499178 0.7495446 0.7548334 0.7822465 0.8160264 0.8361538 0.8801743\n[50] 0.9048077\n\n\nPlot variable importance for random survival forest using permutation of features and measure impact on Brier score.\n\nModelRF&lt;-survex::model_parts(explainer)\n\n\nplot(ModelRF)\n\n\n\n\n\n\n\n\nPlot partial dependence\n\nModel_PD&lt;-model_profile(explainer)\nplot(Model_PD)\n\n\n\n\n\n\n\n\n\n\n8.3.1.3 Random survival forest with ranger\nRandom forest can be used for performing survival analysis using ranger, randomforestSRC. The example below is an example using the lung cancer trial.\n\n#data from survival package on NCCTG lung cancer trial\n#https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html\ndata(cancer, package=\"survival\")\n\n#time in days\n#status censored=1, dead=2\n#sex:Male=1 Female=2\n\nlibrary(ranger)\n\n\nAttaching package: 'ranger'\n\n\nThe following object is masked from 'package:rattle':\n\n    importance\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.2     ✔ stringr   1.5.0\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ stringr::boundary() masks strucchange::boundary()\n✖ dplyr::explain()    masks survex::explain()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\n✖ purrr::lift()       masks caret::lift()\n✖ purrr::partial()    masks randomForestSRC::partial()\n✖ dplyr::where()      masks party::where()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(survival)\n\ncancer2&lt;-cancer %&gt;% dplyr::select(time, status, age,sex, ph.ecog) %&gt;% na.omit()  \n\nsurvival_formula&lt;-formula(paste('Surv(', 'time', ',', 'status', ') ~ ','age+sex+ph.ecog'))\n  \nsurvival_forest &lt;- ranger(survival_formula,\n                         data = cancer2,  \n                         seed = 1234,\n                         importance = 'permutation',\n                         mtry = 2,\n                         verbose = TRUE,\n                         num.trees = 200,\n                         write.forest=TRUE)\n\nprint(\"error:\"); print(survival_forest$prediction.error)\n\n[1] \"error:\"\n\n\n[1] 0.391924\n\n\nPrint variable importance\n\nsort(survival_forest$variable.importance)\n\n        age         sex     ph.ecog \n0.008386083 0.030801816 0.065970251 \n\n\nProbability of survival\n\nplot(survival_forest$unique.death.times, survival_forest$survival[1,], type='l', col='orange', ylim=c(0.01,1))\nlines(survival_forest$unique.death.times, survival_forest$survival[56,], col='blue')\n\n\n\n\n\n\n\n\n\nplot(survival_forest$unique.death.times, survival_forest$survival[1,], type='l', col='orange', ylim=c(0.01,1))\nfor (x in c(2:100)) {\n  lines(survival_forest$unique.death.times, survival_forest$survival[x,], col='red')\n}\n\n\n\n\n\n\n\n\n\n\n\n8.3.2 Boosting trees\nBoosting trees take a different approach from bagging by sequentially working on the weak learners to improve the residual of the model. This statement means that the model uses the features early whereas the averaging from bagging is done towards the end.\n\n8.3.2.1 Gradient Boost Machine\nGradient boost machine is available as gradient boost machine_gbm.\n\n#the breast cancer data from random forest is used here\n\n# specify that the resampling method is \ngbm_control &lt;- trainControl(## 10-fold CV\n                           method = \"repeatedcv\",\n                           number = 10)\n\n#scaling data is performed here under preProcess\n#note that ranger handles the outcome variable as factor\ngbm &lt;- caret::train(Class ~ ., \n                    data = Train, \n                  method = \"gbm\",\n                 trControl=gbm_control,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneLength = 10)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1522             nan     0.1000    0.0657\n     2        1.0452             nan     0.1000    0.0520\n     3        0.9465             nan     0.1000    0.0486\n     4        0.8699             nan     0.1000    0.0376\n     5        0.8031             nan     0.1000    0.0314\n     6        0.7480             nan     0.1000    0.0279\n     7        0.6949             nan     0.1000    0.0264\n     8        0.6500             nan     0.1000    0.0206\n     9        0.6110             nan     0.1000    0.0178\n    10        0.5772             nan     0.1000    0.0173\n    20        0.3674             nan     0.1000    0.0043\n    40        0.2328             nan     0.1000   -0.0004\n    60        0.1811             nan     0.1000   -0.0003\n    80        0.1553             nan     0.1000   -0.0002\n   100        0.1395             nan     0.1000   -0.0000\n   120        0.1248             nan     0.1000   -0.0004\n   140        0.1138             nan     0.1000    0.0003\n   160        0.1063             nan     0.1000   -0.0006\n   180        0.1042             nan     0.1000   -0.0003\n   200        0.0970             nan     0.1000   -0.0013\n   220        0.0955             nan     0.1000   -0.0010\n   240        0.0905             nan     0.1000   -0.0000\n   260        0.0870             nan     0.1000   -0.0001\n   280        0.0838             nan     0.1000    0.0003\n   300        0.0798             nan     0.1000   -0.0006\n   320        0.0756             nan     0.1000   -0.0002\n   340        0.0742             nan     0.1000   -0.0009\n   360        0.0718             nan     0.1000   -0.0008\n   380        0.0701             nan     0.1000   -0.0005\n   400        0.0692             nan     0.1000   -0.0012\n   420        0.0675             nan     0.1000   -0.0006\n   440        0.0662             nan     0.1000   -0.0006\n   460        0.0648             nan     0.1000   -0.0010\n   480        0.0633             nan     0.1000   -0.0004\n   500        0.0621             nan     0.1000   -0.0006\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1503             nan     0.1000    0.0720\n     2        1.0312             nan     0.1000    0.0637\n     3        0.9344             nan     0.1000    0.0459\n     4        0.8459             nan     0.1000    0.0426\n     5        0.7714             nan     0.1000    0.0350\n     6        0.7103             nan     0.1000    0.0297\n     7        0.6549             nan     0.1000    0.0252\n     8        0.6086             nan     0.1000    0.0224\n     9        0.5655             nan     0.1000    0.0203\n    10        0.5309             nan     0.1000    0.0142\n    20        0.3027             nan     0.1000    0.0035\n    40        0.1678             nan     0.1000    0.0004\n    60        0.1251             nan     0.1000   -0.0007\n    80        0.1006             nan     0.1000   -0.0000\n   100        0.0839             nan     0.1000   -0.0003\n   120        0.0722             nan     0.1000   -0.0004\n   140        0.0644             nan     0.1000   -0.0007\n   160        0.0563             nan     0.1000    0.0002\n   180        0.0519             nan     0.1000   -0.0002\n   200        0.0462             nan     0.1000   -0.0006\n   220        0.0417             nan     0.1000   -0.0006\n   240        0.0373             nan     0.1000   -0.0001\n   260        0.0346             nan     0.1000   -0.0006\n   280        0.0306             nan     0.1000   -0.0003\n   300        0.0281             nan     0.1000   -0.0002\n   320        0.0247             nan     0.1000   -0.0002\n   340        0.0232             nan     0.1000   -0.0002\n   360        0.0220             nan     0.1000   -0.0002\n   380        0.0202             nan     0.1000   -0.0002\n   400        0.0186             nan     0.1000   -0.0002\n   420        0.0169             nan     0.1000   -0.0002\n   440        0.0157             nan     0.1000   -0.0001\n   460        0.0148             nan     0.1000   -0.0002\n   480        0.0137             nan     0.1000   -0.0001\n   500        0.0124             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1380             nan     0.1000    0.0744\n     2        1.0108             nan     0.1000    0.0622\n     3        0.9064             nan     0.1000    0.0503\n     4        0.8191             nan     0.1000    0.0399\n     5        0.7521             nan     0.1000    0.0326\n     6        0.6867             nan     0.1000    0.0318\n     7        0.6333             nan     0.1000    0.0260\n     8        0.5844             nan     0.1000    0.0241\n     9        0.5419             nan     0.1000    0.0179\n    10        0.5079             nan     0.1000    0.0152\n    20        0.2765             nan     0.1000    0.0045\n    40        0.1446             nan     0.1000   -0.0006\n    60        0.0992             nan     0.1000   -0.0004\n    80        0.0768             nan     0.1000   -0.0003\n   100        0.0569             nan     0.1000   -0.0006\n   120        0.0445             nan     0.1000   -0.0003\n   140        0.0362             nan     0.1000   -0.0001\n   160        0.0296             nan     0.1000   -0.0004\n   180        0.0250             nan     0.1000   -0.0003\n   200        0.0216             nan     0.1000   -0.0003\n   220        0.0186             nan     0.1000   -0.0002\n   240        0.0160             nan     0.1000   -0.0002\n   260        0.0137             nan     0.1000   -0.0002\n   280        0.0120             nan     0.1000   -0.0001\n   300        0.0104             nan     0.1000   -0.0002\n   320        0.0088             nan     0.1000    0.0000\n   340        0.0076             nan     0.1000   -0.0001\n   360        0.0066             nan     0.1000   -0.0001\n   380        0.0054             nan     0.1000   -0.0000\n   400        0.0048             nan     0.1000   -0.0000\n   420        0.0042             nan     0.1000   -0.0001\n   440        0.0035             nan     0.1000   -0.0001\n   460        0.0030             nan     0.1000   -0.0000\n   480        0.0026             nan     0.1000   -0.0000\n   500        0.0023             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1380             nan     0.1000    0.0745\n     2        1.0151             nan     0.1000    0.0609\n     3        0.9111             nan     0.1000    0.0507\n     4        0.8246             nan     0.1000    0.0388\n     5        0.7497             nan     0.1000    0.0348\n     6        0.6838             nan     0.1000    0.0318\n     7        0.6301             nan     0.1000    0.0260\n     8        0.5768             nan     0.1000    0.0258\n     9        0.5342             nan     0.1000    0.0171\n    10        0.4962             nan     0.1000    0.0160\n    20        0.2672             nan     0.1000    0.0028\n    40        0.1320             nan     0.1000   -0.0007\n    60        0.0829             nan     0.1000   -0.0009\n    80        0.0599             nan     0.1000   -0.0007\n   100        0.0443             nan     0.1000   -0.0002\n   120        0.0336             nan     0.1000   -0.0005\n   140        0.0261             nan     0.1000   -0.0002\n   160        0.0201             nan     0.1000   -0.0002\n   180        0.0165             nan     0.1000   -0.0002\n   200        0.0123             nan     0.1000   -0.0000\n   220        0.0096             nan     0.1000   -0.0001\n   240        0.0078             nan     0.1000   -0.0000\n   260        0.0061             nan     0.1000   -0.0000\n   280        0.0050             nan     0.1000   -0.0000\n   300        0.0040             nan     0.1000   -0.0000\n   320        0.0032             nan     0.1000   -0.0000\n   340        0.0025             nan     0.1000   -0.0000\n   360        0.0019             nan     0.1000   -0.0000\n   380        0.0016             nan     0.1000   -0.0000\n   400        0.0013             nan     0.1000   -0.0000\n   420        0.0010             nan     0.1000   -0.0000\n   440        0.0009             nan     0.1000   -0.0000\n   460        0.0008             nan     0.1000   -0.0000\n   480        0.0007             nan     0.1000   -0.0000\n   500        0.0005             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1362             nan     0.1000    0.0781\n     2        1.0087             nan     0.1000    0.0624\n     3        0.8981             nan     0.1000    0.0531\n     4        0.8123             nan     0.1000    0.0414\n     5        0.7390             nan     0.1000    0.0332\n     6        0.6771             nan     0.1000    0.0292\n     7        0.6219             nan     0.1000    0.0254\n     8        0.5681             nan     0.1000    0.0241\n     9        0.5241             nan     0.1000    0.0211\n    10        0.4851             nan     0.1000    0.0180\n    20        0.2645             nan     0.1000    0.0039\n    40        0.1160             nan     0.1000   -0.0017\n    60        0.0678             nan     0.1000    0.0004\n    80        0.0452             nan     0.1000   -0.0000\n   100        0.0320             nan     0.1000   -0.0005\n   120        0.0241             nan     0.1000   -0.0002\n   140        0.0187             nan     0.1000   -0.0002\n   160        0.0139             nan     0.1000   -0.0001\n   180        0.0105             nan     0.1000   -0.0000\n   200        0.0078             nan     0.1000   -0.0001\n   220        0.0063             nan     0.1000   -0.0001\n   240        0.0045             nan     0.1000   -0.0000\n   260        0.0034             nan     0.1000   -0.0000\n   280        0.0027             nan     0.1000   -0.0000\n   300        0.0022             nan     0.1000   -0.0001\n   320        0.0019             nan     0.1000   -0.0000\n   340        0.0014             nan     0.1000   -0.0000\n   360        0.0011             nan     0.1000   -0.0000\n   380        0.0008             nan     0.1000   -0.0000\n   400        0.0006             nan     0.1000   -0.0000\n   420        0.0005             nan     0.1000   -0.0000\n   440        0.0004             nan     0.1000   -0.0000\n   460        0.0003             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0002             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1368             nan     0.1000    0.0740\n     2        1.0143             nan     0.1000    0.0581\n     3        0.9146             nan     0.1000    0.0474\n     4        0.8273             nan     0.1000    0.0431\n     5        0.7535             nan     0.1000    0.0343\n     6        0.6916             nan     0.1000    0.0293\n     7        0.6344             nan     0.1000    0.0249\n     8        0.5814             nan     0.1000    0.0239\n     9        0.5391             nan     0.1000    0.0180\n    10        0.5006             nan     0.1000    0.0150\n    20        0.2654             nan     0.1000    0.0034\n    40        0.1111             nan     0.1000   -0.0015\n    60        0.0611             nan     0.1000   -0.0007\n    80        0.0404             nan     0.1000    0.0000\n   100        0.0252             nan     0.1000   -0.0001\n   120        0.0189             nan     0.1000   -0.0006\n   140        0.0139             nan     0.1000   -0.0002\n   160        0.0103             nan     0.1000   -0.0001\n   180        0.0071             nan     0.1000   -0.0001\n   200        0.0053             nan     0.1000   -0.0000\n   220        0.0039             nan     0.1000   -0.0000\n   240        0.0027             nan     0.1000    0.0000\n   260        0.0020             nan     0.1000   -0.0000\n   280        0.0015             nan     0.1000   -0.0000\n   300        0.0010             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000    0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1362             nan     0.1000    0.0753\n     2        1.0097             nan     0.1000    0.0619\n     3        0.9048             nan     0.1000    0.0515\n     4        0.8157             nan     0.1000    0.0408\n     5        0.7377             nan     0.1000    0.0370\n     6        0.6757             nan     0.1000    0.0299\n     7        0.6180             nan     0.1000    0.0273\n     8        0.5687             nan     0.1000    0.0226\n     9        0.5281             nan     0.1000    0.0168\n    10        0.4904             nan     0.1000    0.0161\n    20        0.2650             nan     0.1000    0.0044\n    40        0.1114             nan     0.1000   -0.0012\n    60        0.0652             nan     0.1000   -0.0011\n    80        0.0388             nan     0.1000   -0.0002\n   100        0.0227             nan     0.1000   -0.0003\n   120        0.0159             nan     0.1000   -0.0003\n   140        0.0118             nan     0.1000   -0.0003\n   160        0.0084             nan     0.1000   -0.0001\n   180        0.0062             nan     0.1000   -0.0000\n   200        0.0046             nan     0.1000   -0.0001\n   220        0.0032             nan     0.1000   -0.0001\n   240        0.0023             nan     0.1000   -0.0000\n   260        0.0017             nan     0.1000   -0.0000\n   280        0.0010             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1338             nan     0.1000    0.0751\n     2        1.0053             nan     0.1000    0.0581\n     3        0.9014             nan     0.1000    0.0491\n     4        0.8204             nan     0.1000    0.0394\n     5        0.7485             nan     0.1000    0.0339\n     6        0.6793             nan     0.1000    0.0328\n     7        0.6213             nan     0.1000    0.0279\n     8        0.5710             nan     0.1000    0.0228\n     9        0.5266             nan     0.1000    0.0200\n    10        0.4876             nan     0.1000    0.0182\n    20        0.2600             nan     0.1000    0.0037\n    40        0.1073             nan     0.1000    0.0005\n    60        0.0607             nan     0.1000   -0.0004\n    80        0.0374             nan     0.1000   -0.0009\n   100        0.0245             nan     0.1000   -0.0000\n   120        0.0169             nan     0.1000   -0.0003\n   140        0.0099             nan     0.1000   -0.0002\n   160        0.0069             nan     0.1000    0.0000\n   180        0.0048             nan     0.1000   -0.0000\n   200        0.0038             nan     0.1000   -0.0001\n   220        0.0027             nan     0.1000    0.0000\n   240        0.0017             nan     0.1000   -0.0000\n   260        0.0012             nan     0.1000   -0.0000\n   280        0.0009             nan     0.1000   -0.0000\n   300        0.0007             nan     0.1000   -0.0000\n   320        0.0005             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1314             nan     0.1000    0.0719\n     2        1.0112             nan     0.1000    0.0607\n     3        0.9147             nan     0.1000    0.0448\n     4        0.8263             nan     0.1000    0.0416\n     5        0.7520             nan     0.1000    0.0375\n     6        0.6875             nan     0.1000    0.0291\n     7        0.6299             nan     0.1000    0.0272\n     8        0.5805             nan     0.1000    0.0224\n     9        0.5349             nan     0.1000    0.0193\n    10        0.4987             nan     0.1000    0.0153\n    20        0.2555             nan     0.1000    0.0051\n    40        0.0999             nan     0.1000    0.0014\n    60        0.0533             nan     0.1000   -0.0008\n    80        0.0325             nan     0.1000   -0.0004\n   100        0.0219             nan     0.1000   -0.0001\n   120        0.0159             nan     0.1000    0.0000\n   140        0.0099             nan     0.1000   -0.0003\n   160        0.0079             nan     0.1000   -0.0003\n   180        0.0054             nan     0.1000   -0.0002\n   200        0.0033             nan     0.1000   -0.0000\n   220        0.0022             nan     0.1000   -0.0000\n   240        0.0018             nan     0.1000   -0.0001\n   260        0.0013             nan     0.1000   -0.0000\n   280        0.0010             nan     0.1000    0.0000\n   300        0.0007             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0001             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1379             nan     0.1000    0.0704\n     2        1.0121             nan     0.1000    0.0599\n     3        0.9038             nan     0.1000    0.0500\n     4        0.8196             nan     0.1000    0.0421\n     5        0.7441             nan     0.1000    0.0339\n     6        0.6851             nan     0.1000    0.0267\n     7        0.6277             nan     0.1000    0.0271\n     8        0.5790             nan     0.1000    0.0206\n     9        0.5351             nan     0.1000    0.0204\n    10        0.4976             nan     0.1000    0.0161\n    20        0.2644             nan     0.1000    0.0048\n    40        0.1094             nan     0.1000    0.0002\n    60        0.0598             nan     0.1000   -0.0009\n    80        0.0374             nan     0.1000    0.0001\n   100        0.0245             nan     0.1000   -0.0002\n   120        0.0168             nan     0.1000   -0.0003\n   140        0.0118             nan     0.1000   -0.0003\n   160        0.0087             nan     0.1000   -0.0002\n   180        0.0062             nan     0.1000    0.0000\n   200        0.0043             nan     0.1000   -0.0000\n   220        0.0030             nan     0.1000   -0.0000\n   240        0.0021             nan     0.1000    0.0000\n   260        0.0016             nan     0.1000   -0.0000\n   280        0.0012             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1483             nan     0.1000    0.0660\n     2        1.0427             nan     0.1000    0.0513\n     3        0.9512             nan     0.1000    0.0461\n     4        0.8777             nan     0.1000    0.0373\n     5        0.8136             nan     0.1000    0.0320\n     6        0.7544             nan     0.1000    0.0285\n     7        0.7039             nan     0.1000    0.0226\n     8        0.6653             nan     0.1000    0.0171\n     9        0.6268             nan     0.1000    0.0173\n    10        0.5916             nan     0.1000    0.0158\n    20        0.3829             nan     0.1000    0.0056\n    40        0.2474             nan     0.1000    0.0008\n    60        0.2045             nan     0.1000    0.0008\n    80        0.1797             nan     0.1000   -0.0003\n   100        0.1651             nan     0.1000   -0.0008\n   120        0.1514             nan     0.1000   -0.0006\n   140        0.1451             nan     0.1000   -0.0007\n   160        0.1360             nan     0.1000   -0.0004\n   180        0.1307             nan     0.1000   -0.0002\n   200        0.1278             nan     0.1000   -0.0006\n   220        0.1229             nan     0.1000   -0.0018\n   240        0.1167             nan     0.1000   -0.0009\n   260        0.1144             nan     0.1000   -0.0021\n   280        0.1119             nan     0.1000   -0.0006\n   300        0.1065             nan     0.1000   -0.0010\n   320        0.1034             nan     0.1000   -0.0010\n   340        0.0993             nan     0.1000   -0.0008\n   360        0.0994             nan     0.1000   -0.0019\n   380        0.0979             nan     0.1000   -0.0005\n   400        0.0972             nan     0.1000   -0.0005\n   420        0.0957             nan     0.1000   -0.0008\n   440        0.0936             nan     0.1000   -0.0011\n   460        0.0926             nan     0.1000   -0.0005\n   480        0.0923             nan     0.1000   -0.0004\n   500        0.0918             nan     0.1000   -0.0006\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1342             nan     0.1000    0.0714\n     2        1.0162             nan     0.1000    0.0592\n     3        0.9188             nan     0.1000    0.0460\n     4        0.8413             nan     0.1000    0.0370\n     5        0.7696             nan     0.1000    0.0350\n     6        0.7082             nan     0.1000    0.0279\n     7        0.6599             nan     0.1000    0.0223\n     8        0.6125             nan     0.1000    0.0225\n     9        0.5724             nan     0.1000    0.0190\n    10        0.5379             nan     0.1000    0.0143\n    20        0.3182             nan     0.1000    0.0055\n    40        0.1896             nan     0.1000   -0.0005\n    60        0.1514             nan     0.1000   -0.0008\n    80        0.1264             nan     0.1000   -0.0013\n   100        0.1106             nan     0.1000   -0.0003\n   120        0.0989             nan     0.1000   -0.0006\n   140        0.0863             nan     0.1000   -0.0004\n   160        0.0755             nan     0.1000   -0.0004\n   180        0.0664             nan     0.1000   -0.0003\n   200        0.0601             nan     0.1000   -0.0004\n   220        0.0549             nan     0.1000   -0.0003\n   240        0.0501             nan     0.1000   -0.0004\n   260        0.0459             nan     0.1000   -0.0005\n   280        0.0414             nan     0.1000   -0.0003\n   300        0.0383             nan     0.1000   -0.0004\n   320        0.0353             nan     0.1000   -0.0003\n   340        0.0323             nan     0.1000   -0.0002\n   360        0.0296             nan     0.1000   -0.0002\n   380        0.0279             nan     0.1000   -0.0002\n   400        0.0265             nan     0.1000   -0.0002\n   420        0.0241             nan     0.1000   -0.0001\n   440        0.0222             nan     0.1000   -0.0001\n   460        0.0201             nan     0.1000   -0.0002\n   480        0.0185             nan     0.1000   -0.0002\n   500        0.0164             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1377             nan     0.1000    0.0749\n     2        1.0149             nan     0.1000    0.0601\n     3        0.9131             nan     0.1000    0.0486\n     4        0.8280             nan     0.1000    0.0357\n     5        0.7546             nan     0.1000    0.0372\n     6        0.6949             nan     0.1000    0.0270\n     7        0.6405             nan     0.1000    0.0255\n     8        0.5907             nan     0.1000    0.0226\n     9        0.5499             nan     0.1000    0.0184\n    10        0.5107             nan     0.1000    0.0177\n    20        0.2912             nan     0.1000    0.0062\n    40        0.1619             nan     0.1000   -0.0012\n    60        0.1097             nan     0.1000    0.0009\n    80        0.0842             nan     0.1000   -0.0006\n   100        0.0684             nan     0.1000   -0.0010\n   120        0.0564             nan     0.1000   -0.0008\n   140        0.0471             nan     0.1000   -0.0003\n   160        0.0407             nan     0.1000   -0.0004\n   180        0.0319             nan     0.1000   -0.0001\n   200        0.0251             nan     0.1000   -0.0001\n   220        0.0206             nan     0.1000   -0.0001\n   240        0.0177             nan     0.1000   -0.0002\n   260        0.0156             nan     0.1000   -0.0001\n   280        0.0134             nan     0.1000   -0.0001\n   300        0.0115             nan     0.1000   -0.0001\n   320        0.0101             nan     0.1000   -0.0001\n   340        0.0088             nan     0.1000   -0.0001\n   360        0.0076             nan     0.1000   -0.0001\n   380        0.0065             nan     0.1000   -0.0001\n   400        0.0056             nan     0.1000   -0.0000\n   420        0.0048             nan     0.1000   -0.0000\n   440        0.0042             nan     0.1000   -0.0000\n   460        0.0037             nan     0.1000   -0.0000\n   480        0.0032             nan     0.1000   -0.0000\n   500        0.0028             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1268             nan     0.1000    0.0769\n     2        1.0068             nan     0.1000    0.0570\n     3        0.9047             nan     0.1000    0.0476\n     4        0.8201             nan     0.1000    0.0398\n     5        0.7439             nan     0.1000    0.0367\n     6        0.6816             nan     0.1000    0.0286\n     7        0.6296             nan     0.1000    0.0219\n     8        0.5793             nan     0.1000    0.0221\n     9        0.5404             nan     0.1000    0.0165\n    10        0.5041             nan     0.1000    0.0171\n    20        0.2851             nan     0.1000    0.0053\n    40        0.1446             nan     0.1000    0.0006\n    60        0.0930             nan     0.1000    0.0005\n    80        0.0681             nan     0.1000   -0.0009\n   100        0.0512             nan     0.1000   -0.0005\n   120        0.0398             nan     0.1000   -0.0002\n   140        0.0323             nan     0.1000   -0.0002\n   160        0.0259             nan     0.1000   -0.0000\n   180        0.0222             nan     0.1000   -0.0004\n   200        0.0176             nan     0.1000   -0.0001\n   220        0.0151             nan     0.1000   -0.0002\n   240        0.0125             nan     0.1000   -0.0000\n   260        0.0107             nan     0.1000   -0.0001\n   280        0.0081             nan     0.1000   -0.0000\n   300        0.0063             nan     0.1000   -0.0000\n   320        0.0052             nan     0.1000   -0.0001\n   340        0.0042             nan     0.1000   -0.0000\n   360        0.0035             nan     0.1000   -0.0000\n   380        0.0029             nan     0.1000   -0.0000\n   400        0.0025             nan     0.1000   -0.0001\n   420        0.0020             nan     0.1000   -0.0000\n   440        0.0016             nan     0.1000   -0.0000\n   460        0.0013             nan     0.1000   -0.0000\n   480        0.0011             nan     0.1000   -0.0000\n   500        0.0009             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1352             nan     0.1000    0.0716\n     2        1.0122             nan     0.1000    0.0583\n     3        0.9094             nan     0.1000    0.0471\n     4        0.8230             nan     0.1000    0.0373\n     5        0.7545             nan     0.1000    0.0325\n     6        0.6911             nan     0.1000    0.0294\n     7        0.6316             nan     0.1000    0.0252\n     8        0.5823             nan     0.1000    0.0214\n     9        0.5387             nan     0.1000    0.0202\n    10        0.5056             nan     0.1000    0.0130\n    20        0.2824             nan     0.1000    0.0043\n    40        0.1389             nan     0.1000   -0.0007\n    60        0.0873             nan     0.1000   -0.0008\n    80        0.0552             nan     0.1000   -0.0003\n   100        0.0419             nan     0.1000   -0.0006\n   120        0.0289             nan     0.1000   -0.0005\n   140        0.0218             nan     0.1000   -0.0002\n   160        0.0169             nan     0.1000   -0.0001\n   180        0.0129             nan     0.1000   -0.0002\n   200        0.0104             nan     0.1000   -0.0002\n   220        0.0080             nan     0.1000   -0.0001\n   240        0.0059             nan     0.1000   -0.0001\n   260        0.0045             nan     0.1000   -0.0001\n   280        0.0036             nan     0.1000    0.0000\n   300        0.0029             nan     0.1000   -0.0000\n   320        0.0020             nan     0.1000   -0.0000\n   340        0.0016             nan     0.1000   -0.0000\n   360        0.0013             nan     0.1000   -0.0000\n   380        0.0010             nan     0.1000   -0.0000\n   400        0.0008             nan     0.1000    0.0000\n   420        0.0006             nan     0.1000   -0.0000\n   440        0.0005             nan     0.1000   -0.0000\n   460        0.0004             nan     0.1000   -0.0000\n   480        0.0003             nan     0.1000    0.0000\n   500        0.0003             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1333             nan     0.1000    0.0751\n     2        1.0092             nan     0.1000    0.0595\n     3        0.9123             nan     0.1000    0.0440\n     4        0.8287             nan     0.1000    0.0417\n     5        0.7558             nan     0.1000    0.0347\n     6        0.6900             nan     0.1000    0.0303\n     7        0.6355             nan     0.1000    0.0240\n     8        0.5864             nan     0.1000    0.0234\n     9        0.5414             nan     0.1000    0.0224\n    10        0.5033             nan     0.1000    0.0157\n    20        0.2783             nan     0.1000    0.0038\n    40        0.1241             nan     0.1000    0.0006\n    60        0.0721             nan     0.1000   -0.0009\n    80        0.0503             nan     0.1000   -0.0013\n   100        0.0384             nan     0.1000   -0.0006\n   120        0.0283             nan     0.1000   -0.0006\n   140        0.0206             nan     0.1000   -0.0002\n   160        0.0140             nan     0.1000   -0.0002\n   180        0.0105             nan     0.1000   -0.0002\n   200        0.0077             nan     0.1000   -0.0001\n   220        0.0060             nan     0.1000   -0.0002\n   240        0.0044             nan     0.1000   -0.0001\n   260        0.0032             nan     0.1000   -0.0000\n   280        0.0026             nan     0.1000   -0.0000\n   300        0.0018             nan     0.1000   -0.0000\n   320        0.0015             nan     0.1000   -0.0000\n   340        0.0011             nan     0.1000   -0.0000\n   360        0.0008             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0004             nan     0.1000   -0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1362             nan     0.1000    0.0723\n     2        1.0098             nan     0.1000    0.0587\n     3        0.9041             nan     0.1000    0.0466\n     4        0.8182             nan     0.1000    0.0382\n     5        0.7424             nan     0.1000    0.0329\n     6        0.6810             nan     0.1000    0.0280\n     7        0.6232             nan     0.1000    0.0262\n     8        0.5743             nan     0.1000    0.0235\n     9        0.5331             nan     0.1000    0.0177\n    10        0.4973             nan     0.1000    0.0146\n    20        0.2697             nan     0.1000    0.0046\n    40        0.1188             nan     0.1000   -0.0001\n    60        0.0684             nan     0.1000   -0.0007\n    80        0.0465             nan     0.1000   -0.0004\n   100        0.0302             nan     0.1000   -0.0011\n   120        0.0219             nan     0.1000   -0.0004\n   140        0.0152             nan     0.1000   -0.0000\n   160        0.0094             nan     0.1000   -0.0000\n   180        0.0073             nan     0.1000   -0.0001\n   200        0.0050             nan     0.1000   -0.0001\n   220        0.0039             nan     0.1000   -0.0001\n   240        0.0030             nan     0.1000   -0.0001\n   260        0.0023             nan     0.1000   -0.0000\n   280        0.0019             nan     0.1000   -0.0001\n   300        0.0013             nan     0.1000   -0.0000\n   320        0.0010             nan     0.1000    0.0000\n   340        0.0007             nan     0.1000   -0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000    0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1354             nan     0.1000    0.0743\n     2        1.0105             nan     0.1000    0.0568\n     3        0.9111             nan     0.1000    0.0497\n     4        0.8276             nan     0.1000    0.0390\n     5        0.7478             nan     0.1000    0.0355\n     6        0.6883             nan     0.1000    0.0266\n     7        0.6312             nan     0.1000    0.0261\n     8        0.5831             nan     0.1000    0.0207\n     9        0.5384             nan     0.1000    0.0191\n    10        0.4979             nan     0.1000    0.0182\n    20        0.2739             nan     0.1000    0.0052\n    40        0.1147             nan     0.1000   -0.0002\n    60        0.0631             nan     0.1000   -0.0010\n    80        0.0405             nan     0.1000   -0.0006\n   100        0.0281             nan     0.1000   -0.0005\n   120        0.0199             nan     0.1000   -0.0002\n   140        0.0140             nan     0.1000   -0.0001\n   160        0.0092             nan     0.1000   -0.0002\n   180        0.0076             nan     0.1000   -0.0001\n   200        0.0055             nan     0.1000   -0.0001\n   220        0.0037             nan     0.1000   -0.0001\n   240        0.0031             nan     0.1000   -0.0001\n   260        0.0022             nan     0.1000   -0.0001\n   280        0.0015             nan     0.1000   -0.0000\n   300        0.0011             nan     0.1000   -0.0000\n   320        0.0008             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1390             nan     0.1000    0.0741\n     2        1.0180             nan     0.1000    0.0554\n     3        0.9125             nan     0.1000    0.0502\n     4        0.8252             nan     0.1000    0.0412\n     5        0.7554             nan     0.1000    0.0328\n     6        0.6921             nan     0.1000    0.0283\n     7        0.6320             nan     0.1000    0.0265\n     8        0.5874             nan     0.1000    0.0192\n     9        0.5427             nan     0.1000    0.0184\n    10        0.4980             nan     0.1000    0.0204\n    20        0.2686             nan     0.1000    0.0047\n    40        0.1162             nan     0.1000    0.0004\n    60        0.0619             nan     0.1000   -0.0006\n    80        0.0402             nan     0.1000   -0.0001\n   100        0.0251             nan     0.1000   -0.0005\n   120        0.0171             nan     0.1000   -0.0002\n   140        0.0119             nan     0.1000   -0.0001\n   160        0.0089             nan     0.1000   -0.0002\n   180        0.0062             nan     0.1000   -0.0001\n   200        0.0047             nan     0.1000   -0.0001\n   220        0.0039             nan     0.1000   -0.0001\n   240        0.0030             nan     0.1000   -0.0000\n   260        0.0022             nan     0.1000   -0.0000\n   280        0.0016             nan     0.1000   -0.0000\n   300        0.0011             nan     0.1000   -0.0000\n   320        0.0008             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1418             nan     0.1000    0.0772\n     2        1.0167             nan     0.1000    0.0577\n     3        0.9129             nan     0.1000    0.0548\n     4        0.8249             nan     0.1000    0.0420\n     5        0.7461             nan     0.1000    0.0346\n     6        0.6801             nan     0.1000    0.0313\n     7        0.6274             nan     0.1000    0.0240\n     8        0.5797             nan     0.1000    0.0199\n     9        0.5373             nan     0.1000    0.0196\n    10        0.4990             nan     0.1000    0.0156\n    20        0.2792             nan     0.1000    0.0049\n    40        0.1227             nan     0.1000   -0.0002\n    60        0.0641             nan     0.1000   -0.0010\n    80        0.0406             nan     0.1000   -0.0008\n   100        0.0251             nan     0.1000   -0.0006\n   120        0.0175             nan     0.1000   -0.0001\n   140        0.0115             nan     0.1000   -0.0001\n   160        0.0081             nan     0.1000   -0.0001\n   180        0.0068             nan     0.1000   -0.0002\n   200        0.0052             nan     0.1000   -0.0002\n   220        0.0039             nan     0.1000   -0.0000\n   240        0.0031             nan     0.1000   -0.0000\n   260        0.0021             nan     0.1000   -0.0000\n   280        0.0015             nan     0.1000   -0.0000\n   300        0.0012             nan     0.1000   -0.0000\n   320        0.0009             nan     0.1000   -0.0000\n   340        0.0009             nan     0.1000   -0.0000\n   360        0.0009             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000   -0.0000\n   420        0.0003             nan     0.1000    0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1613             nan     0.1000    0.0631\n     2        1.0553             nan     0.1000    0.0510\n     3        0.9610             nan     0.1000    0.0491\n     4        0.8846             nan     0.1000    0.0379\n     5        0.8118             nan     0.1000    0.0345\n     6        0.7602             nan     0.1000    0.0245\n     7        0.7073             nan     0.1000    0.0259\n     8        0.6631             nan     0.1000    0.0225\n     9        0.6241             nan     0.1000    0.0169\n    10        0.5903             nan     0.1000    0.0164\n    20        0.3859             nan     0.1000    0.0043\n    40        0.2497             nan     0.1000    0.0005\n    60        0.2098             nan     0.1000   -0.0008\n    80        0.1830             nan     0.1000   -0.0007\n   100        0.1686             nan     0.1000    0.0005\n   120        0.1521             nan     0.1000   -0.0010\n   140        0.1449             nan     0.1000   -0.0005\n   160        0.1401             nan     0.1000   -0.0011\n   180        0.1351             nan     0.1000   -0.0015\n   200        0.1268             nan     0.1000   -0.0006\n   220        0.1199             nan     0.1000   -0.0009\n   240        0.1158             nan     0.1000   -0.0002\n   260        0.1141             nan     0.1000   -0.0010\n   280        0.1134             nan     0.1000   -0.0014\n   300        0.1097             nan     0.1000   -0.0008\n   320        0.1084             nan     0.1000   -0.0013\n   340        0.1054             nan     0.1000   -0.0008\n   360        0.1041             nan     0.1000   -0.0016\n   380        0.1037             nan     0.1000   -0.0007\n   400        0.0989             nan     0.1000   -0.0002\n   420        0.0985             nan     0.1000   -0.0004\n   440        0.0962             nan     0.1000   -0.0007\n   460        0.0965             nan     0.1000   -0.0014\n   480        0.0955             nan     0.1000   -0.0010\n   500        0.0944             nan     0.1000   -0.0005\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1341             nan     0.1000    0.0735\n     2        1.0147             nan     0.1000    0.0594\n     3        0.9153             nan     0.1000    0.0488\n     4        0.8325             nan     0.1000    0.0409\n     5        0.7605             nan     0.1000    0.0346\n     6        0.6995             nan     0.1000    0.0300\n     7        0.6503             nan     0.1000    0.0233\n     8        0.6041             nan     0.1000    0.0213\n     9        0.5624             nan     0.1000    0.0186\n    10        0.5236             nan     0.1000    0.0182\n    20        0.3140             nan     0.1000    0.0027\n    40        0.1830             nan     0.1000   -0.0013\n    60        0.1434             nan     0.1000    0.0002\n    80        0.1211             nan     0.1000   -0.0006\n   100        0.1018             nan     0.1000   -0.0007\n   120        0.0883             nan     0.1000   -0.0012\n   140        0.0757             nan     0.1000   -0.0011\n   160        0.0666             nan     0.1000   -0.0011\n   180        0.0591             nan     0.1000   -0.0001\n   200        0.0528             nan     0.1000   -0.0004\n   220        0.0466             nan     0.1000   -0.0004\n   240        0.0430             nan     0.1000   -0.0002\n   260        0.0387             nan     0.1000    0.0000\n   280        0.0352             nan     0.1000   -0.0005\n   300        0.0315             nan     0.1000   -0.0003\n   320        0.0282             nan     0.1000   -0.0004\n   340        0.0253             nan     0.1000   -0.0001\n   360        0.0235             nan     0.1000   -0.0002\n   380        0.0212             nan     0.1000   -0.0001\n   400        0.0190             nan     0.1000   -0.0000\n   420        0.0171             nan     0.1000   -0.0001\n   440        0.0162             nan     0.1000   -0.0001\n   460        0.0152             nan     0.1000   -0.0002\n   480        0.0143             nan     0.1000   -0.0002\n   500        0.0133             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1365             nan     0.1000    0.0771\n     2        1.0120             nan     0.1000    0.0640\n     3        0.9064             nan     0.1000    0.0484\n     4        0.8233             nan     0.1000    0.0408\n     5        0.7522             nan     0.1000    0.0344\n     6        0.6931             nan     0.1000    0.0284\n     7        0.6385             nan     0.1000    0.0260\n     8        0.5885             nan     0.1000    0.0225\n     9        0.5415             nan     0.1000    0.0205\n    10        0.5010             nan     0.1000    0.0176\n    20        0.2841             nan     0.1000    0.0055\n    40        0.1578             nan     0.1000   -0.0015\n    60        0.1146             nan     0.1000   -0.0010\n    80        0.0890             nan     0.1000    0.0001\n   100        0.0688             nan     0.1000   -0.0005\n   120        0.0555             nan     0.1000   -0.0006\n   140        0.0441             nan     0.1000    0.0000\n   160        0.0351             nan     0.1000   -0.0003\n   180        0.0279             nan     0.1000   -0.0002\n   200        0.0236             nan     0.1000   -0.0002\n   220        0.0208             nan     0.1000   -0.0002\n   240        0.0175             nan     0.1000   -0.0001\n   260        0.0150             nan     0.1000   -0.0002\n   280        0.0130             nan     0.1000   -0.0002\n   300        0.0107             nan     0.1000   -0.0001\n   320        0.0093             nan     0.1000   -0.0002\n   340        0.0081             nan     0.1000   -0.0000\n   360        0.0071             nan     0.1000   -0.0000\n   380        0.0061             nan     0.1000   -0.0001\n   400        0.0051             nan     0.1000   -0.0001\n   420        0.0044             nan     0.1000   -0.0000\n   440        0.0037             nan     0.1000   -0.0001\n   460        0.0032             nan     0.1000   -0.0001\n   480        0.0028             nan     0.1000   -0.0000\n   500        0.0023             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1443             nan     0.1000    0.0686\n     2        1.0194             nan     0.1000    0.0593\n     3        0.9210             nan     0.1000    0.0477\n     4        0.8396             nan     0.1000    0.0388\n     5        0.7628             nan     0.1000    0.0363\n     6        0.6994             nan     0.1000    0.0309\n     7        0.6436             nan     0.1000    0.0246\n     8        0.5874             nan     0.1000    0.0243\n     9        0.5467             nan     0.1000    0.0193\n    10        0.5093             nan     0.1000    0.0171\n    20        0.2871             nan     0.1000    0.0045\n    40        0.1422             nan     0.1000    0.0013\n    60        0.0895             nan     0.1000   -0.0008\n    80        0.0663             nan     0.1000   -0.0008\n   100        0.0490             nan     0.1000   -0.0002\n   120        0.0392             nan     0.1000   -0.0004\n   140        0.0319             nan     0.1000   -0.0004\n   160        0.0252             nan     0.1000   -0.0005\n   180        0.0190             nan     0.1000   -0.0003\n   200        0.0148             nan     0.1000   -0.0002\n   220        0.0117             nan     0.1000   -0.0001\n   240        0.0097             nan     0.1000   -0.0002\n   260        0.0074             nan     0.1000   -0.0001\n   280        0.0060             nan     0.1000   -0.0001\n   300        0.0048             nan     0.1000   -0.0000\n   320        0.0041             nan     0.1000   -0.0000\n   340        0.0032             nan     0.1000   -0.0000\n   360        0.0026             nan     0.1000   -0.0000\n   380        0.0022             nan     0.1000   -0.0000\n   400        0.0019             nan     0.1000   -0.0000\n   420        0.0015             nan     0.1000   -0.0000\n   440        0.0013             nan     0.1000   -0.0000\n   460        0.0011             nan     0.1000   -0.0000\n   480        0.0009             nan     0.1000   -0.0000\n   500        0.0007             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1342             nan     0.1000    0.0735\n     2        1.0106             nan     0.1000    0.0598\n     3        0.9114             nan     0.1000    0.0471\n     4        0.8277             nan     0.1000    0.0376\n     5        0.7512             nan     0.1000    0.0386\n     6        0.6900             nan     0.1000    0.0307\n     7        0.6335             nan     0.1000    0.0289\n     8        0.5867             nan     0.1000    0.0204\n     9        0.5398             nan     0.1000    0.0214\n    10        0.5033             nan     0.1000    0.0162\n    20        0.2769             nan     0.1000    0.0036\n    40        0.1218             nan     0.1000   -0.0012\n    60        0.0752             nan     0.1000   -0.0007\n    80        0.0535             nan     0.1000   -0.0010\n   100        0.0382             nan     0.1000   -0.0008\n   120        0.0290             nan     0.1000   -0.0004\n   140        0.0206             nan     0.1000   -0.0002\n   160        0.0168             nan     0.1000   -0.0003\n   180        0.0120             nan     0.1000   -0.0001\n   200        0.0086             nan     0.1000   -0.0000\n   220        0.0067             nan     0.1000   -0.0001\n   240        0.0050             nan     0.1000   -0.0000\n   260        0.0040             nan     0.1000   -0.0000\n   280        0.0031             nan     0.1000   -0.0001\n   300        0.0023             nan     0.1000   -0.0000\n   320        0.0017             nan     0.1000   -0.0000\n   340        0.0013             nan     0.1000   -0.0000\n   360        0.0010             nan     0.1000   -0.0000\n   380        0.0007             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000   -0.0000\n   420        0.0005             nan     0.1000   -0.0000\n   440        0.0004             nan     0.1000   -0.0000\n   460        0.0003             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0002             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1350             nan     0.1000    0.0694\n     2        1.0050             nan     0.1000    0.0577\n     3        0.8994             nan     0.1000    0.0495\n     4        0.8119             nan     0.1000    0.0426\n     5        0.7381             nan     0.1000    0.0333\n     6        0.6721             nan     0.1000    0.0318\n     7        0.6145             nan     0.1000    0.0228\n     8        0.5624             nan     0.1000    0.0232\n     9        0.5172             nan     0.1000    0.0190\n    10        0.4836             nan     0.1000    0.0127\n    20        0.2649             nan     0.1000    0.0023\n    40        0.1197             nan     0.1000   -0.0006\n    60        0.0728             nan     0.1000   -0.0016\n    80        0.0462             nan     0.1000   -0.0005\n   100        0.0326             nan     0.1000   -0.0001\n   120        0.0211             nan     0.1000   -0.0001\n   140        0.0155             nan     0.1000   -0.0001\n   160        0.0109             nan     0.1000   -0.0001\n   180        0.0082             nan     0.1000   -0.0001\n   200        0.0060             nan     0.1000   -0.0001\n   220        0.0044             nan     0.1000   -0.0000\n   240        0.0033             nan     0.1000   -0.0000\n   260        0.0024             nan     0.1000   -0.0000\n   280        0.0018             nan     0.1000   -0.0001\n   300        0.0014             nan     0.1000   -0.0000\n   320        0.0010             nan     0.1000   -0.0000\n   340        0.0008             nan     0.1000   -0.0000\n   360        0.0006             nan     0.1000   -0.0000\n   380        0.0005             nan     0.1000   -0.0000\n   400        0.0004             nan     0.1000   -0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0003             nan     0.1000   -0.0000\n   480        0.0003             nan     0.1000   -0.0000\n   500        0.0002             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1292             nan     0.1000    0.0731\n     2        1.0025             nan     0.1000    0.0615\n     3        0.9037             nan     0.1000    0.0462\n     4        0.8177             nan     0.1000    0.0404\n     5        0.7445             nan     0.1000    0.0367\n     6        0.6832             nan     0.1000    0.0286\n     7        0.6326             nan     0.1000    0.0208\n     8        0.5858             nan     0.1000    0.0205\n     9        0.5412             nan     0.1000    0.0186\n    10        0.5031             nan     0.1000    0.0146\n    20        0.2703             nan     0.1000    0.0038\n    40        0.1143             nan     0.1000   -0.0011\n    60        0.0623             nan     0.1000   -0.0003\n    80        0.0390             nan     0.1000   -0.0003\n   100        0.0260             nan     0.1000   -0.0004\n   120        0.0172             nan     0.1000   -0.0003\n   140        0.0120             nan     0.1000   -0.0000\n   160        0.0086             nan     0.1000   -0.0001\n   180        0.0055             nan     0.1000   -0.0000\n   200        0.0043             nan     0.1000   -0.0001\n   220        0.0031             nan     0.1000   -0.0000\n   240        0.0025             nan     0.1000   -0.0001\n   260        0.0015             nan     0.1000   -0.0000\n   280        0.0012             nan     0.1000   -0.0000\n   300        0.0009             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0006             nan     0.1000   -0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000    0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1288             nan     0.1000    0.0711\n     2        1.0064             nan     0.1000    0.0595\n     3        0.9036             nan     0.1000    0.0466\n     4        0.8216             nan     0.1000    0.0395\n     5        0.7497             nan     0.1000    0.0338\n     6        0.6853             nan     0.1000    0.0298\n     7        0.6338             nan     0.1000    0.0237\n     8        0.5863             nan     0.1000    0.0214\n     9        0.5397             nan     0.1000    0.0209\n    10        0.5025             nan     0.1000    0.0160\n    20        0.2693             nan     0.1000    0.0062\n    40        0.1143             nan     0.1000    0.0008\n    60        0.0619             nan     0.1000   -0.0014\n    80        0.0368             nan     0.1000   -0.0005\n   100        0.0252             nan     0.1000   -0.0001\n   120        0.0171             nan     0.1000   -0.0003\n   140        0.0129             nan     0.1000   -0.0004\n   160        0.0089             nan     0.1000    0.0000\n   180        0.0064             nan     0.1000   -0.0001\n   200        0.0049             nan     0.1000   -0.0001\n   220        0.0039             nan     0.1000    0.0000\n   240        0.0027             nan     0.1000   -0.0000\n   260        0.0021             nan     0.1000   -0.0000\n   280        0.0017             nan     0.1000   -0.0001\n   300        0.0012             nan     0.1000   -0.0000\n   320        0.0008             nan     0.1000    0.0000\n   340        0.0007             nan     0.1000   -0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000    0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1356             nan     0.1000    0.0751\n     2        1.0164             nan     0.1000    0.0600\n     3        0.9132             nan     0.1000    0.0486\n     4        0.8236             nan     0.1000    0.0396\n     5        0.7520             nan     0.1000    0.0330\n     6        0.6900             nan     0.1000    0.0283\n     7        0.6317             nan     0.1000    0.0274\n     8        0.5851             nan     0.1000    0.0220\n     9        0.5400             nan     0.1000    0.0186\n    10        0.4985             nan     0.1000    0.0174\n    20        0.2714             nan     0.1000    0.0051\n    40        0.1090             nan     0.1000   -0.0012\n    60        0.0593             nan     0.1000   -0.0007\n    80        0.0396             nan     0.1000   -0.0001\n   100        0.0256             nan     0.1000    0.0001\n   120        0.0192             nan     0.1000   -0.0003\n   140        0.0143             nan     0.1000   -0.0002\n   160        0.0103             nan     0.1000   -0.0002\n   180        0.0074             nan     0.1000   -0.0003\n   200        0.0045             nan     0.1000   -0.0001\n   220        0.0029             nan     0.1000   -0.0001\n   240        0.0022             nan     0.1000   -0.0000\n   260        0.0014             nan     0.1000   -0.0000\n   280        0.0010             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0006             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1309             nan     0.1000    0.0724\n     2        1.0101             nan     0.1000    0.0554\n     3        0.9109             nan     0.1000    0.0475\n     4        0.8252             nan     0.1000    0.0398\n     5        0.7524             nan     0.1000    0.0328\n     6        0.6917             nan     0.1000    0.0266\n     7        0.6385             nan     0.1000    0.0237\n     8        0.5857             nan     0.1000    0.0225\n     9        0.5416             nan     0.1000    0.0199\n    10        0.5062             nan     0.1000    0.0146\n    20        0.2705             nan     0.1000    0.0051\n    40        0.1151             nan     0.1000    0.0000\n    60        0.0600             nan     0.1000   -0.0006\n    80        0.0352             nan     0.1000   -0.0003\n   100        0.0216             nan     0.1000   -0.0003\n   120        0.0140             nan     0.1000   -0.0001\n   140        0.0099             nan     0.1000   -0.0002\n   160        0.0067             nan     0.1000   -0.0000\n   180        0.0058             nan     0.1000   -0.0002\n   200        0.0035             nan     0.1000   -0.0000\n   220        0.0025             nan     0.1000   -0.0000\n   240        0.0021             nan     0.1000   -0.0001\n   260        0.0015             nan     0.1000    0.0000\n   280        0.0013             nan     0.1000   -0.0000\n   300        0.0011             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000    0.0000\n   380        0.0003             nan     0.1000    0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000    0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1529             nan     0.1000    0.0627\n     2        1.0422             nan     0.1000    0.0535\n     3        0.9454             nan     0.1000    0.0485\n     4        0.8647             nan     0.1000    0.0404\n     5        0.7992             nan     0.1000    0.0333\n     6        0.7419             nan     0.1000    0.0277\n     7        0.6939             nan     0.1000    0.0234\n     8        0.6501             nan     0.1000    0.0194\n     9        0.6073             nan     0.1000    0.0196\n    10        0.5718             nan     0.1000    0.0159\n    20        0.3737             nan     0.1000    0.0059\n    40        0.2386             nan     0.1000   -0.0002\n    60        0.1871             nan     0.1000    0.0011\n    80        0.1652             nan     0.1000   -0.0006\n   100        0.1487             nan     0.1000   -0.0011\n   120        0.1391             nan     0.1000   -0.0023\n   140        0.1302             nan     0.1000   -0.0004\n   160        0.1236             nan     0.1000    0.0001\n   180        0.1141             nan     0.1000   -0.0001\n   200        0.1080             nan     0.1000   -0.0010\n   220        0.1053             nan     0.1000   -0.0009\n   240        0.0989             nan     0.1000   -0.0007\n   260        0.0958             nan     0.1000   -0.0008\n   280        0.0936             nan     0.1000   -0.0005\n   300        0.0901             nan     0.1000   -0.0005\n   320        0.0882             nan     0.1000   -0.0003\n   340        0.0873             nan     0.1000   -0.0010\n   360        0.0851             nan     0.1000   -0.0006\n   380        0.0834             nan     0.1000   -0.0007\n   400        0.0812             nan     0.1000   -0.0009\n   420        0.0776             nan     0.1000   -0.0010\n   440        0.0764             nan     0.1000   -0.0011\n   460        0.0758             nan     0.1000   -0.0003\n   480        0.0747             nan     0.1000   -0.0005\n   500        0.0737             nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1456             nan     0.1000    0.0697\n     2        1.0206             nan     0.1000    0.0677\n     3        0.9204             nan     0.1000    0.0506\n     4        0.8354             nan     0.1000    0.0393\n     5        0.7583             nan     0.1000    0.0349\n     6        0.6986             nan     0.1000    0.0282\n     7        0.6396             nan     0.1000    0.0269\n     8        0.5938             nan     0.1000    0.0225\n     9        0.5520             nan     0.1000    0.0188\n    10        0.5137             nan     0.1000    0.0158\n    20        0.2921             nan     0.1000    0.0064\n    40        0.1787             nan     0.1000   -0.0003\n    60        0.1342             nan     0.1000   -0.0001\n    80        0.1113             nan     0.1000   -0.0004\n   100        0.0922             nan     0.1000   -0.0005\n   120        0.0789             nan     0.1000   -0.0002\n   140        0.0658             nan     0.1000   -0.0010\n   160        0.0585             nan     0.1000   -0.0008\n   180        0.0517             nan     0.1000   -0.0008\n   200        0.0460             nan     0.1000   -0.0005\n   220        0.0401             nan     0.1000   -0.0002\n   240        0.0358             nan     0.1000   -0.0001\n   260        0.0340             nan     0.1000   -0.0007\n   280        0.0308             nan     0.1000   -0.0001\n   300        0.0277             nan     0.1000   -0.0003\n   320        0.0253             nan     0.1000   -0.0003\n   340        0.0230             nan     0.1000   -0.0001\n   360        0.0211             nan     0.1000   -0.0001\n   380        0.0193             nan     0.1000   -0.0003\n   400        0.0178             nan     0.1000   -0.0002\n   420        0.0159             nan     0.1000   -0.0003\n   440        0.0147             nan     0.1000   -0.0000\n   460        0.0133             nan     0.1000   -0.0001\n   480        0.0123             nan     0.1000   -0.0001\n   500        0.0109             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1408             nan     0.1000    0.0641\n     2        1.0134             nan     0.1000    0.0611\n     3        0.9112             nan     0.1000    0.0464\n     4        0.8236             nan     0.1000    0.0396\n     5        0.7548             nan     0.1000    0.0329\n     6        0.6971             nan     0.1000    0.0272\n     7        0.6442             nan     0.1000    0.0242\n     8        0.5944             nan     0.1000    0.0237\n     9        0.5523             nan     0.1000    0.0188\n    10        0.5140             nan     0.1000    0.0177\n    20        0.2908             nan     0.1000    0.0052\n    40        0.1461             nan     0.1000    0.0002\n    60        0.1002             nan     0.1000   -0.0002\n    80        0.0739             nan     0.1000   -0.0006\n   100        0.0591             nan     0.1000   -0.0002\n   120        0.0462             nan     0.1000   -0.0001\n   140        0.0369             nan     0.1000   -0.0004\n   160        0.0310             nan     0.1000   -0.0003\n   180        0.0269             nan     0.1000   -0.0005\n   200        0.0222             nan     0.1000   -0.0001\n   220        0.0182             nan     0.1000    0.0001\n   240        0.0154             nan     0.1000   -0.0001\n   260        0.0131             nan     0.1000   -0.0001\n   280        0.0113             nan     0.1000   -0.0000\n   300        0.0099             nan     0.1000   -0.0001\n   320        0.0084             nan     0.1000   -0.0001\n   340        0.0072             nan     0.1000   -0.0000\n   360        0.0061             nan     0.1000   -0.0000\n   380        0.0055             nan     0.1000   -0.0000\n   400        0.0047             nan     0.1000   -0.0000\n   420        0.0038             nan     0.1000   -0.0000\n   440        0.0033             nan     0.1000   -0.0000\n   460        0.0028             nan     0.1000   -0.0000\n   480        0.0024             nan     0.1000    0.0000\n   500        0.0021             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1331             nan     0.1000    0.0773\n     2        1.0115             nan     0.1000    0.0621\n     3        0.9083             nan     0.1000    0.0462\n     4        0.8296             nan     0.1000    0.0383\n     5        0.7528             nan     0.1000    0.0336\n     6        0.6912             nan     0.1000    0.0256\n     7        0.6359             nan     0.1000    0.0249\n     8        0.5833             nan     0.1000    0.0243\n     9        0.5405             nan     0.1000    0.0164\n    10        0.5022             nan     0.1000    0.0183\n    20        0.2835             nan     0.1000    0.0053\n    40        0.1367             nan     0.1000    0.0016\n    60        0.0891             nan     0.1000    0.0002\n    80        0.0588             nan     0.1000   -0.0009\n   100        0.0448             nan     0.1000   -0.0006\n   120        0.0343             nan     0.1000   -0.0002\n   140        0.0282             nan     0.1000   -0.0004\n   160        0.0208             nan     0.1000   -0.0002\n   180        0.0172             nan     0.1000   -0.0002\n   200        0.0130             nan     0.1000   -0.0001\n   220        0.0101             nan     0.1000   -0.0001\n   240        0.0078             nan     0.1000   -0.0001\n   260        0.0065             nan     0.1000   -0.0001\n   280        0.0051             nan     0.1000   -0.0000\n   300        0.0042             nan     0.1000   -0.0000\n   320        0.0033             nan     0.1000   -0.0000\n   340        0.0026             nan     0.1000    0.0000\n   360        0.0021             nan     0.1000   -0.0000\n   380        0.0016             nan     0.1000   -0.0000\n   400        0.0014             nan     0.1000   -0.0000\n   420        0.0011             nan     0.1000   -0.0000\n   440        0.0009             nan     0.1000   -0.0000\n   460        0.0007             nan     0.1000   -0.0000\n   480        0.0006             nan     0.1000   -0.0000\n   500        0.0005             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1344             nan     0.1000    0.0733\n     2        1.0068             nan     0.1000    0.0618\n     3        0.9022             nan     0.1000    0.0507\n     4        0.8127             nan     0.1000    0.0401\n     5        0.7384             nan     0.1000    0.0358\n     6        0.6715             nan     0.1000    0.0304\n     7        0.6126             nan     0.1000    0.0273\n     8        0.5668             nan     0.1000    0.0195\n     9        0.5265             nan     0.1000    0.0174\n    10        0.4867             nan     0.1000    0.0187\n    20        0.2667             nan     0.1000    0.0062\n    40        0.1212             nan     0.1000    0.0011\n    60        0.0713             nan     0.1000   -0.0002\n    80        0.0442             nan     0.1000    0.0000\n   100        0.0327             nan     0.1000   -0.0005\n   120        0.0237             nan     0.1000   -0.0004\n   140        0.0175             nan     0.1000   -0.0001\n   160        0.0124             nan     0.1000    0.0000\n   180        0.0089             nan     0.1000   -0.0001\n   200        0.0066             nan     0.1000   -0.0001\n   220        0.0047             nan     0.1000   -0.0001\n   240        0.0037             nan     0.1000   -0.0001\n   260        0.0028             nan     0.1000   -0.0001\n   280        0.0022             nan     0.1000   -0.0000\n   300        0.0017             nan     0.1000   -0.0000\n   320        0.0013             nan     0.1000   -0.0000\n   340        0.0010             nan     0.1000   -0.0000\n   360        0.0008             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0004             nan     0.1000   -0.0000\n   420        0.0003             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1342             nan     0.1000    0.0749\n     2        1.0105             nan     0.1000    0.0593\n     3        0.9092             nan     0.1000    0.0485\n     4        0.8211             nan     0.1000    0.0415\n     5        0.7409             nan     0.1000    0.0383\n     6        0.6751             nan     0.1000    0.0316\n     7        0.6194             nan     0.1000    0.0272\n     8        0.5717             nan     0.1000    0.0219\n     9        0.5267             nan     0.1000    0.0203\n    10        0.4913             nan     0.1000    0.0170\n    20        0.2586             nan     0.1000    0.0041\n    40        0.1213             nan     0.1000   -0.0010\n    60        0.0718             nan     0.1000   -0.0004\n    80        0.0437             nan     0.1000   -0.0002\n   100        0.0291             nan     0.1000   -0.0004\n   120        0.0209             nan     0.1000   -0.0006\n   140        0.0146             nan     0.1000   -0.0002\n   160        0.0101             nan     0.1000   -0.0003\n   180        0.0074             nan     0.1000   -0.0001\n   200        0.0054             nan     0.1000   -0.0001\n   220        0.0042             nan     0.1000   -0.0001\n   240        0.0032             nan     0.1000   -0.0000\n   260        0.0025             nan     0.1000   -0.0000\n   280        0.0018             nan     0.1000   -0.0000\n   300        0.0013             nan     0.1000   -0.0000\n   320        0.0010             nan     0.1000   -0.0000\n   340        0.0009             nan     0.1000   -0.0000\n   360        0.0007             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000   -0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1331             nan     0.1000    0.0715\n     2        1.0134             nan     0.1000    0.0589\n     3        0.9158             nan     0.1000    0.0442\n     4        0.8229             nan     0.1000    0.0400\n     5        0.7494             nan     0.1000    0.0337\n     6        0.6853             nan     0.1000    0.0291\n     7        0.6320             nan     0.1000    0.0247\n     8        0.5837             nan     0.1000    0.0216\n     9        0.5405             nan     0.1000    0.0201\n    10        0.4996             nan     0.1000    0.0179\n    20        0.2679             nan     0.1000    0.0055\n    40        0.1203             nan     0.1000    0.0003\n    60        0.0684             nan     0.1000   -0.0005\n    80        0.0435             nan     0.1000   -0.0002\n   100        0.0299             nan     0.1000   -0.0007\n   120        0.0185             nan     0.1000   -0.0003\n   140        0.0138             nan     0.1000   -0.0005\n   160        0.0090             nan     0.1000   -0.0001\n   180        0.0068             nan     0.1000   -0.0002\n   200        0.0048             nan     0.1000   -0.0001\n   220        0.0038             nan     0.1000    0.0000\n   240        0.0029             nan     0.1000    0.0000\n   260        0.0021             nan     0.1000   -0.0000\n   280        0.0015             nan     0.1000   -0.0000\n   300        0.0012             nan     0.1000   -0.0000\n   320        0.0011             nan     0.1000   -0.0000\n   340        0.0008             nan     0.1000   -0.0000\n   360        0.0006             nan     0.1000    0.0000\n   380        0.0005             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000   -0.0000\n   420        0.0005             nan     0.1000   -0.0000\n   440        0.0004             nan     0.1000   -0.0000\n   460        0.0003             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1238             nan     0.1000    0.0828\n     2        0.9996             nan     0.1000    0.0619\n     3        0.8962             nan     0.1000    0.0476\n     4        0.8064             nan     0.1000    0.0428\n     5        0.7354             nan     0.1000    0.0337\n     6        0.6674             nan     0.1000    0.0312\n     7        0.6102             nan     0.1000    0.0249\n     8        0.5662             nan     0.1000    0.0185\n     9        0.5241             nan     0.1000    0.0197\n    10        0.4847             nan     0.1000    0.0179\n    20        0.2566             nan     0.1000    0.0053\n    40        0.0967             nan     0.1000    0.0008\n    60        0.0487             nan     0.1000   -0.0002\n    80        0.0289             nan     0.1000   -0.0000\n   100        0.0194             nan     0.1000   -0.0001\n   120        0.0137             nan     0.1000   -0.0003\n   140        0.0093             nan     0.1000   -0.0001\n   160        0.0072             nan     0.1000   -0.0002\n   180        0.0047             nan     0.1000   -0.0001\n   200        0.0033             nan     0.1000   -0.0000\n   220        0.0024             nan     0.1000   -0.0000\n   240        0.0020             nan     0.1000   -0.0001\n   260        0.0013             nan     0.1000   -0.0000\n   280        0.0009             nan     0.1000   -0.0000\n   300        0.0007             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000    0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000    0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1292             nan     0.1000    0.0808\n     2        1.0022             nan     0.1000    0.0586\n     3        0.9062             nan     0.1000    0.0442\n     4        0.8209             nan     0.1000    0.0405\n     5        0.7437             nan     0.1000    0.0388\n     6        0.6800             nan     0.1000    0.0301\n     7        0.6251             nan     0.1000    0.0254\n     8        0.5803             nan     0.1000    0.0193\n     9        0.5320             nan     0.1000    0.0234\n    10        0.4928             nan     0.1000    0.0161\n    20        0.2588             nan     0.1000    0.0049\n    40        0.0983             nan     0.1000    0.0008\n    60        0.0515             nan     0.1000   -0.0008\n    80        0.0317             nan     0.1000   -0.0004\n   100        0.0227             nan     0.1000   -0.0002\n   120        0.0144             nan     0.1000   -0.0001\n   140        0.0129             nan     0.1000   -0.0005\n   160        0.0086             nan     0.1000    0.0000\n   180        0.0061             nan     0.1000    0.0000\n   200        0.0044             nan     0.1000   -0.0001\n   220        0.0034             nan     0.1000   -0.0001\n   240        0.0024             nan     0.1000   -0.0000\n   260        0.0016             nan     0.1000   -0.0000\n   280        0.0013             nan     0.1000   -0.0001\n   300        0.0011             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1386             nan     0.1000    0.0720\n     2        1.0105             nan     0.1000    0.0640\n     3        0.9081             nan     0.1000    0.0487\n     4        0.8222             nan     0.1000    0.0398\n     5        0.7427             nan     0.1000    0.0354\n     6        0.6802             nan     0.1000    0.0302\n     7        0.6262             nan     0.1000    0.0249\n     8        0.5728             nan     0.1000    0.0250\n     9        0.5276             nan     0.1000    0.0188\n    10        0.4918             nan     0.1000    0.0158\n    20        0.2586             nan     0.1000    0.0053\n    40        0.1033             nan     0.1000   -0.0004\n    60        0.0529             nan     0.1000   -0.0008\n    80        0.0337             nan     0.1000   -0.0003\n   100        0.0212             nan     0.1000   -0.0004\n   120        0.0146             nan     0.1000   -0.0004\n   140        0.0097             nan     0.1000   -0.0003\n   160        0.0075             nan     0.1000   -0.0002\n   180        0.0056             nan     0.1000   -0.0000\n   200        0.0039             nan     0.1000   -0.0001\n   220        0.0026             nan     0.1000   -0.0001\n   240        0.0021             nan     0.1000   -0.0001\n   260        0.0017             nan     0.1000   -0.0001\n   280        0.0013             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000    0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1522             nan     0.1000    0.0678\n     2        1.0417             nan     0.1000    0.0503\n     3        0.9464             nan     0.1000    0.0455\n     4        0.8631             nan     0.1000    0.0418\n     5        0.7950             nan     0.1000    0.0337\n     6        0.7381             nan     0.1000    0.0270\n     7        0.6856             nan     0.1000    0.0255\n     8        0.6401             nan     0.1000    0.0194\n     9        0.6049             nan     0.1000    0.0181\n    10        0.5647             nan     0.1000    0.0180\n    20        0.3614             nan     0.1000    0.0060\n    40        0.2264             nan     0.1000    0.0012\n    60        0.1873             nan     0.1000   -0.0009\n    80        0.1633             nan     0.1000   -0.0006\n   100        0.1459             nan     0.1000   -0.0008\n   120        0.1334             nan     0.1000    0.0001\n   140        0.1258             nan     0.1000   -0.0010\n   160        0.1195             nan     0.1000   -0.0008\n   180        0.1112             nan     0.1000    0.0000\n   200        0.1075             nan     0.1000   -0.0007\n   220        0.1029             nan     0.1000   -0.0007\n   240        0.0995             nan     0.1000   -0.0005\n   260        0.0976             nan     0.1000   -0.0003\n   280        0.0956             nan     0.1000   -0.0001\n   300        0.0930             nan     0.1000   -0.0007\n   320        0.0899             nan     0.1000   -0.0009\n   340        0.0885             nan     0.1000   -0.0006\n   360        0.0869             nan     0.1000   -0.0007\n   380        0.0855             nan     0.1000   -0.0007\n   400        0.0840             nan     0.1000   -0.0007\n   420        0.0822             nan     0.1000   -0.0007\n   440        0.0819             nan     0.1000   -0.0003\n   460        0.0801             nan     0.1000   -0.0005\n   480        0.0786             nan     0.1000   -0.0004\n   500        0.0783             nan     0.1000   -0.0011\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1353             nan     0.1000    0.0730\n     2        1.0175             nan     0.1000    0.0620\n     3        0.9192             nan     0.1000    0.0481\n     4        0.8336             nan     0.1000    0.0414\n     5        0.7661             nan     0.1000    0.0334\n     6        0.7064             nan     0.1000    0.0265\n     7        0.6485             nan     0.1000    0.0275\n     8        0.5948             nan     0.1000    0.0240\n     9        0.5526             nan     0.1000    0.0195\n    10        0.5175             nan     0.1000    0.0160\n    20        0.2955             nan     0.1000    0.0056\n    40        0.1693             nan     0.1000   -0.0016\n    60        0.1321             nan     0.1000   -0.0006\n    80        0.1114             nan     0.1000   -0.0005\n   100        0.0958             nan     0.1000   -0.0009\n   120        0.0828             nan     0.1000   -0.0003\n   140        0.0743             nan     0.1000   -0.0006\n   160        0.0657             nan     0.1000   -0.0007\n   180        0.0613             nan     0.1000   -0.0009\n   200        0.0539             nan     0.1000   -0.0006\n   220        0.0472             nan     0.1000   -0.0001\n   240        0.0436             nan     0.1000   -0.0002\n   260        0.0402             nan     0.1000   -0.0004\n   280        0.0352             nan     0.1000   -0.0002\n   300        0.0326             nan     0.1000   -0.0002\n   320        0.0297             nan     0.1000   -0.0003\n   340        0.0272             nan     0.1000   -0.0001\n   360        0.0248             nan     0.1000   -0.0001\n   380        0.0222             nan     0.1000   -0.0002\n   400        0.0199             nan     0.1000   -0.0002\n   420        0.0183             nan     0.1000   -0.0001\n   440        0.0169             nan     0.1000   -0.0001\n   460        0.0152             nan     0.1000   -0.0001\n   480        0.0135             nan     0.1000   -0.0001\n   500        0.0122             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1341             nan     0.1000    0.0725\n     2        1.0093             nan     0.1000    0.0586\n     3        0.9047             nan     0.1000    0.0492\n     4        0.8189             nan     0.1000    0.0427\n     5        0.7440             nan     0.1000    0.0363\n     6        0.6800             nan     0.1000    0.0322\n     7        0.6238             nan     0.1000    0.0265\n     8        0.5767             nan     0.1000    0.0221\n     9        0.5332             nan     0.1000    0.0190\n    10        0.4980             nan     0.1000    0.0158\n    20        0.2783             nan     0.1000    0.0054\n    40        0.1453             nan     0.1000   -0.0000\n    60        0.1008             nan     0.1000    0.0000\n    80        0.0788             nan     0.1000   -0.0006\n   100        0.0632             nan     0.1000   -0.0007\n   120        0.0501             nan     0.1000   -0.0009\n   140        0.0399             nan     0.1000   -0.0003\n   160        0.0311             nan     0.1000   -0.0004\n   180        0.0270             nan     0.1000   -0.0001\n   200        0.0227             nan     0.1000   -0.0002\n   220        0.0180             nan     0.1000   -0.0002\n   240        0.0147             nan     0.1000   -0.0001\n   260        0.0127             nan     0.1000   -0.0002\n   280        0.0109             nan     0.1000   -0.0001\n   300        0.0091             nan     0.1000   -0.0001\n   320        0.0079             nan     0.1000   -0.0001\n   340        0.0069             nan     0.1000   -0.0000\n   360        0.0060             nan     0.1000   -0.0000\n   380        0.0050             nan     0.1000   -0.0001\n   400        0.0044             nan     0.1000   -0.0000\n   420        0.0037             nan     0.1000   -0.0000\n   440        0.0032             nan     0.1000   -0.0000\n   460        0.0029             nan     0.1000   -0.0000\n   480        0.0024             nan     0.1000   -0.0000\n   500        0.0021             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1324             nan     0.1000    0.0750\n     2        1.0074             nan     0.1000    0.0583\n     3        0.9005             nan     0.1000    0.0509\n     4        0.8152             nan     0.1000    0.0404\n     5        0.7381             nan     0.1000    0.0361\n     6        0.6761             nan     0.1000    0.0296\n     7        0.6224             nan     0.1000    0.0244\n     8        0.5741             nan     0.1000    0.0213\n     9        0.5342             nan     0.1000    0.0161\n    10        0.4949             nan     0.1000    0.0177\n    20        0.2735             nan     0.1000    0.0043\n    40        0.1340             nan     0.1000    0.0008\n    60        0.0867             nan     0.1000   -0.0010\n    80        0.0622             nan     0.1000   -0.0007\n   100        0.0479             nan     0.1000   -0.0005\n   120        0.0372             nan     0.1000   -0.0001\n   140        0.0283             nan     0.1000   -0.0003\n   160        0.0229             nan     0.1000   -0.0005\n   180        0.0184             nan     0.1000   -0.0003\n   200        0.0144             nan     0.1000   -0.0002\n   220        0.0113             nan     0.1000   -0.0000\n   240        0.0095             nan     0.1000   -0.0001\n   260        0.0077             nan     0.1000   -0.0001\n   280        0.0063             nan     0.1000   -0.0001\n   300        0.0052             nan     0.1000   -0.0000\n   320        0.0041             nan     0.1000   -0.0001\n   340        0.0032             nan     0.1000   -0.0001\n   360        0.0027             nan     0.1000   -0.0000\n   380        0.0022             nan     0.1000   -0.0001\n   400        0.0018             nan     0.1000   -0.0000\n   420        0.0014             nan     0.1000   -0.0000\n   440        0.0011             nan     0.1000   -0.0000\n   460        0.0009             nan     0.1000   -0.0000\n   480        0.0008             nan     0.1000   -0.0000\n   500        0.0006             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1259             nan     0.1000    0.0782\n     2        1.0025             nan     0.1000    0.0620\n     3        0.9038             nan     0.1000    0.0460\n     4        0.8143             nan     0.1000    0.0411\n     5        0.7391             nan     0.1000    0.0344\n     6        0.6779             nan     0.1000    0.0279\n     7        0.6214             nan     0.1000    0.0275\n     8        0.5703             nan     0.1000    0.0241\n     9        0.5216             nan     0.1000    0.0215\n    10        0.4854             nan     0.1000    0.0177\n    20        0.2617             nan     0.1000    0.0048\n    40        0.1137             nan     0.1000   -0.0002\n    60        0.0720             nan     0.1000   -0.0001\n    80        0.0473             nan     0.1000   -0.0006\n   100        0.0343             nan     0.1000   -0.0001\n   120        0.0248             nan     0.1000    0.0000\n   140        0.0187             nan     0.1000   -0.0004\n   160        0.0129             nan     0.1000   -0.0001\n   180        0.0104             nan     0.1000   -0.0003\n   200        0.0075             nan     0.1000   -0.0000\n   220        0.0060             nan     0.1000   -0.0001\n   240        0.0042             nan     0.1000   -0.0000\n   260        0.0031             nan     0.1000   -0.0000\n   280        0.0023             nan     0.1000   -0.0000\n   300        0.0018             nan     0.1000   -0.0000\n   320        0.0014             nan     0.1000   -0.0000\n   340        0.0011             nan     0.1000   -0.0000\n   360        0.0008             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000   -0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1276             nan     0.1000    0.0761\n     2        1.0000             nan     0.1000    0.0627\n     3        0.8967             nan     0.1000    0.0480\n     4        0.8121             nan     0.1000    0.0376\n     5        0.7377             nan     0.1000    0.0352\n     6        0.6741             nan     0.1000    0.0317\n     7        0.6170             nan     0.1000    0.0264\n     8        0.5700             nan     0.1000    0.0211\n     9        0.5272             nan     0.1000    0.0200\n    10        0.4892             nan     0.1000    0.0171\n    20        0.2586             nan     0.1000    0.0059\n    40        0.1141             nan     0.1000   -0.0008\n    60        0.0654             nan     0.1000   -0.0001\n    80        0.0413             nan     0.1000   -0.0005\n   100        0.0271             nan     0.1000   -0.0003\n   120        0.0205             nan     0.1000   -0.0001\n   140        0.0151             nan     0.1000   -0.0000\n   160        0.0102             nan     0.1000   -0.0001\n   180        0.0073             nan     0.1000    0.0000\n   200        0.0055             nan     0.1000   -0.0001\n   220        0.0041             nan     0.1000   -0.0000\n   240        0.0030             nan     0.1000   -0.0000\n   260        0.0023             nan     0.1000   -0.0000\n   280        0.0018             nan     0.1000   -0.0000\n   300        0.0015             nan     0.1000    0.0000\n   320        0.0013             nan     0.1000   -0.0000\n   340        0.0009             nan     0.1000   -0.0000\n   360        0.0008             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000    0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1260             nan     0.1000    0.0743\n     2        1.0022             nan     0.1000    0.0590\n     3        0.9013             nan     0.1000    0.0481\n     4        0.8139             nan     0.1000    0.0436\n     5        0.7402             nan     0.1000    0.0350\n     6        0.6766             nan     0.1000    0.0292\n     7        0.6185             nan     0.1000    0.0263\n     8        0.5691             nan     0.1000    0.0197\n     9        0.5295             nan     0.1000    0.0153\n    10        0.4912             nan     0.1000    0.0148\n    20        0.2662             nan     0.1000    0.0053\n    40        0.1215             nan     0.1000    0.0013\n    60        0.0663             nan     0.1000   -0.0015\n    80        0.0424             nan     0.1000   -0.0001\n   100        0.0271             nan     0.1000   -0.0005\n   120        0.0185             nan     0.1000   -0.0004\n   140        0.0123             nan     0.1000   -0.0001\n   160        0.0091             nan     0.1000   -0.0001\n   180        0.0069             nan     0.1000   -0.0001\n   200        0.0053             nan     0.1000   -0.0001\n   220        0.0043             nan     0.1000   -0.0001\n   240        0.0033             nan     0.1000   -0.0000\n   260        0.0026             nan     0.1000   -0.0000\n   280        0.0023             nan     0.1000   -0.0000\n   300        0.0016             nan     0.1000   -0.0000\n   320        0.0011             nan     0.1000   -0.0000\n   340        0.0010             nan     0.1000    0.0000\n   360        0.0008             nan     0.1000   -0.0000\n   380        0.0005             nan     0.1000   -0.0000\n   400        0.0004             nan     0.1000   -0.0000\n   420        0.0005             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1304             nan     0.1000    0.0793\n     2        1.0040             nan     0.1000    0.0661\n     3        0.8991             nan     0.1000    0.0527\n     4        0.8117             nan     0.1000    0.0407\n     5        0.7399             nan     0.1000    0.0341\n     6        0.6753             nan     0.1000    0.0288\n     7        0.6158             nan     0.1000    0.0276\n     8        0.5681             nan     0.1000    0.0199\n     9        0.5283             nan     0.1000    0.0182\n    10        0.4889             nan     0.1000    0.0193\n    20        0.2578             nan     0.1000    0.0055\n    40        0.1026             nan     0.1000   -0.0002\n    60        0.0584             nan     0.1000   -0.0008\n    80        0.0358             nan     0.1000   -0.0006\n   100        0.0233             nan     0.1000   -0.0002\n   120        0.0155             nan     0.1000   -0.0002\n   140        0.0104             nan     0.1000   -0.0003\n   160        0.0077             nan     0.1000   -0.0002\n   180        0.0057             nan     0.1000   -0.0001\n   200        0.0043             nan     0.1000   -0.0002\n   220        0.0033             nan     0.1000   -0.0000\n   240        0.0021             nan     0.1000    0.0000\n   260        0.0015             nan     0.1000   -0.0000\n   280        0.0011             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0001             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1289             nan     0.1000    0.0775\n     2        1.0045             nan     0.1000    0.0552\n     3        0.9004             nan     0.1000    0.0528\n     4        0.8131             nan     0.1000    0.0418\n     5        0.7422             nan     0.1000    0.0344\n     6        0.6785             nan     0.1000    0.0289\n     7        0.6192             nan     0.1000    0.0294\n     8        0.5730             nan     0.1000    0.0206\n     9        0.5294             nan     0.1000    0.0202\n    10        0.4906             nan     0.1000    0.0161\n    20        0.2536             nan     0.1000    0.0053\n    40        0.1083             nan     0.1000   -0.0015\n    60        0.0616             nan     0.1000   -0.0004\n    80        0.0422             nan     0.1000   -0.0006\n   100        0.0263             nan     0.1000    0.0002\n   120        0.0192             nan     0.1000    0.0002\n   140        0.0136             nan     0.1000   -0.0002\n   160        0.0106             nan     0.1000   -0.0004\n   180        0.0076             nan     0.1000   -0.0002\n   200        0.0051             nan     0.1000   -0.0001\n   220        0.0038             nan     0.1000    0.0000\n   240        0.0028             nan     0.1000   -0.0001\n   260        0.0023             nan     0.1000   -0.0001\n   280        0.0015             nan     0.1000   -0.0000\n   300        0.0014             nan     0.1000   -0.0000\n   320        0.0012             nan     0.1000   -0.0000\n   340        0.0011             nan     0.1000   -0.0000\n   360        0.0008             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000   -0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0005             nan     0.1000    0.0000\n   460        0.0003             nan     0.1000    0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1356             nan     0.1000    0.0688\n     2        1.0110             nan     0.1000    0.0564\n     3        0.9145             nan     0.1000    0.0428\n     4        0.8241             nan     0.1000    0.0403\n     5        0.7496             nan     0.1000    0.0358\n     6        0.6851             nan     0.1000    0.0302\n     7        0.6268             nan     0.1000    0.0273\n     8        0.5772             nan     0.1000    0.0225\n     9        0.5356             nan     0.1000    0.0173\n    10        0.4986             nan     0.1000    0.0163\n    20        0.2640             nan     0.1000    0.0062\n    40        0.1149             nan     0.1000    0.0002\n    60        0.0587             nan     0.1000   -0.0001\n    80        0.0399             nan     0.1000   -0.0008\n   100        0.0267             nan     0.1000   -0.0005\n   120        0.0190             nan     0.1000   -0.0001\n   140        0.0137             nan     0.1000   -0.0003\n   160        0.0095             nan     0.1000   -0.0000\n   180        0.0078             nan     0.1000    0.0000\n   200        0.0056             nan     0.1000   -0.0001\n   220        0.0041             nan     0.1000   -0.0000\n   240        0.0032             nan     0.1000   -0.0001\n   260        0.0025             nan     0.1000   -0.0001\n   280        0.0021             nan     0.1000   -0.0001\n   300        0.0014             nan     0.1000   -0.0000\n   320        0.0009             nan     0.1000   -0.0000\n   340        0.0008             nan     0.1000   -0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1439             nan     0.1000    0.0628\n     2        1.0362             nan     0.1000    0.0525\n     3        0.9414             nan     0.1000    0.0446\n     4        0.8649             nan     0.1000    0.0371\n     5        0.8013             nan     0.1000    0.0324\n     6        0.7436             nan     0.1000    0.0276\n     7        0.6945             nan     0.1000    0.0243\n     8        0.6499             nan     0.1000    0.0201\n     9        0.6103             nan     0.1000    0.0187\n    10        0.5753             nan     0.1000    0.0170\n    20        0.3754             nan     0.1000    0.0059\n    40        0.2446             nan     0.1000    0.0015\n    60        0.2035             nan     0.1000    0.0006\n    80        0.1753             nan     0.1000   -0.0002\n   100        0.1601             nan     0.1000   -0.0008\n   120        0.1478             nan     0.1000   -0.0002\n   140        0.1379             nan     0.1000   -0.0015\n   160        0.1278             nan     0.1000   -0.0008\n   180        0.1220             nan     0.1000   -0.0006\n   200        0.1184             nan     0.1000   -0.0010\n   220        0.1142             nan     0.1000   -0.0007\n   240        0.1094             nan     0.1000   -0.0005\n   260        0.1068             nan     0.1000   -0.0003\n   280        0.1045             nan     0.1000   -0.0010\n   300        0.1012             nan     0.1000   -0.0006\n   320        0.1000             nan     0.1000   -0.0012\n   340        0.0976             nan     0.1000   -0.0003\n   360        0.0962             nan     0.1000   -0.0007\n   380        0.0936             nan     0.1000   -0.0012\n   400        0.0925             nan     0.1000   -0.0006\n   420        0.0908             nan     0.1000   -0.0009\n   440        0.0887             nan     0.1000   -0.0011\n   460        0.0868             nan     0.1000   -0.0004\n   480        0.0872             nan     0.1000   -0.0002\n   500        0.0860             nan     0.1000   -0.0008\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1368             nan     0.1000    0.0781\n     2        1.0196             nan     0.1000    0.0561\n     3        0.9214             nan     0.1000    0.0442\n     4        0.8391             nan     0.1000    0.0420\n     5        0.7680             nan     0.1000    0.0325\n     6        0.7099             nan     0.1000    0.0291\n     7        0.6545             nan     0.1000    0.0233\n     8        0.6047             nan     0.1000    0.0238\n     9        0.5672             nan     0.1000    0.0174\n    10        0.5289             nan     0.1000    0.0168\n    20        0.3168             nan     0.1000    0.0059\n    40        0.1817             nan     0.1000    0.0001\n    60        0.1395             nan     0.1000   -0.0013\n    80        0.1077             nan     0.1000    0.0003\n   100        0.0910             nan     0.1000   -0.0006\n   120        0.0808             nan     0.1000   -0.0002\n   140        0.0728             nan     0.1000   -0.0004\n   160        0.0634             nan     0.1000   -0.0009\n   180        0.0563             nan     0.1000   -0.0006\n   200        0.0489             nan     0.1000   -0.0004\n   220        0.0447             nan     0.1000   -0.0004\n   240        0.0407             nan     0.1000   -0.0003\n   260        0.0367             nan     0.1000   -0.0003\n   280        0.0339             nan     0.1000   -0.0007\n   300        0.0312             nan     0.1000   -0.0000\n   320        0.0281             nan     0.1000   -0.0001\n   340        0.0261             nan     0.1000   -0.0002\n   360        0.0236             nan     0.1000   -0.0002\n   380        0.0215             nan     0.1000   -0.0003\n   400        0.0201             nan     0.1000   -0.0002\n   420        0.0184             nan     0.1000   -0.0004\n   440        0.0170             nan     0.1000   -0.0001\n   460        0.0161             nan     0.1000   -0.0001\n   480        0.0148             nan     0.1000   -0.0002\n   500        0.0136             nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1336             nan     0.1000    0.0742\n     2        1.0140             nan     0.1000    0.0601\n     3        0.9102             nan     0.1000    0.0511\n     4        0.8274             nan     0.1000    0.0371\n     5        0.7544             nan     0.1000    0.0369\n     6        0.6897             nan     0.1000    0.0314\n     7        0.6370             nan     0.1000    0.0251\n     8        0.5884             nan     0.1000    0.0234\n     9        0.5453             nan     0.1000    0.0215\n    10        0.5089             nan     0.1000    0.0153\n    20        0.2903             nan     0.1000    0.0059\n    40        0.1557             nan     0.1000    0.0001\n    60        0.1081             nan     0.1000   -0.0001\n    80        0.0854             nan     0.1000   -0.0004\n   100        0.0676             nan     0.1000   -0.0008\n   120        0.0549             nan     0.1000   -0.0004\n   140        0.0464             nan     0.1000   -0.0003\n   160        0.0369             nan     0.1000   -0.0003\n   180        0.0317             nan     0.1000   -0.0003\n   200        0.0268             nan     0.1000   -0.0004\n   220        0.0226             nan     0.1000   -0.0005\n   240        0.0187             nan     0.1000   -0.0001\n   260        0.0158             nan     0.1000   -0.0000\n   280        0.0141             nan     0.1000   -0.0002\n   300        0.0122             nan     0.1000   -0.0002\n   320        0.0104             nan     0.1000   -0.0003\n   340        0.0089             nan     0.1000   -0.0001\n   360        0.0077             nan     0.1000   -0.0000\n   380        0.0068             nan     0.1000   -0.0000\n   400        0.0060             nan     0.1000   -0.0001\n   420        0.0051             nan     0.1000   -0.0001\n   440        0.0042             nan     0.1000   -0.0000\n   460        0.0037             nan     0.1000   -0.0000\n   480        0.0031             nan     0.1000   -0.0000\n   500        0.0026             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1381             nan     0.1000    0.0713\n     2        1.0158             nan     0.1000    0.0565\n     3        0.9123             nan     0.1000    0.0473\n     4        0.8283             nan     0.1000    0.0401\n     5        0.7559             nan     0.1000    0.0323\n     6        0.6946             nan     0.1000    0.0296\n     7        0.6408             nan     0.1000    0.0248\n     8        0.5922             nan     0.1000    0.0211\n     9        0.5514             nan     0.1000    0.0165\n    10        0.5135             nan     0.1000    0.0166\n    20        0.2820             nan     0.1000    0.0049\n    40        0.1424             nan     0.1000    0.0009\n    60        0.0906             nan     0.1000    0.0001\n    80        0.0651             nan     0.1000   -0.0002\n   100        0.0470             nan     0.1000   -0.0007\n   120        0.0371             nan     0.1000   -0.0005\n   140        0.0285             nan     0.1000   -0.0002\n   160        0.0226             nan     0.1000   -0.0002\n   180        0.0183             nan     0.1000   -0.0001\n   200        0.0147             nan     0.1000   -0.0001\n   220        0.0125             nan     0.1000   -0.0002\n   240        0.0100             nan     0.1000    0.0000\n   260        0.0081             nan     0.1000   -0.0001\n   280        0.0067             nan     0.1000   -0.0000\n   300        0.0055             nan     0.1000   -0.0001\n   320        0.0045             nan     0.1000   -0.0001\n   340        0.0036             nan     0.1000   -0.0000\n   360        0.0029             nan     0.1000   -0.0000\n   380        0.0023             nan     0.1000   -0.0000\n   400        0.0019             nan     0.1000   -0.0000\n   420        0.0014             nan     0.1000   -0.0000\n   440        0.0012             nan     0.1000   -0.0000\n   460        0.0009             nan     0.1000   -0.0000\n   480        0.0008             nan     0.1000   -0.0000\n   500        0.0006             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1299             nan     0.1000    0.0747\n     2        1.0100             nan     0.1000    0.0587\n     3        0.9072             nan     0.1000    0.0512\n     4        0.8194             nan     0.1000    0.0396\n     5        0.7458             nan     0.1000    0.0340\n     6        0.6783             nan     0.1000    0.0320\n     7        0.6198             nan     0.1000    0.0267\n     8        0.5760             nan     0.1000    0.0177\n     9        0.5306             nan     0.1000    0.0228\n    10        0.4917             nan     0.1000    0.0173\n    20        0.2727             nan     0.1000    0.0040\n    40        0.1270             nan     0.1000   -0.0003\n    60        0.0778             nan     0.1000   -0.0005\n    80        0.0526             nan     0.1000   -0.0016\n   100        0.0381             nan     0.1000   -0.0005\n   120        0.0295             nan     0.1000   -0.0004\n   140        0.0222             nan     0.1000   -0.0004\n   160        0.0172             nan     0.1000   -0.0003\n   180        0.0136             nan     0.1000   -0.0002\n   200        0.0105             nan     0.1000   -0.0000\n   220        0.0080             nan     0.1000   -0.0001\n   240        0.0066             nan     0.1000   -0.0000\n   260        0.0050             nan     0.1000   -0.0001\n   280        0.0041             nan     0.1000   -0.0001\n   300        0.0030             nan     0.1000   -0.0000\n   320        0.0023             nan     0.1000   -0.0000\n   340        0.0018             nan     0.1000   -0.0000\n   360        0.0014             nan     0.1000   -0.0000\n   380        0.0011             nan     0.1000   -0.0000\n   400        0.0008             nan     0.1000   -0.0000\n   420        0.0007             nan     0.1000   -0.0000\n   440        0.0005             nan     0.1000   -0.0000\n   460        0.0004             nan     0.1000   -0.0000\n   480        0.0003             nan     0.1000   -0.0000\n   500        0.0002             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1304             nan     0.1000    0.0740\n     2        1.0050             nan     0.1000    0.0534\n     3        0.9058             nan     0.1000    0.0466\n     4        0.8176             nan     0.1000    0.0410\n     5        0.7434             nan     0.1000    0.0367\n     6        0.6801             nan     0.1000    0.0268\n     7        0.6200             nan     0.1000    0.0255\n     8        0.5742             nan     0.1000    0.0209\n     9        0.5338             nan     0.1000    0.0168\n    10        0.4955             nan     0.1000    0.0161\n    20        0.2681             nan     0.1000    0.0050\n    40        0.1229             nan     0.1000    0.0005\n    60        0.0730             nan     0.1000    0.0002\n    80        0.0468             nan     0.1000   -0.0003\n   100        0.0317             nan     0.1000   -0.0004\n   120        0.0224             nan     0.1000   -0.0005\n   140        0.0163             nan     0.1000   -0.0001\n   160        0.0116             nan     0.1000   -0.0001\n   180        0.0091             nan     0.1000   -0.0002\n   200        0.0070             nan     0.1000   -0.0002\n   220        0.0059             nan     0.1000   -0.0000\n   240        0.0044             nan     0.1000   -0.0001\n   260        0.0030             nan     0.1000    0.0000\n   280        0.0022             nan     0.1000   -0.0000\n   300        0.0017             nan     0.1000   -0.0000\n   320        0.0012             nan     0.1000   -0.0000\n   340        0.0010             nan     0.1000   -0.0000\n   360        0.0008             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000   -0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1298             nan     0.1000    0.0723\n     2        1.0059             nan     0.1000    0.0560\n     3        0.9021             nan     0.1000    0.0467\n     4        0.8181             nan     0.1000    0.0404\n     5        0.7409             nan     0.1000    0.0361\n     6        0.6805             nan     0.1000    0.0302\n     7        0.6287             nan     0.1000    0.0248\n     8        0.5798             nan     0.1000    0.0220\n     9        0.5379             nan     0.1000    0.0196\n    10        0.5015             nan     0.1000    0.0146\n    20        0.2709             nan     0.1000    0.0050\n    40        0.1164             nan     0.1000   -0.0013\n    60        0.0680             nan     0.1000   -0.0005\n    80        0.0423             nan     0.1000   -0.0007\n   100        0.0308             nan     0.1000   -0.0007\n   120        0.0203             nan     0.1000   -0.0002\n   140        0.0148             nan     0.1000   -0.0001\n   160        0.0116             nan     0.1000   -0.0003\n   180        0.0075             nan     0.1000   -0.0000\n   200        0.0062             nan     0.1000   -0.0002\n   220        0.0052             nan     0.1000   -0.0000\n   240        0.0037             nan     0.1000   -0.0000\n   260        0.0026             nan     0.1000   -0.0001\n   280        0.0021             nan     0.1000   -0.0000\n   300        0.0015             nan     0.1000   -0.0000\n   320        0.0011             nan     0.1000   -0.0000\n   340        0.0008             nan     0.1000   -0.0000\n   360        0.0006             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1336             nan     0.1000    0.0754\n     2        1.0054             nan     0.1000    0.0583\n     3        0.9056             nan     0.1000    0.0482\n     4        0.8212             nan     0.1000    0.0364\n     5        0.7457             nan     0.1000    0.0358\n     6        0.6833             nan     0.1000    0.0270\n     7        0.6284             nan     0.1000    0.0252\n     8        0.5812             nan     0.1000    0.0228\n     9        0.5378             nan     0.1000    0.0191\n    10        0.4989             nan     0.1000    0.0163\n    20        0.2683             nan     0.1000    0.0066\n    40        0.1162             nan     0.1000   -0.0001\n    60        0.0628             nan     0.1000   -0.0009\n    80        0.0370             nan     0.1000   -0.0005\n   100        0.0257             nan     0.1000   -0.0005\n   120        0.0179             nan     0.1000   -0.0004\n   140        0.0138             nan     0.1000   -0.0004\n   160        0.0095             nan     0.1000   -0.0002\n   180        0.0077             nan     0.1000   -0.0002\n   200        0.0062             nan     0.1000    0.0001\n   220        0.0047             nan     0.1000   -0.0001\n   240        0.0042             nan     0.1000   -0.0002\n   260        0.0030             nan     0.1000   -0.0000\n   280        0.0028             nan     0.1000   -0.0001\n   300        0.0026             nan     0.1000   -0.0002\n   320        0.0017             nan     0.1000   -0.0000\n   340        0.0014             nan     0.1000   -0.0001\n   360        0.0010             nan     0.1000   -0.0000\n   380        0.0007             nan     0.1000   -0.0000\n   400        0.0008             nan     0.1000   -0.0000\n   420        0.0008             nan     0.1000   -0.0000\n   440        0.0005             nan     0.1000   -0.0000\n   460        0.0004             nan     0.1000   -0.0000\n   480        0.0005             nan     0.1000   -0.0000\n   500        0.0004             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1365             nan     0.1000    0.0680\n     2        1.0047             nan     0.1000    0.0644\n     3        0.9008             nan     0.1000    0.0501\n     4        0.8158             nan     0.1000    0.0429\n     5        0.7441             nan     0.1000    0.0355\n     6        0.6784             nan     0.1000    0.0295\n     7        0.6200             nan     0.1000    0.0282\n     8        0.5706             nan     0.1000    0.0216\n     9        0.5297             nan     0.1000    0.0183\n    10        0.4921             nan     0.1000    0.0168\n    20        0.2710             nan     0.1000    0.0040\n    40        0.1081             nan     0.1000   -0.0007\n    60        0.0632             nan     0.1000   -0.0005\n    80        0.0391             nan     0.1000   -0.0007\n   100        0.0273             nan     0.1000   -0.0002\n   120        0.0191             nan     0.1000   -0.0002\n   140        0.0123             nan     0.1000   -0.0002\n   160        0.0090             nan     0.1000   -0.0001\n   180        0.0072             nan     0.1000    0.0000\n   200        0.0049             nan     0.1000   -0.0001\n   220        0.0033             nan     0.1000   -0.0000\n   240        0.0024             nan     0.1000   -0.0001\n   260        0.0019             nan     0.1000   -0.0001\n   280        0.0014             nan     0.1000   -0.0000\n   300        0.0011             nan     0.1000   -0.0000\n   320        0.0008             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000    0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1341             nan     0.1000    0.0790\n     2        1.0139             nan     0.1000    0.0541\n     3        0.9144             nan     0.1000    0.0455\n     4        0.8284             nan     0.1000    0.0395\n     5        0.7531             nan     0.1000    0.0327\n     6        0.6886             nan     0.1000    0.0293\n     7        0.6337             nan     0.1000    0.0246\n     8        0.5828             nan     0.1000    0.0231\n     9        0.5426             nan     0.1000    0.0178\n    10        0.5051             nan     0.1000    0.0153\n    20        0.2743             nan     0.1000    0.0057\n    40        0.1147             nan     0.1000   -0.0004\n    60        0.0622             nan     0.1000   -0.0008\n    80        0.0377             nan     0.1000   -0.0005\n   100        0.0271             nan     0.1000   -0.0005\n   120        0.0172             nan     0.1000   -0.0003\n   140        0.0106             nan     0.1000    0.0000\n   160        0.0072             nan     0.1000   -0.0000\n   180        0.0053             nan     0.1000   -0.0001\n   200        0.0038             nan     0.1000   -0.0000\n   220        0.0026             nan     0.1000   -0.0000\n   240        0.0017             nan     0.1000   -0.0000\n   260        0.0015             nan     0.1000   -0.0000\n   280        0.0011             nan     0.1000   -0.0000\n   300        0.0009             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000    0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1541             nan     0.1000    0.0701\n     2        1.0366             nan     0.1000    0.0537\n     3        0.9429             nan     0.1000    0.0447\n     4        0.8686             nan     0.1000    0.0360\n     5        0.8031             nan     0.1000    0.0308\n     6        0.7428             nan     0.1000    0.0276\n     7        0.6927             nan     0.1000    0.0250\n     8        0.6555             nan     0.1000    0.0159\n     9        0.6149             nan     0.1000    0.0175\n    10        0.5782             nan     0.1000    0.0173\n    20        0.3729             nan     0.1000    0.0042\n    40        0.2388             nan     0.1000    0.0012\n    60        0.1871             nan     0.1000    0.0007\n    80        0.1567             nan     0.1000   -0.0003\n   100        0.1379             nan     0.1000    0.0000\n   120        0.1253             nan     0.1000   -0.0007\n   140        0.1141             nan     0.1000   -0.0005\n   160        0.1083             nan     0.1000   -0.0003\n   180        0.1024             nan     0.1000   -0.0009\n   200        0.0952             nan     0.1000   -0.0005\n   220        0.0904             nan     0.1000   -0.0002\n   240        0.0883             nan     0.1000   -0.0004\n   260        0.0875             nan     0.1000   -0.0006\n   280        0.0851             nan     0.1000   -0.0008\n   300        0.0802             nan     0.1000   -0.0012\n   320        0.0769             nan     0.1000   -0.0008\n   340        0.0754             nan     0.1000   -0.0015\n   360        0.0724             nan     0.1000   -0.0000\n   380        0.0701             nan     0.1000   -0.0004\n   400        0.0684             nan     0.1000   -0.0004\n   420        0.0665             nan     0.1000   -0.0004\n   440        0.0658             nan     0.1000   -0.0002\n   460        0.0639             nan     0.1000   -0.0005\n   480        0.0620             nan     0.1000   -0.0001\n   500        0.0606             nan     0.1000   -0.0007\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1377             nan     0.1000    0.0701\n     2        1.0237             nan     0.1000    0.0559\n     3        0.9250             nan     0.1000    0.0485\n     4        0.8476             nan     0.1000    0.0372\n     5        0.7783             nan     0.1000    0.0346\n     6        0.7205             nan     0.1000    0.0265\n     7        0.6672             nan     0.1000    0.0261\n     8        0.6152             nan     0.1000    0.0238\n     9        0.5692             nan     0.1000    0.0209\n    10        0.5321             nan     0.1000    0.0170\n    20        0.3164             nan     0.1000    0.0027\n    40        0.1731             nan     0.1000    0.0003\n    60        0.1319             nan     0.1000   -0.0006\n    80        0.1063             nan     0.1000   -0.0003\n   100        0.0872             nan     0.1000    0.0001\n   120        0.0742             nan     0.1000   -0.0004\n   140        0.0671             nan     0.1000   -0.0001\n   160        0.0592             nan     0.1000   -0.0004\n   180        0.0519             nan     0.1000   -0.0004\n   200        0.0450             nan     0.1000   -0.0002\n   220        0.0391             nan     0.1000   -0.0008\n   240        0.0347             nan     0.1000   -0.0002\n   260        0.0306             nan     0.1000   -0.0003\n   280        0.0268             nan     0.1000   -0.0002\n   300        0.0243             nan     0.1000   -0.0003\n   320        0.0218             nan     0.1000   -0.0002\n   340        0.0200             nan     0.1000   -0.0003\n   360        0.0184             nan     0.1000   -0.0001\n   380        0.0161             nan     0.1000   -0.0003\n   400        0.0148             nan     0.1000   -0.0002\n   420        0.0141             nan     0.1000   -0.0003\n   440        0.0129             nan     0.1000   -0.0001\n   460        0.0116             nan     0.1000   -0.0001\n   480        0.0104             nan     0.1000   -0.0001\n   500        0.0095             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1367             nan     0.1000    0.0773\n     2        1.0090             nan     0.1000    0.0615\n     3        0.9080             nan     0.1000    0.0490\n     4        0.8262             nan     0.1000    0.0391\n     5        0.7475             nan     0.1000    0.0355\n     6        0.6878             nan     0.1000    0.0277\n     7        0.6326             nan     0.1000    0.0255\n     8        0.5823             nan     0.1000    0.0229\n     9        0.5410             nan     0.1000    0.0190\n    10        0.5049             nan     0.1000    0.0148\n    20        0.2832             nan     0.1000    0.0076\n    40        0.1466             nan     0.1000    0.0001\n    60        0.0996             nan     0.1000   -0.0008\n    80        0.0749             nan     0.1000   -0.0002\n   100        0.0580             nan     0.1000   -0.0001\n   120        0.0473             nan     0.1000   -0.0006\n   140        0.0392             nan     0.1000   -0.0002\n   160        0.0323             nan     0.1000   -0.0001\n   180        0.0264             nan     0.1000   -0.0003\n   200        0.0217             nan     0.1000   -0.0002\n   220        0.0177             nan     0.1000   -0.0001\n   240        0.0147             nan     0.1000   -0.0002\n   260        0.0121             nan     0.1000   -0.0001\n   280        0.0102             nan     0.1000   -0.0001\n   300        0.0088             nan     0.1000   -0.0001\n   320        0.0079             nan     0.1000   -0.0001\n   340        0.0068             nan     0.1000   -0.0001\n   360        0.0059             nan     0.1000   -0.0001\n   380        0.0052             nan     0.1000   -0.0001\n   400        0.0044             nan     0.1000   -0.0000\n   420        0.0036             nan     0.1000   -0.0000\n   440        0.0029             nan     0.1000   -0.0000\n   460        0.0025             nan     0.1000   -0.0000\n   480        0.0021             nan     0.1000   -0.0000\n   500        0.0017             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1415             nan     0.1000    0.0712\n     2        1.0170             nan     0.1000    0.0584\n     3        0.9133             nan     0.1000    0.0477\n     4        0.8278             nan     0.1000    0.0435\n     5        0.7548             nan     0.1000    0.0353\n     6        0.6904             nan     0.1000    0.0281\n     7        0.6323             nan     0.1000    0.0262\n     8        0.5851             nan     0.1000    0.0234\n     9        0.5439             nan     0.1000    0.0186\n    10        0.5059             nan     0.1000    0.0164\n    20        0.2762             nan     0.1000    0.0045\n    40        0.1312             nan     0.1000    0.0003\n    60        0.0857             nan     0.1000   -0.0008\n    80        0.0619             nan     0.1000   -0.0006\n   100        0.0445             nan     0.1000   -0.0004\n   120        0.0321             nan     0.1000   -0.0000\n   140        0.0241             nan     0.1000   -0.0002\n   160        0.0191             nan     0.1000   -0.0002\n   180        0.0154             nan     0.1000   -0.0002\n   200        0.0126             nan     0.1000   -0.0002\n   220        0.0099             nan     0.1000   -0.0002\n   240        0.0078             nan     0.1000   -0.0001\n   260        0.0063             nan     0.1000   -0.0001\n   280        0.0052             nan     0.1000   -0.0001\n   300        0.0044             nan     0.1000   -0.0001\n   320        0.0035             nan     0.1000   -0.0000\n   340        0.0029             nan     0.1000   -0.0000\n   360        0.0023             nan     0.1000   -0.0000\n   380        0.0018             nan     0.1000   -0.0000\n   400        0.0015             nan     0.1000   -0.0000\n   420        0.0012             nan     0.1000   -0.0000\n   440        0.0009             nan     0.1000   -0.0000\n   460        0.0008             nan     0.1000   -0.0000\n   480        0.0006             nan     0.1000   -0.0000\n   500        0.0005             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1311             nan     0.1000    0.0731\n     2        1.0055             nan     0.1000    0.0606\n     3        0.8984             nan     0.1000    0.0490\n     4        0.8133             nan     0.1000    0.0394\n     5        0.7441             nan     0.1000    0.0330\n     6        0.6847             nan     0.1000    0.0275\n     7        0.6309             nan     0.1000    0.0259\n     8        0.5832             nan     0.1000    0.0223\n     9        0.5365             nan     0.1000    0.0211\n    10        0.4969             nan     0.1000    0.0171\n    20        0.2702             nan     0.1000    0.0053\n    40        0.1174             nan     0.1000   -0.0006\n    60        0.0693             nan     0.1000   -0.0001\n    80        0.0443             nan     0.1000    0.0001\n   100        0.0318             nan     0.1000   -0.0003\n   120        0.0230             nan     0.1000   -0.0002\n   140        0.0172             nan     0.1000   -0.0001\n   160        0.0123             nan     0.1000   -0.0001\n   180        0.0099             nan     0.1000   -0.0003\n   200        0.0073             nan     0.1000   -0.0001\n   220        0.0053             nan     0.1000   -0.0001\n   240        0.0040             nan     0.1000   -0.0000\n   260        0.0032             nan     0.1000   -0.0000\n   280        0.0025             nan     0.1000   -0.0000\n   300        0.0020             nan     0.1000   -0.0000\n   320        0.0014             nan     0.1000   -0.0000\n   340        0.0010             nan     0.1000   -0.0000\n   360        0.0008             nan     0.1000   -0.0000\n   380        0.0006             nan     0.1000   -0.0000\n   400        0.0005             nan     0.1000   -0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1415             nan     0.1000    0.0616\n     2        1.0165             nan     0.1000    0.0583\n     3        0.9169             nan     0.1000    0.0478\n     4        0.8295             nan     0.1000    0.0430\n     5        0.7522             nan     0.1000    0.0355\n     6        0.6870             nan     0.1000    0.0312\n     7        0.6320             nan     0.1000    0.0238\n     8        0.5833             nan     0.1000    0.0227\n     9        0.5402             nan     0.1000    0.0203\n    10        0.4976             nan     0.1000    0.0181\n    20        0.2580             nan     0.1000    0.0054\n    40        0.1060             nan     0.1000    0.0005\n    60        0.0593             nan     0.1000   -0.0008\n    80        0.0379             nan     0.1000   -0.0002\n   100        0.0258             nan     0.1000   -0.0003\n   120        0.0179             nan     0.1000   -0.0001\n   140        0.0119             nan     0.1000   -0.0001\n   160        0.0083             nan     0.1000   -0.0001\n   180        0.0056             nan     0.1000   -0.0000\n   200        0.0044             nan     0.1000   -0.0001\n   220        0.0032             nan     0.1000   -0.0001\n   240        0.0024             nan     0.1000   -0.0000\n   260        0.0017             nan     0.1000    0.0000\n   280        0.0013             nan     0.1000   -0.0000\n   300        0.0010             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0006             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000    0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1389             nan     0.1000    0.0709\n     2        1.0121             nan     0.1000    0.0599\n     3        0.9074             nan     0.1000    0.0502\n     4        0.8216             nan     0.1000    0.0402\n     5        0.7502             nan     0.1000    0.0318\n     6        0.6878             nan     0.1000    0.0294\n     7        0.6314             nan     0.1000    0.0247\n     8        0.5847             nan     0.1000    0.0201\n     9        0.5357             nan     0.1000    0.0213\n    10        0.4972             nan     0.1000    0.0175\n    20        0.2655             nan     0.1000    0.0057\n    40        0.1135             nan     0.1000    0.0002\n    60        0.0609             nan     0.1000   -0.0003\n    80        0.0368             nan     0.1000    0.0000\n   100        0.0250             nan     0.1000    0.0000\n   120        0.0165             nan     0.1000   -0.0002\n   140        0.0108             nan     0.1000   -0.0002\n   160        0.0083             nan     0.1000   -0.0001\n   180        0.0059             nan     0.1000   -0.0001\n   200        0.0042             nan     0.1000    0.0000\n   220        0.0029             nan     0.1000   -0.0001\n   240        0.0021             nan     0.1000   -0.0000\n   260        0.0017             nan     0.1000   -0.0000\n   280        0.0014             nan     0.1000   -0.0000\n   300        0.0010             nan     0.1000    0.0000\n   320        0.0008             nan     0.1000   -0.0000\n   340        0.0006             nan     0.1000    0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000    0.0000\n   480        0.0001             nan     0.1000    0.0000\n   500        0.0001             nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1272             nan     0.1000    0.0776\n     2        1.0056             nan     0.1000    0.0574\n     3        0.9048             nan     0.1000    0.0446\n     4        0.8199             nan     0.1000    0.0422\n     5        0.7485             nan     0.1000    0.0347\n     6        0.6868             nan     0.1000    0.0265\n     7        0.6313             nan     0.1000    0.0260\n     8        0.5848             nan     0.1000    0.0201\n     9        0.5417             nan     0.1000    0.0193\n    10        0.5048             nan     0.1000    0.0146\n    20        0.2634             nan     0.1000    0.0058\n    40        0.1083             nan     0.1000   -0.0002\n    60        0.0532             nan     0.1000   -0.0001\n    80        0.0332             nan     0.1000   -0.0006\n   100        0.0237             nan     0.1000   -0.0005\n   120        0.0165             nan     0.1000   -0.0000\n   140        0.0121             nan     0.1000   -0.0001\n   160        0.0085             nan     0.1000   -0.0003\n   180        0.0061             nan     0.1000   -0.0001\n   200        0.0042             nan     0.1000   -0.0001\n   220        0.0031             nan     0.1000   -0.0000\n   240        0.0019             nan     0.1000   -0.0000\n   260        0.0014             nan     0.1000   -0.0000\n   280        0.0009             nan     0.1000   -0.0000\n   300        0.0007             nan     0.1000   -0.0000\n   320        0.0005             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0001             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0000             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1307             nan     0.1000    0.0759\n     2        1.0111             nan     0.1000    0.0623\n     3        0.9089             nan     0.1000    0.0512\n     4        0.8201             nan     0.1000    0.0438\n     5        0.7432             nan     0.1000    0.0354\n     6        0.6809             nan     0.1000    0.0281\n     7        0.6275             nan     0.1000    0.0218\n     8        0.5808             nan     0.1000    0.0210\n     9        0.5356             nan     0.1000    0.0211\n    10        0.4953             nan     0.1000    0.0188\n    20        0.2613             nan     0.1000    0.0032\n    40        0.1067             nan     0.1000    0.0003\n    60        0.0561             nan     0.1000    0.0008\n    80        0.0322             nan     0.1000    0.0003\n   100        0.0226             nan     0.1000   -0.0004\n   120        0.0150             nan     0.1000   -0.0003\n   140        0.0096             nan     0.1000   -0.0002\n   160        0.0068             nan     0.1000   -0.0000\n   180        0.0046             nan     0.1000   -0.0001\n   200        0.0032             nan     0.1000   -0.0001\n   220        0.0024             nan     0.1000   -0.0001\n   240        0.0019             nan     0.1000   -0.0001\n   260        0.0013             nan     0.1000   -0.0000\n   280        0.0011             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000    0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000    0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1387             nan     0.1000    0.0768\n     2        1.0158             nan     0.1000    0.0572\n     3        0.9154             nan     0.1000    0.0463\n     4        0.8269             nan     0.1000    0.0429\n     5        0.7474             nan     0.1000    0.0373\n     6        0.6857             nan     0.1000    0.0266\n     7        0.6315             nan     0.1000    0.0233\n     8        0.5814             nan     0.1000    0.0214\n     9        0.5406             nan     0.1000    0.0184\n    10        0.5029             nan     0.1000    0.0158\n    20        0.2681             nan     0.1000    0.0035\n    40        0.1060             nan     0.1000   -0.0004\n    60        0.0518             nan     0.1000   -0.0003\n    80        0.0290             nan     0.1000   -0.0004\n   100        0.0200             nan     0.1000   -0.0003\n   120        0.0136             nan     0.1000   -0.0003\n   140        0.0106             nan     0.1000   -0.0002\n   160        0.0067             nan     0.1000   -0.0000\n   180        0.0043             nan     0.1000   -0.0001\n   200        0.0032             nan     0.1000   -0.0001\n   220        0.0021             nan     0.1000   -0.0000\n   240        0.0015             nan     0.1000   -0.0000\n   260        0.0011             nan     0.1000   -0.0000\n   280        0.0008             nan     0.1000   -0.0000\n   300        0.0006             nan     0.1000   -0.0000\n   320        0.0004             nan     0.1000   -0.0000\n   340        0.0003             nan     0.1000    0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1465             nan     0.1000    0.0655\n     2        1.0386             nan     0.1000    0.0534\n     3        0.9470             nan     0.1000    0.0428\n     4        0.8707             nan     0.1000    0.0344\n     5        0.8022             nan     0.1000    0.0328\n     6        0.7459             nan     0.1000    0.0293\n     7        0.6991             nan     0.1000    0.0204\n     8        0.6544             nan     0.1000    0.0202\n     9        0.6085             nan     0.1000    0.0193\n    10        0.5731             nan     0.1000    0.0168\n    20        0.3679             nan     0.1000    0.0059\n    40        0.2498             nan     0.1000   -0.0004\n    60        0.1936             nan     0.1000   -0.0014\n    80        0.1705             nan     0.1000    0.0002\n   100        0.1564             nan     0.1000   -0.0016\n   120        0.1442             nan     0.1000   -0.0016\n   140        0.1354             nan     0.1000   -0.0010\n   160        0.1302             nan     0.1000   -0.0017\n   180        0.1284             nan     0.1000   -0.0009\n   200        0.1220             nan     0.1000   -0.0009\n   220        0.1185             nan     0.1000   -0.0017\n   240        0.1156             nan     0.1000   -0.0012\n   260        0.1135             nan     0.1000   -0.0009\n   280        0.1122             nan     0.1000   -0.0007\n   300        0.1071             nan     0.1000   -0.0003\n   320        0.1058             nan     0.1000   -0.0011\n   340        0.1047             nan     0.1000   -0.0004\n   360        0.1024             nan     0.1000   -0.0005\n   380        0.1001             nan     0.1000   -0.0007\n   400        0.0997             nan     0.1000   -0.0008\n   420        0.0969             nan     0.1000   -0.0006\n   440        0.0958             nan     0.1000   -0.0003\n   460        0.0955             nan     0.1000   -0.0008\n   480        0.0953             nan     0.1000   -0.0005\n   500        0.0941             nan     0.1000   -0.0012\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1383             nan     0.1000    0.0719\n     2        1.0233             nan     0.1000    0.0560\n     3        0.9258             nan     0.1000    0.0472\n     4        0.8407             nan     0.1000    0.0403\n     5        0.7715             nan     0.1000    0.0339\n     6        0.7066             nan     0.1000    0.0301\n     7        0.6541             nan     0.1000    0.0259\n     8        0.6047             nan     0.1000    0.0231\n     9        0.5600             nan     0.1000    0.0209\n    10        0.5232             nan     0.1000    0.0175\n    20        0.3084             nan     0.1000    0.0056\n    40        0.1879             nan     0.1000    0.0002\n    60        0.1427             nan     0.1000    0.0001\n    80        0.1177             nan     0.1000   -0.0005\n   100        0.1042             nan     0.1000   -0.0013\n   120        0.0935             nan     0.1000   -0.0012\n   140        0.0813             nan     0.1000   -0.0013\n   160        0.0736             nan     0.1000    0.0002\n   180        0.0633             nan     0.1000   -0.0002\n   200        0.0549             nan     0.1000   -0.0007\n   220        0.0498             nan     0.1000   -0.0006\n   240        0.0460             nan     0.1000   -0.0004\n   260        0.0425             nan     0.1000   -0.0005\n   280        0.0385             nan     0.1000   -0.0002\n   300        0.0346             nan     0.1000   -0.0006\n   320        0.0304             nan     0.1000   -0.0002\n   340        0.0280             nan     0.1000   -0.0002\n   360        0.0258             nan     0.1000   -0.0002\n   380        0.0230             nan     0.1000   -0.0001\n   400        0.0221             nan     0.1000   -0.0001\n   420        0.0203             nan     0.1000   -0.0002\n   440        0.0188             nan     0.1000   -0.0001\n   460        0.0176             nan     0.1000   -0.0001\n   480        0.0166             nan     0.1000   -0.0002\n   500        0.0154             nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1330             nan     0.1000    0.0730\n     2        1.0129             nan     0.1000    0.0563\n     3        0.9141             nan     0.1000    0.0455\n     4        0.8308             nan     0.1000    0.0389\n     5        0.7536             nan     0.1000    0.0381\n     6        0.6910             nan     0.1000    0.0285\n     7        0.6324             nan     0.1000    0.0279\n     8        0.5817             nan     0.1000    0.0247\n     9        0.5357             nan     0.1000    0.0205\n    10        0.5025             nan     0.1000    0.0148\n    20        0.2862             nan     0.1000    0.0049\n    40        0.1491             nan     0.1000   -0.0001\n    60        0.1035             nan     0.1000   -0.0004\n    80        0.0809             nan     0.1000   -0.0007\n   100        0.0679             nan     0.1000   -0.0009\n   120        0.0513             nan     0.1000   -0.0004\n   140        0.0426             nan     0.1000   -0.0007\n   160        0.0352             nan     0.1000   -0.0003\n   180        0.0286             nan     0.1000   -0.0003\n   200        0.0237             nan     0.1000   -0.0002\n   220        0.0203             nan     0.1000   -0.0002\n   240        0.0173             nan     0.1000   -0.0000\n   260        0.0159             nan     0.1000    0.0000\n   280        0.0136             nan     0.1000   -0.0001\n   300        0.0114             nan     0.1000   -0.0001\n   320        0.0100             nan     0.1000   -0.0001\n   340        0.0086             nan     0.1000   -0.0001\n   360        0.0074             nan     0.1000   -0.0001\n   380        0.0063             nan     0.1000   -0.0001\n   400        0.0056             nan     0.1000   -0.0001\n   420        0.0048             nan     0.1000   -0.0001\n   440        0.0041             nan     0.1000   -0.0000\n   460        0.0036             nan     0.1000   -0.0000\n   480        0.0031             nan     0.1000   -0.0000\n   500        0.0028             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1427             nan     0.1000    0.0749\n     2        1.0182             nan     0.1000    0.0586\n     3        0.9185             nan     0.1000    0.0493\n     4        0.8359             nan     0.1000    0.0375\n     5        0.7577             nan     0.1000    0.0371\n     6        0.6942             nan     0.1000    0.0309\n     7        0.6411             nan     0.1000    0.0227\n     8        0.5923             nan     0.1000    0.0220\n     9        0.5520             nan     0.1000    0.0169\n    10        0.5097             nan     0.1000    0.0177\n    20        0.2837             nan     0.1000    0.0051\n    40        0.1424             nan     0.1000    0.0004\n    60        0.0915             nan     0.1000   -0.0005\n    80        0.0665             nan     0.1000   -0.0008\n   100        0.0495             nan     0.1000   -0.0004\n   120        0.0384             nan     0.1000   -0.0004\n   140        0.0281             nan     0.1000   -0.0001\n   160        0.0218             nan     0.1000   -0.0002\n   180        0.0170             nan     0.1000   -0.0001\n   200        0.0134             nan     0.1000   -0.0002\n   220        0.0113             nan     0.1000   -0.0002\n   240        0.0088             nan     0.1000   -0.0002\n   260        0.0071             nan     0.1000   -0.0001\n   280        0.0057             nan     0.1000   -0.0001\n   300        0.0045             nan     0.1000   -0.0000\n   320        0.0037             nan     0.1000   -0.0000\n   340        0.0029             nan     0.1000   -0.0000\n   360        0.0024             nan     0.1000   -0.0000\n   380        0.0019             nan     0.1000   -0.0000\n   400        0.0017             nan     0.1000   -0.0000\n   420        0.0013             nan     0.1000   -0.0000\n   440        0.0011             nan     0.1000   -0.0000\n   460        0.0008             nan     0.1000   -0.0000\n   480        0.0007             nan     0.1000   -0.0000\n   500        0.0006             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1334             nan     0.1000    0.0794\n     2        1.0111             nan     0.1000    0.0582\n     3        0.9049             nan     0.1000    0.0492\n     4        0.8172             nan     0.1000    0.0394\n     5        0.7420             nan     0.1000    0.0365\n     6        0.6830             nan     0.1000    0.0260\n     7        0.6211             nan     0.1000    0.0280\n     8        0.5729             nan     0.1000    0.0223\n     9        0.5292             nan     0.1000    0.0191\n    10        0.4908             nan     0.1000    0.0164\n    20        0.2612             nan     0.1000    0.0064\n    40        0.1273             nan     0.1000   -0.0008\n    60        0.0740             nan     0.1000    0.0002\n    80        0.0498             nan     0.1000   -0.0008\n   100        0.0339             nan     0.1000   -0.0002\n   120        0.0225             nan     0.1000    0.0000\n   140        0.0165             nan     0.1000   -0.0001\n   160        0.0124             nan     0.1000   -0.0002\n   180        0.0091             nan     0.1000   -0.0002\n   200        0.0070             nan     0.1000   -0.0001\n   220        0.0052             nan     0.1000   -0.0001\n   240        0.0043             nan     0.1000   -0.0001\n   260        0.0033             nan     0.1000   -0.0001\n   280        0.0026             nan     0.1000   -0.0001\n   300        0.0021             nan     0.1000   -0.0000\n   320        0.0015             nan     0.1000   -0.0000\n   340        0.0011             nan     0.1000   -0.0000\n   360        0.0009             nan     0.1000    0.0000\n   380        0.0008             nan     0.1000    0.0000\n   400        0.0006             nan     0.1000   -0.0000\n   420        0.0004             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0003             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0002             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1248             nan     0.1000    0.0775\n     2        1.0020             nan     0.1000    0.0586\n     3        0.8990             nan     0.1000    0.0477\n     4        0.8099             nan     0.1000    0.0435\n     5        0.7371             nan     0.1000    0.0343\n     6        0.6758             nan     0.1000    0.0305\n     7        0.6179             nan     0.1000    0.0268\n     8        0.5670             nan     0.1000    0.0235\n     9        0.5227             nan     0.1000    0.0184\n    10        0.4856             nan     0.1000    0.0161\n    20        0.2601             nan     0.1000    0.0042\n    40        0.1128             nan     0.1000   -0.0004\n    60        0.0642             nan     0.1000   -0.0002\n    80        0.0417             nan     0.1000   -0.0002\n   100        0.0286             nan     0.1000   -0.0002\n   120        0.0196             nan     0.1000   -0.0003\n   140        0.0142             nan     0.1000   -0.0003\n   160        0.0098             nan     0.1000   -0.0000\n   180        0.0074             nan     0.1000    0.0000\n   200        0.0055             nan     0.1000   -0.0001\n   220        0.0041             nan     0.1000   -0.0001\n   240        0.0030             nan     0.1000    0.0000\n   260        0.0022             nan     0.1000   -0.0001\n   280        0.0017             nan     0.1000    0.0000\n   300        0.0012             nan     0.1000   -0.0000\n   320        0.0011             nan     0.1000   -0.0000\n   340        0.0007             nan     0.1000    0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0003             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1349             nan     0.1000    0.0731\n     2        1.0166             nan     0.1000    0.0585\n     3        0.9210             nan     0.1000    0.0474\n     4        0.8317             nan     0.1000    0.0455\n     5        0.7565             nan     0.1000    0.0332\n     6        0.6954             nan     0.1000    0.0273\n     7        0.6350             nan     0.1000    0.0277\n     8        0.5868             nan     0.1000    0.0226\n     9        0.5399             nan     0.1000    0.0213\n    10        0.5002             nan     0.1000    0.0180\n    20        0.2673             nan     0.1000    0.0049\n    40        0.1078             nan     0.1000    0.0000\n    60        0.0588             nan     0.1000   -0.0007\n    80        0.0389             nan     0.1000   -0.0005\n   100        0.0264             nan     0.1000   -0.0001\n   120        0.0166             nan     0.1000    0.0001\n   140        0.0119             nan     0.1000   -0.0001\n   160        0.0078             nan     0.1000   -0.0000\n   180        0.0056             nan     0.1000   -0.0001\n   200        0.0043             nan     0.1000   -0.0000\n   220        0.0033             nan     0.1000   -0.0000\n   240        0.0024             nan     0.1000   -0.0001\n   260        0.0020             nan     0.1000   -0.0000\n   280        0.0016             nan     0.1000   -0.0000\n   300        0.0011             nan     0.1000   -0.0000\n   320        0.0009             nan     0.1000   -0.0000\n   340        0.0007             nan     0.1000   -0.0000\n   360        0.0006             nan     0.1000   -0.0000\n   380        0.0005             nan     0.1000   -0.0000\n   400        0.0004             nan     0.1000   -0.0000\n   420        0.0003             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0002             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1362             nan     0.1000    0.0673\n     2        1.0171             nan     0.1000    0.0571\n     3        0.9143             nan     0.1000    0.0484\n     4        0.8277             nan     0.1000    0.0392\n     5        0.7516             nan     0.1000    0.0350\n     6        0.6888             nan     0.1000    0.0268\n     7        0.6286             nan     0.1000    0.0273\n     8        0.5822             nan     0.1000    0.0206\n     9        0.5341             nan     0.1000    0.0223\n    10        0.4953             nan     0.1000    0.0157\n    20        0.2710             nan     0.1000    0.0059\n    40        0.1102             nan     0.1000    0.0001\n    60        0.0576             nan     0.1000   -0.0005\n    80        0.0407             nan     0.1000   -0.0015\n   100        0.0272             nan     0.1000   -0.0005\n   120        0.0193             nan     0.1000   -0.0003\n   140        0.0133             nan     0.1000   -0.0004\n   160        0.0092             nan     0.1000   -0.0001\n   180        0.0061             nan     0.1000   -0.0001\n   200        0.0045             nan     0.1000   -0.0001\n   220        0.0032             nan     0.1000   -0.0000\n   240        0.0023             nan     0.1000   -0.0001\n   260        0.0017             nan     0.1000   -0.0001\n   280        0.0014             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000    0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1367             nan     0.1000    0.0799\n     2        1.0113             nan     0.1000    0.0588\n     3        0.9111             nan     0.1000    0.0477\n     4        0.8227             nan     0.1000    0.0429\n     5        0.7496             nan     0.1000    0.0316\n     6        0.6814             nan     0.1000    0.0321\n     7        0.6282             nan     0.1000    0.0239\n     8        0.5802             nan     0.1000    0.0230\n     9        0.5375             nan     0.1000    0.0189\n    10        0.5015             nan     0.1000    0.0153\n    20        0.2652             nan     0.1000    0.0052\n    40        0.1092             nan     0.1000   -0.0000\n    60        0.0577             nan     0.1000   -0.0006\n    80        0.0335             nan     0.1000    0.0000\n   100        0.0228             nan     0.1000    0.0001\n   120        0.0156             nan     0.1000   -0.0001\n   140        0.0105             nan     0.1000   -0.0001\n   160        0.0066             nan     0.1000   -0.0001\n   180        0.0048             nan     0.1000   -0.0001\n   200        0.0041             nan     0.1000   -0.0001\n   220        0.0030             nan     0.1000   -0.0001\n   240        0.0023             nan     0.1000   -0.0001\n   260        0.0017             nan     0.1000   -0.0000\n   280        0.0012             nan     0.1000   -0.0000\n   300        0.0009             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1337             nan     0.1000    0.0727\n     2        1.0061             nan     0.1000    0.0577\n     3        0.9016             nan     0.1000    0.0474\n     4        0.8122             nan     0.1000    0.0436\n     5        0.7370             nan     0.1000    0.0352\n     6        0.6708             nan     0.1000    0.0299\n     7        0.6119             nan     0.1000    0.0245\n     8        0.5633             nan     0.1000    0.0215\n     9        0.5190             nan     0.1000    0.0180\n    10        0.4801             nan     0.1000    0.0183\n    20        0.2560             nan     0.1000    0.0046\n    40        0.1069             nan     0.1000   -0.0007\n    60        0.0629             nan     0.1000   -0.0002\n    80        0.0389             nan     0.1000   -0.0007\n   100        0.0265             nan     0.1000   -0.0008\n   120        0.0177             nan     0.1000   -0.0005\n   140        0.0122             nan     0.1000   -0.0001\n   160        0.0080             nan     0.1000   -0.0001\n   180        0.0051             nan     0.1000   -0.0001\n   200        0.0037             nan     0.1000   -0.0000\n   220        0.0031             nan     0.1000   -0.0001\n   240        0.0024             nan     0.1000   -0.0001\n   260        0.0018             nan     0.1000    0.0000\n   280        0.0012             nan     0.1000    0.0000\n   300        0.0009             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000    0.0000\n   360        0.0004             nan     0.1000    0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1471             nan     0.1000    0.0669\n     2        1.0395             nan     0.1000    0.0476\n     3        0.9467             nan     0.1000    0.0480\n     4        0.8642             nan     0.1000    0.0397\n     5        0.7925             nan     0.1000    0.0339\n     6        0.7360             nan     0.1000    0.0312\n     7        0.6830             nan     0.1000    0.0254\n     8        0.6341             nan     0.1000    0.0209\n     9        0.5967             nan     0.1000    0.0193\n    10        0.5578             nan     0.1000    0.0167\n    20        0.3582             nan     0.1000    0.0047\n    40        0.2303             nan     0.1000    0.0005\n    60        0.1875             nan     0.1000    0.0003\n    80        0.1639             nan     0.1000   -0.0007\n   100        0.1466             nan     0.1000   -0.0007\n   120        0.1341             nan     0.1000   -0.0008\n   140        0.1249             nan     0.1000   -0.0003\n   160        0.1199             nan     0.1000   -0.0005\n   180        0.1150             nan     0.1000   -0.0010\n   200        0.1091             nan     0.1000   -0.0008\n   220        0.1049             nan     0.1000   -0.0011\n   240        0.1010             nan     0.1000   -0.0008\n   260        0.1004             nan     0.1000   -0.0004\n   280        0.0988             nan     0.1000   -0.0011\n   300        0.0939             nan     0.1000   -0.0003\n   320        0.0912             nan     0.1000   -0.0009\n   340        0.0888             nan     0.1000   -0.0006\n   360        0.0868             nan     0.1000   -0.0009\n   380        0.0869             nan     0.1000   -0.0010\n   400        0.0843             nan     0.1000   -0.0006\n   420        0.0850             nan     0.1000   -0.0011\n   440        0.0841             nan     0.1000   -0.0003\n   460        0.0827             nan     0.1000   -0.0003\n   480        0.0812             nan     0.1000   -0.0012\n   500        0.0807             nan     0.1000   -0.0004\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1395             nan     0.1000    0.0737\n     2        1.0166             nan     0.1000    0.0593\n     3        0.9162             nan     0.1000    0.0493\n     4        0.8289             nan     0.1000    0.0428\n     5        0.7546             nan     0.1000    0.0350\n     6        0.6894             nan     0.1000    0.0320\n     7        0.6372             nan     0.1000    0.0238\n     8        0.5902             nan     0.1000    0.0213\n     9        0.5541             nan     0.1000    0.0165\n    10        0.5144             nan     0.1000    0.0178\n    20        0.3018             nan     0.1000    0.0016\n    40        0.1804             nan     0.1000   -0.0006\n    60        0.1397             nan     0.1000   -0.0016\n    80        0.1143             nan     0.1000   -0.0006\n   100        0.0981             nan     0.1000   -0.0016\n   120        0.0838             nan     0.1000   -0.0003\n   140        0.0732             nan     0.1000   -0.0011\n   160        0.0629             nan     0.1000   -0.0008\n   180        0.0565             nan     0.1000   -0.0005\n   200        0.0511             nan     0.1000   -0.0004\n   220        0.0456             nan     0.1000   -0.0006\n   240        0.0416             nan     0.1000   -0.0004\n   260        0.0374             nan     0.1000   -0.0004\n   280        0.0338             nan     0.1000   -0.0005\n   300        0.0309             nan     0.1000   -0.0000\n   320        0.0290             nan     0.1000   -0.0003\n   340        0.0269             nan     0.1000   -0.0001\n   360        0.0244             nan     0.1000   -0.0003\n   380        0.0232             nan     0.1000   -0.0002\n   400        0.0216             nan     0.1000   -0.0001\n   420        0.0200             nan     0.1000    0.0001\n   440        0.0181             nan     0.1000   -0.0002\n   460        0.0167             nan     0.1000   -0.0001\n   480        0.0157             nan     0.1000   -0.0001\n   500        0.0145             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1362             nan     0.1000    0.0736\n     2        1.0107             nan     0.1000    0.0609\n     3        0.9082             nan     0.1000    0.0485\n     4        0.8214             nan     0.1000    0.0417\n     5        0.7484             nan     0.1000    0.0362\n     6        0.6836             nan     0.1000    0.0311\n     7        0.6293             nan     0.1000    0.0275\n     8        0.5800             nan     0.1000    0.0227\n     9        0.5361             nan     0.1000    0.0213\n    10        0.4959             nan     0.1000    0.0176\n    20        0.2752             nan     0.1000    0.0051\n    40        0.1543             nan     0.1000   -0.0014\n    60        0.1074             nan     0.1000   -0.0006\n    80        0.0817             nan     0.1000    0.0001\n   100        0.0646             nan     0.1000   -0.0006\n   120        0.0532             nan     0.1000   -0.0001\n   140        0.0453             nan     0.1000   -0.0004\n   160        0.0369             nan     0.1000   -0.0002\n   180        0.0302             nan     0.1000   -0.0004\n   200        0.0255             nan     0.1000   -0.0002\n   220        0.0216             nan     0.1000   -0.0003\n   240        0.0184             nan     0.1000   -0.0003\n   260        0.0150             nan     0.1000   -0.0002\n   280        0.0126             nan     0.1000   -0.0001\n   300        0.0109             nan     0.1000   -0.0001\n   320        0.0094             nan     0.1000   -0.0001\n   340        0.0080             nan     0.1000   -0.0001\n   360        0.0068             nan     0.1000   -0.0001\n   380        0.0060             nan     0.1000   -0.0000\n   400        0.0053             nan     0.1000   -0.0000\n   420        0.0044             nan     0.1000   -0.0000\n   440        0.0038             nan     0.1000   -0.0000\n   460        0.0034             nan     0.1000   -0.0000\n   480        0.0029             nan     0.1000   -0.0000\n   500        0.0025             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1332             nan     0.1000    0.0718\n     2        1.0043             nan     0.1000    0.0626\n     3        0.9047             nan     0.1000    0.0474\n     4        0.8178             nan     0.1000    0.0411\n     5        0.7488             nan     0.1000    0.0337\n     6        0.6847             nan     0.1000    0.0306\n     7        0.6287             nan     0.1000    0.0268\n     8        0.5792             nan     0.1000    0.0215\n     9        0.5335             nan     0.1000    0.0220\n    10        0.4962             nan     0.1000    0.0150\n    20        0.2747             nan     0.1000    0.0062\n    40        0.1322             nan     0.1000   -0.0001\n    60        0.0865             nan     0.1000   -0.0011\n    80        0.0620             nan     0.1000   -0.0007\n   100        0.0446             nan     0.1000   -0.0004\n   120        0.0349             nan     0.1000   -0.0004\n   140        0.0266             nan     0.1000   -0.0003\n   160        0.0213             nan     0.1000   -0.0002\n   180        0.0173             nan     0.1000   -0.0002\n   200        0.0130             nan     0.1000   -0.0003\n   220        0.0110             nan     0.1000   -0.0001\n   240        0.0090             nan     0.1000   -0.0001\n   260        0.0074             nan     0.1000   -0.0002\n   280        0.0061             nan     0.1000   -0.0001\n   300        0.0046             nan     0.1000   -0.0001\n   320        0.0039             nan     0.1000   -0.0001\n   340        0.0031             nan     0.1000   -0.0000\n   360        0.0026             nan     0.1000   -0.0000\n   380        0.0021             nan     0.1000   -0.0000\n   400        0.0017             nan     0.1000   -0.0000\n   420        0.0014             nan     0.1000   -0.0000\n   440        0.0011             nan     0.1000   -0.0000\n   460        0.0009             nan     0.1000   -0.0000\n   480        0.0008             nan     0.1000   -0.0000\n   500        0.0006             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1326             nan     0.1000    0.0707\n     2        1.0062             nan     0.1000    0.0624\n     3        0.9061             nan     0.1000    0.0471\n     4        0.8203             nan     0.1000    0.0399\n     5        0.7461             nan     0.1000    0.0339\n     6        0.6796             nan     0.1000    0.0333\n     7        0.6202             nan     0.1000    0.0298\n     8        0.5715             nan     0.1000    0.0222\n     9        0.5285             nan     0.1000    0.0181\n    10        0.4913             nan     0.1000    0.0169\n    20        0.2669             nan     0.1000    0.0052\n    40        0.1270             nan     0.1000   -0.0008\n    60        0.0764             nan     0.1000   -0.0008\n    80        0.0498             nan     0.1000   -0.0003\n   100        0.0355             nan     0.1000   -0.0001\n   120        0.0248             nan     0.1000   -0.0002\n   140        0.0187             nan     0.1000   -0.0002\n   160        0.0151             nan     0.1000   -0.0002\n   180        0.0105             nan     0.1000   -0.0001\n   200        0.0083             nan     0.1000   -0.0001\n   220        0.0066             nan     0.1000   -0.0001\n   240        0.0054             nan     0.1000   -0.0001\n   260        0.0042             nan     0.1000   -0.0000\n   280        0.0031             nan     0.1000   -0.0000\n   300        0.0025             nan     0.1000    0.0000\n   320        0.0021             nan     0.1000   -0.0000\n   340        0.0016             nan     0.1000   -0.0000\n   360        0.0013             nan     0.1000   -0.0000\n   380        0.0010             nan     0.1000   -0.0000\n   400        0.0009             nan     0.1000   -0.0000\n   420        0.0007             nan     0.1000    0.0000\n   440        0.0006             nan     0.1000   -0.0000\n   460        0.0005             nan     0.1000   -0.0000\n   480        0.0005             nan     0.1000   -0.0000\n   500        0.0003             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1308             nan     0.1000    0.0746\n     2        1.0121             nan     0.1000    0.0532\n     3        0.9105             nan     0.1000    0.0506\n     4        0.8204             nan     0.1000    0.0444\n     5        0.7404             nan     0.1000    0.0397\n     6        0.6794             nan     0.1000    0.0267\n     7        0.6229             nan     0.1000    0.0229\n     8        0.5729             nan     0.1000    0.0216\n     9        0.5323             nan     0.1000    0.0187\n    10        0.4943             nan     0.1000    0.0150\n    20        0.2627             nan     0.1000    0.0059\n    40        0.1123             nan     0.1000    0.0013\n    60        0.0746             nan     0.1000   -0.0015\n    80        0.0492             nan     0.1000   -0.0002\n   100        0.0319             nan     0.1000   -0.0005\n   120        0.0250             nan     0.1000   -0.0004\n   140        0.0171             nan     0.1000   -0.0002\n   160        0.0126             nan     0.1000   -0.0002\n   180        0.0084             nan     0.1000   -0.0001\n   200        0.0062             nan     0.1000   -0.0000\n   220        0.0047             nan     0.1000   -0.0001\n   240        0.0035             nan     0.1000   -0.0000\n   260        0.0028             nan     0.1000   -0.0000\n   280        0.0021             nan     0.1000   -0.0000\n   300        0.0016             nan     0.1000   -0.0000\n   320        0.0013             nan     0.1000   -0.0000\n   340        0.0009             nan     0.1000   -0.0000\n   360        0.0007             nan     0.1000    0.0000\n   380        0.0007             nan     0.1000   -0.0000\n   400        0.0004             nan     0.1000   -0.0000\n   420        0.0003             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000    0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1283             nan     0.1000    0.0768\n     2        1.0007             nan     0.1000    0.0611\n     3        0.9009             nan     0.1000    0.0493\n     4        0.8158             nan     0.1000    0.0419\n     5        0.7425             nan     0.1000    0.0344\n     6        0.6781             nan     0.1000    0.0315\n     7        0.6212             nan     0.1000    0.0268\n     8        0.5726             nan     0.1000    0.0226\n     9        0.5323             nan     0.1000    0.0173\n    10        0.4942             nan     0.1000    0.0152\n    20        0.2605             nan     0.1000    0.0042\n    40        0.1140             nan     0.1000    0.0004\n    60        0.0620             nan     0.1000   -0.0010\n    80        0.0379             nan     0.1000   -0.0004\n   100        0.0253             nan     0.1000   -0.0002\n   120        0.0171             nan     0.1000   -0.0003\n   140        0.0121             nan     0.1000   -0.0004\n   160        0.0084             nan     0.1000   -0.0001\n   180        0.0059             nan     0.1000   -0.0001\n   200        0.0043             nan     0.1000   -0.0001\n   220        0.0038             nan     0.1000   -0.0001\n   240        0.0031             nan     0.1000    0.0000\n   260        0.0020             nan     0.1000   -0.0001\n   280        0.0013             nan     0.1000    0.0000\n   300        0.0011             nan     0.1000   -0.0000\n   320        0.0008             nan     0.1000   -0.0000\n   340        0.0007             nan     0.1000   -0.0000\n   360        0.0005             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1352             nan     0.1000    0.0764\n     2        1.0084             nan     0.1000    0.0584\n     3        0.9052             nan     0.1000    0.0501\n     4        0.8148             nan     0.1000    0.0448\n     5        0.7366             nan     0.1000    0.0350\n     6        0.6703             nan     0.1000    0.0299\n     7        0.6150             nan     0.1000    0.0229\n     8        0.5660             nan     0.1000    0.0225\n     9        0.5247             nan     0.1000    0.0174\n    10        0.4907             nan     0.1000    0.0152\n    20        0.2597             nan     0.1000    0.0039\n    40        0.1129             nan     0.1000   -0.0004\n    60        0.0620             nan     0.1000   -0.0005\n    80        0.0398             nan     0.1000   -0.0002\n   100        0.0262             nan     0.1000   -0.0002\n   120        0.0177             nan     0.1000   -0.0002\n   140        0.0122             nan     0.1000   -0.0004\n   160        0.0079             nan     0.1000   -0.0003\n   180        0.0059             nan     0.1000   -0.0000\n   200        0.0041             nan     0.1000   -0.0001\n   220        0.0028             nan     0.1000   -0.0000\n   240        0.0019             nan     0.1000   -0.0000\n   260        0.0015             nan     0.1000   -0.0000\n   280        0.0011             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000    0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0000             nan     0.1000   -0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1319             nan     0.1000    0.0768\n     2        1.0097             nan     0.1000    0.0575\n     3        0.9010             nan     0.1000    0.0520\n     4        0.8182             nan     0.1000    0.0368\n     5        0.7401             nan     0.1000    0.0372\n     6        0.6727             nan     0.1000    0.0305\n     7        0.6163             nan     0.1000    0.0268\n     8        0.5656             nan     0.1000    0.0237\n     9        0.5228             nan     0.1000    0.0191\n    10        0.4806             nan     0.1000    0.0177\n    20        0.2537             nan     0.1000    0.0066\n    40        0.1029             nan     0.1000   -0.0006\n    60        0.0591             nan     0.1000   -0.0008\n    80        0.0360             nan     0.1000   -0.0005\n   100        0.0251             nan     0.1000   -0.0004\n   120        0.0177             nan     0.1000   -0.0001\n   140        0.0131             nan     0.1000   -0.0004\n   160        0.0095             nan     0.1000   -0.0000\n   180        0.0068             nan     0.1000   -0.0002\n   200        0.0054             nan     0.1000   -0.0000\n   220        0.0045             nan     0.1000   -0.0000\n   240        0.0031             nan     0.1000   -0.0000\n   260        0.0024             nan     0.1000   -0.0000\n   280        0.0016             nan     0.1000   -0.0000\n   300        0.0012             nan     0.1000   -0.0000\n   320        0.0009             nan     0.1000   -0.0000\n   340        0.0007             nan     0.1000   -0.0000\n   360        0.0006             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1361             nan     0.1000    0.0664\n     2        1.0081             nan     0.1000    0.0651\n     3        0.9089             nan     0.1000    0.0453\n     4        0.8242             nan     0.1000    0.0398\n     5        0.7505             nan     0.1000    0.0360\n     6        0.6855             nan     0.1000    0.0304\n     7        0.6271             nan     0.1000    0.0261\n     8        0.5758             nan     0.1000    0.0234\n     9        0.5285             nan     0.1000    0.0218\n    10        0.4886             nan     0.1000    0.0177\n    20        0.2665             nan     0.1000    0.0035\n    40        0.1112             nan     0.1000    0.0006\n    60        0.0654             nan     0.1000   -0.0002\n    80        0.0423             nan     0.1000   -0.0003\n   100        0.0278             nan     0.1000   -0.0008\n   120        0.0200             nan     0.1000   -0.0002\n   140        0.0135             nan     0.1000   -0.0004\n   160        0.0098             nan     0.1000   -0.0003\n   180        0.0081             nan     0.1000   -0.0002\n   200        0.0055             nan     0.1000   -0.0001\n   220        0.0039             nan     0.1000   -0.0001\n   240        0.0028             nan     0.1000   -0.0001\n   260        0.0022             nan     0.1000   -0.0001\n   280        0.0015             nan     0.1000   -0.0000\n   300        0.0012             nan     0.1000   -0.0000\n   320        0.0009             nan     0.1000   -0.0000\n   340        0.0007             nan     0.1000   -0.0000\n   360        0.0006             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0004             nan     0.1000   -0.0000\n   420        0.0003             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1486             nan     0.1000    0.0693\n     2        1.0371             nan     0.1000    0.0560\n     3        0.9406             nan     0.1000    0.0438\n     4        0.8594             nan     0.1000    0.0364\n     5        0.7884             nan     0.1000    0.0352\n     6        0.7323             nan     0.1000    0.0272\n     7        0.6816             nan     0.1000    0.0256\n     8        0.6412             nan     0.1000    0.0166\n     9        0.6004             nan     0.1000    0.0184\n    10        0.5655             nan     0.1000    0.0174\n    20        0.3605             nan     0.1000    0.0051\n    40        0.2243             nan     0.1000   -0.0003\n    60        0.1808             nan     0.1000   -0.0010\n    80        0.1563             nan     0.1000   -0.0013\n   100        0.1349             nan     0.1000   -0.0002\n   120        0.1245             nan     0.1000   -0.0007\n   140        0.1107             nan     0.1000    0.0003\n   160        0.1032             nan     0.1000   -0.0005\n   180        0.0980             nan     0.1000   -0.0017\n   200        0.0934             nan     0.1000   -0.0008\n   220        0.0906             nan     0.1000    0.0001\n   240        0.0872             nan     0.1000   -0.0005\n   260        0.0837             nan     0.1000   -0.0011\n   280        0.0809             nan     0.1000   -0.0003\n   300        0.0775             nan     0.1000    0.0002\n   320        0.0746             nan     0.1000   -0.0002\n   340        0.0730             nan     0.1000   -0.0007\n   360        0.0715             nan     0.1000   -0.0005\n   380        0.0686             nan     0.1000   -0.0005\n   400        0.0677             nan     0.1000   -0.0004\n   420        0.0662             nan     0.1000   -0.0005\n   440        0.0656             nan     0.1000   -0.0004\n   460        0.0640             nan     0.1000   -0.0008\n   480        0.0640             nan     0.1000   -0.0005\n   500        0.0624             nan     0.1000   -0.0006\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1349             nan     0.1000    0.0777\n     2        1.0132             nan     0.1000    0.0609\n     3        0.9132             nan     0.1000    0.0517\n     4        0.8321             nan     0.1000    0.0363\n     5        0.7590             nan     0.1000    0.0348\n     6        0.6956             nan     0.1000    0.0303\n     7        0.6448             nan     0.1000    0.0252\n     8        0.5950             nan     0.1000    0.0228\n     9        0.5534             nan     0.1000    0.0191\n    10        0.5143             nan     0.1000    0.0172\n    20        0.2924             nan     0.1000    0.0059\n    40        0.1708             nan     0.1000   -0.0005\n    60        0.1252             nan     0.1000   -0.0010\n    80        0.0978             nan     0.1000   -0.0009\n   100        0.0808             nan     0.1000   -0.0002\n   120        0.0700             nan     0.1000   -0.0004\n   140        0.0605             nan     0.1000   -0.0001\n   160        0.0513             nan     0.1000   -0.0005\n   180        0.0461             nan     0.1000   -0.0005\n   200        0.0397             nan     0.1000   -0.0002\n   220        0.0359             nan     0.1000   -0.0004\n   240        0.0316             nan     0.1000   -0.0001\n   260        0.0287             nan     0.1000   -0.0002\n   280        0.0260             nan     0.1000   -0.0002\n   300        0.0235             nan     0.1000   -0.0001\n   320        0.0221             nan     0.1000   -0.0002\n   340        0.0203             nan     0.1000   -0.0004\n   360        0.0177             nan     0.1000   -0.0001\n   380        0.0161             nan     0.1000   -0.0002\n   400        0.0142             nan     0.1000   -0.0001\n   420        0.0128             nan     0.1000   -0.0002\n   440        0.0118             nan     0.1000   -0.0000\n   460        0.0105             nan     0.1000   -0.0001\n   480        0.0093             nan     0.1000   -0.0001\n   500        0.0087             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1351             nan     0.1000    0.0762\n     2        1.0134             nan     0.1000    0.0566\n     3        0.9129             nan     0.1000    0.0478\n     4        0.8316             nan     0.1000    0.0415\n     5        0.7607             nan     0.1000    0.0349\n     6        0.6939             nan     0.1000    0.0330\n     7        0.6405             nan     0.1000    0.0233\n     8        0.5900             nan     0.1000    0.0245\n     9        0.5457             nan     0.1000    0.0215\n    10        0.5070             nan     0.1000    0.0177\n    20        0.2781             nan     0.1000    0.0050\n    40        0.1420             nan     0.1000    0.0008\n    60        0.0982             nan     0.1000   -0.0009\n    80        0.0759             nan     0.1000   -0.0006\n   100        0.0561             nan     0.1000   -0.0002\n   120        0.0433             nan     0.1000   -0.0003\n   140        0.0334             nan     0.1000   -0.0006\n   160        0.0274             nan     0.1000   -0.0001\n   180        0.0228             nan     0.1000   -0.0001\n   200        0.0188             nan     0.1000   -0.0000\n   220        0.0159             nan     0.1000   -0.0002\n   240        0.0130             nan     0.1000    0.0000\n   260        0.0110             nan     0.1000   -0.0001\n   280        0.0095             nan     0.1000   -0.0002\n   300        0.0080             nan     0.1000   -0.0001\n   320        0.0070             nan     0.1000   -0.0001\n   340        0.0061             nan     0.1000   -0.0001\n   360        0.0051             nan     0.1000    0.0000\n   380        0.0042             nan     0.1000   -0.0000\n   400        0.0035             nan     0.1000   -0.0001\n   420        0.0029             nan     0.1000   -0.0000\n   440        0.0024             nan     0.1000   -0.0000\n   460        0.0020             nan     0.1000   -0.0000\n   480        0.0016             nan     0.1000   -0.0000\n   500        0.0014             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1318             nan     0.1000    0.0754\n     2        1.0063             nan     0.1000    0.0592\n     3        0.9009             nan     0.1000    0.0511\n     4        0.8209             nan     0.1000    0.0364\n     5        0.7440             nan     0.1000    0.0350\n     6        0.6802             nan     0.1000    0.0292\n     7        0.6276             nan     0.1000    0.0232\n     8        0.5778             nan     0.1000    0.0223\n     9        0.5346             nan     0.1000    0.0211\n    10        0.4963             nan     0.1000    0.0193\n    20        0.2649             nan     0.1000    0.0055\n    40        0.1214             nan     0.1000   -0.0004\n    60        0.0760             nan     0.1000   -0.0006\n    80        0.0575             nan     0.1000   -0.0008\n   100        0.0426             nan     0.1000   -0.0007\n   120        0.0300             nan     0.1000   -0.0004\n   140        0.0227             nan     0.1000   -0.0003\n   160        0.0177             nan     0.1000   -0.0001\n   180        0.0132             nan     0.1000   -0.0001\n   200        0.0103             nan     0.1000   -0.0001\n   220        0.0080             nan     0.1000   -0.0000\n   240        0.0065             nan     0.1000   -0.0001\n   260        0.0052             nan     0.1000   -0.0001\n   280        0.0042             nan     0.1000   -0.0001\n   300        0.0033             nan     0.1000   -0.0000\n   320        0.0024             nan     0.1000   -0.0000\n   340        0.0021             nan     0.1000   -0.0000\n   360        0.0018             nan     0.1000   -0.0000\n   380        0.0016             nan     0.1000   -0.0001\n   400        0.0013             nan     0.1000   -0.0000\n   420        0.0010             nan     0.1000    0.0000\n   440        0.0008             nan     0.1000   -0.0000\n   460        0.0006             nan     0.1000   -0.0000\n   480        0.0005             nan     0.1000   -0.0000\n   500        0.0004             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1364             nan     0.1000    0.0751\n     2        1.0112             nan     0.1000    0.0606\n     3        0.9097             nan     0.1000    0.0465\n     4        0.8215             nan     0.1000    0.0454\n     5        0.7483             nan     0.1000    0.0345\n     6        0.6827             nan     0.1000    0.0306\n     7        0.6260             nan     0.1000    0.0263\n     8        0.5801             nan     0.1000    0.0206\n     9        0.5385             nan     0.1000    0.0189\n    10        0.4989             nan     0.1000    0.0172\n    20        0.2594             nan     0.1000    0.0059\n    40        0.1147             nan     0.1000    0.0008\n    60        0.0709             nan     0.1000   -0.0014\n    80        0.0460             nan     0.1000   -0.0006\n   100        0.0315             nan     0.1000   -0.0003\n   120        0.0214             nan     0.1000   -0.0002\n   140        0.0160             nan     0.1000   -0.0003\n   160        0.0114             nan     0.1000   -0.0003\n   180        0.0091             nan     0.1000   -0.0002\n   200        0.0067             nan     0.1000    0.0000\n   220        0.0049             nan     0.1000   -0.0001\n   240        0.0037             nan     0.1000   -0.0001\n   260        0.0028             nan     0.1000   -0.0001\n   280        0.0020             nan     0.1000   -0.0000\n   300        0.0014             nan     0.1000   -0.0000\n   320        0.0011             nan     0.1000   -0.0000\n   340        0.0009             nan     0.1000   -0.0000\n   360        0.0007             nan     0.1000   -0.0000\n   380        0.0005             nan     0.1000   -0.0000\n   400        0.0004             nan     0.1000   -0.0000\n   420        0.0003             nan     0.1000   -0.0000\n   440        0.0003             nan     0.1000   -0.0000\n   460        0.0002             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1330             nan     0.1000    0.0734\n     2        1.0096             nan     0.1000    0.0591\n     3        0.9074             nan     0.1000    0.0469\n     4        0.8220             nan     0.1000    0.0383\n     5        0.7435             nan     0.1000    0.0371\n     6        0.6788             nan     0.1000    0.0277\n     7        0.6284             nan     0.1000    0.0221\n     8        0.5749             nan     0.1000    0.0237\n     9        0.5341             nan     0.1000    0.0186\n    10        0.4962             nan     0.1000    0.0163\n    20        0.2647             nan     0.1000    0.0054\n    40        0.1160             nan     0.1000    0.0003\n    60        0.0678             nan     0.1000   -0.0004\n    80        0.0453             nan     0.1000   -0.0005\n   100        0.0302             nan     0.1000   -0.0003\n   120        0.0192             nan     0.1000   -0.0004\n   140        0.0121             nan     0.1000   -0.0002\n   160        0.0081             nan     0.1000   -0.0000\n   180        0.0058             nan     0.1000   -0.0001\n   200        0.0041             nan     0.1000   -0.0000\n   220        0.0028             nan     0.1000   -0.0001\n   240        0.0021             nan     0.1000   -0.0001\n   260        0.0015             nan     0.1000   -0.0000\n   280        0.0011             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0004             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1286             nan     0.1000    0.0762\n     2        1.0057             nan     0.1000    0.0573\n     3        0.9064             nan     0.1000    0.0462\n     4        0.8172             nan     0.1000    0.0419\n     5        0.7359             nan     0.1000    0.0343\n     6        0.6717             nan     0.1000    0.0288\n     7        0.6160             nan     0.1000    0.0274\n     8        0.5663             nan     0.1000    0.0222\n     9        0.5248             nan     0.1000    0.0180\n    10        0.4818             nan     0.1000    0.0205\n    20        0.2629             nan     0.1000    0.0039\n    40        0.1285             nan     0.1000   -0.0005\n    60        0.0668             nan     0.1000   -0.0005\n    80        0.0357             nan     0.1000   -0.0002\n   100        0.0240             nan     0.1000   -0.0004\n   120        0.0180             nan     0.1000   -0.0003\n   140        0.0120             nan     0.1000   -0.0002\n   160        0.0087             nan     0.1000   -0.0002\n   180        0.0054             nan     0.1000    0.0000\n   200        0.0037             nan     0.1000   -0.0000\n   220        0.0023             nan     0.1000   -0.0000\n   240        0.0017             nan     0.1000   -0.0000\n   260        0.0013             nan     0.1000   -0.0000\n   280        0.0011             nan     0.1000   -0.0000\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0004             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0002             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0000             nan     0.1000    0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1362             nan     0.1000    0.0808\n     2        1.0087             nan     0.1000    0.0627\n     3        0.9071             nan     0.1000    0.0496\n     4        0.8222             nan     0.1000    0.0395\n     5        0.7433             nan     0.1000    0.0381\n     6        0.6755             nan     0.1000    0.0337\n     7        0.6213             nan     0.1000    0.0239\n     8        0.5722             nan     0.1000    0.0228\n     9        0.5321             nan     0.1000    0.0158\n    10        0.4904             nan     0.1000    0.0189\n    20        0.2564             nan     0.1000    0.0045\n    40        0.0965             nan     0.1000   -0.0001\n    60        0.0489             nan     0.1000   -0.0003\n    80        0.0306             nan     0.1000   -0.0005\n   100        0.0174             nan     0.1000   -0.0001\n   120        0.0129             nan     0.1000   -0.0003\n   140        0.0081             nan     0.1000   -0.0001\n   160        0.0053             nan     0.1000   -0.0001\n   180        0.0037             nan     0.1000   -0.0001\n   200        0.0027             nan     0.1000   -0.0001\n   220        0.0022             nan     0.1000   -0.0001\n   240        0.0019             nan     0.1000   -0.0001\n   260        0.0014             nan     0.1000   -0.0000\n   280        0.0009             nan     0.1000   -0.0000\n   300        0.0006             nan     0.1000   -0.0000\n   320        0.0006             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000    0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000    0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1323             nan     0.1000    0.0686\n     2        1.0041             nan     0.1000    0.0634\n     3        0.9018             nan     0.1000    0.0488\n     4        0.8174             nan     0.1000    0.0390\n     5        0.7427             nan     0.1000    0.0371\n     6        0.6804             nan     0.1000    0.0292\n     7        0.6201             nan     0.1000    0.0280\n     8        0.5749             nan     0.1000    0.0218\n     9        0.5313             nan     0.1000    0.0205\n    10        0.4973             nan     0.1000    0.0148\n    20        0.2640             nan     0.1000    0.0067\n    40        0.1056             nan     0.1000    0.0006\n    60        0.0576             nan     0.1000   -0.0006\n    80        0.0341             nan     0.1000   -0.0007\n   100        0.0230             nan     0.1000   -0.0004\n   120        0.0153             nan     0.1000   -0.0003\n   140        0.0107             nan     0.1000   -0.0003\n   160        0.0075             nan     0.1000   -0.0002\n   180        0.0065             nan     0.1000   -0.0002\n   200        0.0044             nan     0.1000   -0.0001\n   220        0.0036             nan     0.1000   -0.0002\n   240        0.0022             nan     0.1000   -0.0000\n   260        0.0017             nan     0.1000   -0.0001\n   280        0.0015             nan     0.1000   -0.0001\n   300        0.0009             nan     0.1000   -0.0000\n   320        0.0008             nan     0.1000   -0.0000\n   340        0.0006             nan     0.1000   -0.0000\n   360        0.0003             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000    0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0003             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1306             nan     0.1000    0.0777\n     2        1.0014             nan     0.1000    0.0570\n     3        0.9031             nan     0.1000    0.0472\n     4        0.8138             nan     0.1000    0.0431\n     5        0.7376             nan     0.1000    0.0373\n     6        0.6702             nan     0.1000    0.0295\n     7        0.6145             nan     0.1000    0.0252\n     8        0.5651             nan     0.1000    0.0225\n     9        0.5198             nan     0.1000    0.0198\n    10        0.4833             nan     0.1000    0.0155\n    20        0.2539             nan     0.1000    0.0061\n    40        0.1016             nan     0.1000   -0.0001\n    60        0.0497             nan     0.1000   -0.0006\n    80        0.0287             nan     0.1000   -0.0000\n   100        0.0167             nan     0.1000   -0.0003\n   120        0.0102             nan     0.1000   -0.0002\n   140        0.0076             nan     0.1000   -0.0003\n   160        0.0046             nan     0.1000    0.0000\n   180        0.0036             nan     0.1000   -0.0001\n   200        0.0024             nan     0.1000   -0.0000\n   220        0.0021             nan     0.1000   -0.0001\n   240        0.0018             nan     0.1000   -0.0000\n   260        0.0014             nan     0.1000   -0.0000\n   280        0.0012             nan     0.1000   -0.0001\n   300        0.0008             nan     0.1000   -0.0000\n   320        0.0007             nan     0.1000   -0.0000\n   340        0.0005             nan     0.1000   -0.0000\n   360        0.0006             nan     0.1000   -0.0000\n   380        0.0003             nan     0.1000   -0.0000\n   400        0.0002             nan     0.1000   -0.0000\n   420        0.0001             nan     0.1000   -0.0000\n   440        0.0001             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000    0.0000\n   500        0.0000             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1327             nan     0.1000    0.0755\n     2        1.0042             nan     0.1000    0.0633\n     3        0.9032             nan     0.1000    0.0484\n     4        0.8145             nan     0.1000    0.0408\n     5        0.7413             nan     0.1000    0.0335\n     6        0.6778             nan     0.1000    0.0302\n     7        0.6235             nan     0.1000    0.0242\n     8        0.5730             nan     0.1000    0.0226\n     9        0.5296             nan     0.1000    0.0196\n    10        0.4911             nan     0.1000    0.0161\n    20        0.2733             nan     0.1000    0.0062\n    40        0.1181             nan     0.1000    0.0005\n    60        0.0695             nan     0.1000   -0.0006\n    80        0.0437             nan     0.1000   -0.0004\n   100        0.0306             nan     0.1000   -0.0004\n   120        0.0222             nan     0.1000   -0.0004\n   140        0.0152             nan     0.1000   -0.0003\n   160        0.0111             nan     0.1000   -0.0001\n   180        0.0080             nan     0.1000   -0.0001\n   200        0.0056             nan     0.1000   -0.0001\n   220        0.0040             nan     0.1000   -0.0000\n   240        0.0028             nan     0.1000   -0.0000\n   260        0.0021             nan     0.1000   -0.0000\n   280        0.0016             nan     0.1000   -0.0000\n   300        0.0012             nan     0.1000   -0.0000\n   320        0.0009             nan     0.1000   -0.0000\n   340        0.0007             nan     0.1000   -0.0000\n   360        0.0006             nan     0.1000   -0.0000\n   380        0.0004             nan     0.1000   -0.0000\n   400        0.0003             nan     0.1000   -0.0000\n   420        0.0002             nan     0.1000   -0.0000\n   440        0.0002             nan     0.1000   -0.0000\n   460        0.0001             nan     0.1000   -0.0000\n   480        0.0001             nan     0.1000   -0.0000\n   500        0.0001             nan     0.1000   -0.0000\n\nsummary(gbm)\n\n\n\n\n\n\n\n\n                                var      rel.inf\nCell.size.L             Cell.size.L 3.691085e+01\nCell.shape.L           Cell.shape.L 3.352600e+01\nCl.thickness.L       Cl.thickness.L 4.965665e+00\nMarg.adhesion.L     Marg.adhesion.L 2.933673e+00\nEpith.c.size.Q       Epith.c.size.Q 2.113795e+00\nCell.size^8             Cell.size^8 1.847047e+00\nEpith.c.size.L       Epith.c.size.L 1.683156e+00\nCell.shape^8           Cell.shape^8 1.229586e+00\nCell.size^4             Cell.size^4 1.143472e+00\nMarg.adhesion^9     Marg.adhesion^9 1.056156e+00\nMarg.adhesion^5     Marg.adhesion^5 9.743726e-01\nCell.size^5             Cell.size^5 7.047590e-01\nEpith.c.size^8       Epith.c.size^8 6.461022e-01\nCell.size^7             Cell.size^7 6.118808e-01\nCell.size.Q             Cell.size.Q 6.079505e-01\nCl.thickness^9       Cl.thickness^9 5.640595e-01\nCell.shape^9           Cell.shape^9 4.917112e-01\nBl.cromatin5           Bl.cromatin5 4.778087e-01\nBl.cromatin4           Bl.cromatin4 4.620270e-01\nCell.size^9             Cell.size^9 4.150179e-01\nMarg.adhesion.C     Marg.adhesion.C 3.930988e-01\nCl.thickness^7       Cl.thickness^7 3.905901e-01\nMarg.adhesion^8     Marg.adhesion^8 3.898080e-01\nEpith.c.size^9       Epith.c.size^9 3.869839e-01\nCl.thickness^6       Cl.thickness^6 3.857848e-01\nCell.shape^7           Cell.shape^7 3.399454e-01\nCl.thickness^4       Cl.thickness^4 3.041098e-01\nCell.size^6             Cell.size^6 2.973393e-01\nCell.shape^4           Cell.shape^4 2.903475e-01\nMarg.adhesion^7     Marg.adhesion^7 2.897045e-01\nCell.shape.Q           Cell.shape.Q 2.866104e-01\nMarg.adhesion^4     Marg.adhesion^4 2.855576e-01\nEpith.c.size^6       Epith.c.size^6 2.762241e-01\nCell.shape.C           Cell.shape.C 2.617251e-01\nCl.thickness.Q       Cl.thickness.Q 2.533263e-01\nCl.thickness^8       Cl.thickness^8 2.287122e-01\nMarg.adhesion.Q     Marg.adhesion.Q 2.209701e-01\nCell.size.C             Cell.size.C 2.027992e-01\nEpith.c.size^7       Epith.c.size^7 1.962867e-01\nBl.cromatin3           Bl.cromatin3 1.347193e-01\nEpith.c.size^5       Epith.c.size^5 1.027456e-01\nCell.shape^6           Cell.shape^6 9.135224e-02\nEpith.c.size.C       Epith.c.size.C 7.799872e-02\nMitoses2                   Mitoses2 7.275433e-02\nNormal.nucleoli4   Normal.nucleoli4 6.369412e-02\nCl.thickness^5       Cl.thickness^5 6.250742e-02\nCell.shape^5           Cell.shape^5 5.888088e-02\nCl.thickness.C       Cl.thickness.C 5.784223e-02\nNormal.nucleoli10 Normal.nucleoli10 5.642045e-02\nEpith.c.size^4       Epith.c.size^4 5.230402e-02\nNormal.nucleoli8   Normal.nucleoli8 4.854562e-02\nMarg.adhesion^6     Marg.adhesion^6 3.690114e-02\nBl.cromatin7           Bl.cromatin7 3.293722e-02\nNormal.nucleoli3   Normal.nucleoli3 4.768045e-03\nNormal.nucleoli2   Normal.nucleoli2 5.727921e-04\nBl.cromatin8           Bl.cromatin8 4.571245e-05\nBl.cromatin2           Bl.cromatin2 1.529291e-06\nBl.cromatin6           Bl.cromatin6 0.000000e+00\nBl.cromatin9           Bl.cromatin9 0.000000e+00\nBl.cromatin10         Bl.cromatin10 0.000000e+00\nNormal.nucleoli5   Normal.nucleoli5 0.000000e+00\nNormal.nucleoli6   Normal.nucleoli6 0.000000e+00\nNormal.nucleoli7   Normal.nucleoli7 0.000000e+00\nNormal.nucleoli9   Normal.nucleoli9 0.000000e+00\nMitoses3                   Mitoses3 0.000000e+00\nMitoses4                   Mitoses4 0.000000e+00\nMitoses5                   Mitoses5 0.000000e+00\nMitoses6                   Mitoses6 0.000000e+00\nMitoses7                   Mitoses7 0.000000e+00\nMitoses8                   Mitoses8 0.000000e+00\nMitoses10                 Mitoses10 0.000000e+00\n\npred_gbm&lt;-predict(gbm,BreastCancer)\nconfusionMatrix(pred_gbm, BreastCancer$Class)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       456        11\n  malignant      2       230\n                                          \n               Accuracy : 0.9814          \n                 95% CI : (0.9684, 0.9901)\n    No Information Rate : 0.6552          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9585          \n                                          \n Mcnemar's Test P-Value : 0.0265          \n                                          \n            Sensitivity : 0.9956          \n            Specificity : 0.9544          \n         Pos Pred Value : 0.9764          \n         Neg Pred Value : 0.9914          \n             Prevalence : 0.6552          \n         Detection Rate : 0.6524          \n   Detection Prevalence : 0.6681          \n      Balanced Accuracy : 0.9750          \n                                          \n       'Positive' Class : benign          \n                                          \n\nroc_gbm&lt;-pROC::roc(BreastCancer$Class, as.numeric(pred_gbm))\n\nSetting levels: control = benign, case = malignant\n\n\nSetting direction: controls &lt; cases\n\nroc_gbm\n\n\nCall:\nroc.default(response = BreastCancer$Class, predictor = as.numeric(pred_gbm))\n\nData: as.numeric(pred_gbm) in 458 controls (BreastCancer$Class benign) &lt; 241 cases (BreastCancer$Class malignant).\nArea under the curve: 0.975\n\n\n\n\n8.3.2.2 Extreme gradient boost machine\nIn the examples above, the outcome variable is treated as a factor. Extreme gradient boost machine xgboost requires conversion to numeric variable.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nThe following object is masked from 'package:rattle':\n\n    xgboost\n\nlibrary(caret)\n\ndata(\"BreastCancer\",package = \"mlbench\")\n#predict breast cancer\n\nBreastCancer$Class&lt;-as.character(BreastCancer$Class)\nBreastCancer$Class[BreastCancer$Class==\"benign\"]&lt;-0\nBreastCancer$Class[BreastCancer$Class==\"malignant\"]&lt;-1\nBreastCancer$Class&lt;-as.numeric(BreastCancer$Class)\n\n#remove ID column\n#remove column a=with NA \n#remaining 9 columns\n#convert multiple columns to numeric\n#lapply output a list\nBreastCancer2&lt;-lapply(BreastCancer[,-c(1,7)], as.numeric)\nBreastCancer2&lt;-as.data.frame(BreastCancer2)\n\nset.seed(1234)\nparts = createDataPartition(BreastCancer2$Class, p = 0.75, list=F)\ntrain = BreastCancer2[parts, ]\ntest = BreastCancer2[-parts, ]\n\nX_train = data.matrix(train[,-9])          # independent variables for train\ny_train = train[,9]                        # dependent variables for train\nX_test = data.matrix(test[,-9])            # independent variables for test\ny_test = test[,9]                          # dependent variables for test\n\n# convert the train and test data into xgboost matrix type.\nxgboost_train = xgb.DMatrix(data=X_train, label=as.matrix(y_train))\nxgboost_test = xgb.DMatrix(data=X_test, label=as.matrix(y_test))\n\n# train a model using our training data\n# nthread is the number of CPU threads we use\n# nrounds is the number of passes on the data\n\n#the function xgboost exist in xgboost and rattle\nmodel &lt;- xgboost::xgboost(data = xgboost_train, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\", verbose = 2)                           \n\n[1] train-logloss:0.221760 \n[2] train-logloss:0.129150 \n\nsummary(model)\n\n               Length Class              Mode       \nhandle            1   xgb.Booster.handle externalptr\nraw            5890   -none-             raw        \nniter             1   -none-             numeric    \nevaluation_log    2   data.table         list       \ncall             17   -none-             call       \nparams            5   -none-             list       \ncallbacks         2   -none-             list       \nfeature_names     8   -none-             character  \nnfeatures         1   -none-             numeric    \n\n#use model to make predictions on test data\npred_test = predict(model, xgboost_test)\npred_test\n\n  [1] 0.04247886 0.04247886 0.93661994 0.04247886 0.04247886 0.93661994\n  [7] 0.04247886 0.04247886 0.50793731 0.84730983 0.04247886 0.04247886\n [13] 0.04247886 0.93661994 0.13179043 0.61858213 0.04247886 0.93661994\n [19] 0.04247886 0.90757442 0.04247886 0.93661994 0.50793731 0.10175808\n [25] 0.04247886 0.13179043 0.04247886 0.04247886 0.04247886 0.04247886\n [31] 0.04247886 0.04247886 0.04247886 0.93661994 0.90757442 0.90757442\n [37] 0.93661994 0.90757442 0.04247886 0.61858213 0.04247886 0.04247886\n [43] 0.04247886 0.04247886 0.04247886 0.93661994 0.04247886 0.61858213\n [49] 0.04247886 0.04247886 0.93661994 0.61858213 0.93661994 0.04247886\n [55] 0.93661994 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994\n [61] 0.93661994 0.04247886 0.93661994 0.93661994 0.04247886 0.93661994\n [67] 0.13179043 0.04247886 0.93661994 0.61858213 0.04247886 0.93661994\n [73] 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994 0.93661994\n [79] 0.04247886 0.28787071 0.84532809 0.04247886 0.04247886 0.04247886\n [85] 0.04247886 0.93661994 0.93661994 0.93661994 0.04247886 0.84730983\n [91] 0.04247886 0.61858213 0.84730983 0.04247886 0.93661994 0.93661994\n [97] 0.93661994 0.61858213 0.04247886 0.04247886 0.04247886 0.04247886\n[103] 0.84730983 0.93661994 0.04247886 0.04247886 0.04247886 0.04247886\n[109] 0.04247886 0.93661994 0.10175808 0.04247886 0.10175808 0.61858213\n[115] 0.90757442 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994\n[121] 0.04247886 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994\n[127] 0.04247886 0.93661994 0.04247886 0.04247886 0.04247886 0.04247886\n[133] 0.04247886 0.04247886 0.93661994 0.04247886 0.04247886 0.93661994\n[139] 0.04247886 0.04247886 0.04247886 0.93661994 0.93661994 0.13179043\n[145] 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994 0.04247886\n[151] 0.04247886 0.04247886 0.93661994 0.04247886 0.93661994 0.93661994\n[157] 0.04247886 0.93661994 0.93661994 0.10175808 0.04247886 0.04247886\n[163] 0.04247886 0.04247886 0.04247886 0.61858213 0.04247886 0.93661994\n[169] 0.04247886 0.10175808 0.04247886 0.93661994 0.04247886 0.93661994\n\n#classify 1 if prediction &gt;.5\nprediction &lt;- as.numeric(pred_test &gt; 0.5)\nprint(head(prediction))\n\n[1] 0 0 1 0 0 1\n\nerr &lt;- mean(as.numeric(pred_test &gt; 0.5) != y_test)\nprint(paste(\"test-error=\", err))\n\n[1] \"test-error= 0.0632183908045977\"\n\n#plot of the first 2 trees\nxgb.plot.tree(model = model, trees = 1:2)\n\n\n\n\n\n\n\n8.3.2.3 xgboost survival\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\n\n\nAttaching package: 'mlr3verse'\n\n\nThe following object is masked from 'package:randomForestSRC':\n\n    tune\n\n\nThe following object is masked from 'package:bitops':\n\n    %&gt;&gt;%\n\nlibrary(xgboost)\nlibrary(caret)\nlibrary(mlbench)\nlibrary(mlr3pipelines)\n\n\nAttaching package: 'mlr3pipelines'\n\nThe following object is masked from 'package:bitops':\n\n    %&gt;&gt;%\n\nlibrary(mlr3hyperband)\n\nLoading required package: mlr3tuning\n\n\nLoading required package: paradox\n\n\n\nAttaching package: 'mlr3tuning'\n\n\nThe following object is masked from 'package:randomForestSRC':\n\n    tune\n\nlibrary(BBmisc)\n\n\nAttaching package: 'BBmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    coalesce, collapse, symdiff\n\n\nThe following object is masked from 'package:grid':\n\n    explode\n\n\nThe following object is masked from 'package:base':\n\n    isFALSE\n\ndata(Melanoma, package = \"MASS\")\n\nN &lt;- length(Melanoma$status)\n\n#table(Melanoma$ph.karno, cancer$pat.karno)\n\n## if physician's KPS unavailable, then use the patient's\n#h &lt;- which(is.na(cancer$ph.karno))\n#cancer$ph.karno[h] &lt;- cancer$pat.karno[h]\n\ntimes &lt;- Melanoma$time\ntimes &lt;- ceiling(times/7)  ## weeks\n\n#1 died from melanoma, 2 alive, 3 dead from other causes.\n##delta: 0=censored, 1=dead\ndelta=ifelse(Melanoma$status==2,0,1)\n\n## matrix of observed covariates\nx.train &lt;- cbind(Melanoma$sex, Melanoma$age, Melanoma$thickness)\n\n#provide column names\ndimnames(x.train)[[2]] &lt;- c('M(1):F(0)','age', 'thickness')\n\n\n\n\n8.3.3 Bayesian trees method\n\n8.3.3.1 BART\nBART or Bayesian additive regression trees is a non-parametric method that uses a sum of Bayesian trees to estimate an unknown function. Every tree acts as a weak learner in this ensemle method. It can also be used in causal inference. It uses tuning parameters derived from Bayesian priors. Each predicted value has a posterior distribution. BART uses a regularization prior that forces each tree to be able to explain only a limited subset of the relationships between the covariates and the predictor variable. In some instances, BART outperform xgboost.\n\nlibrary(BART)\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:BBmisc':\n\n    collapse\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nLoading required package: nnet\n\ndata(Melanoma, package = \"MASS\")\n\nN &lt;- length(Melanoma$status)\n\n#table(Melanoma$ph.karno, cancer$pat.karno)\n\n## if physician's KPS unavailable, then use the patient's\n#h &lt;- which(is.na(cancer$ph.karno))\n#cancer$ph.karno[h] &lt;- cancer$pat.karno[h]\n\ntimes &lt;- Melanoma$time\ntimes &lt;- ceiling(times/7)  ## weeks\n\n#1 died from melanoma, 2 alive, 3 dead from other causes.\n##delta: 0=censored, 1=dead\ndelta=ifelse(Melanoma$status==2,0,1)\n\n## matrix of observed covariates\nx.train &lt;- cbind(Melanoma$sex, Melanoma$age, Melanoma$thickness)\n\n#provide column names\ndimnames(x.train)[[2]] &lt;- c('M(1):F(0)','age', 'thickness')\n\ntable(x.train[ , 1])\n\n\n  0   1 \n126  79 \n\nsummary(x.train[ , 2])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   4.00   42.00   54.00   52.46   65.00   95.00 \n\ntable(x.train[ , 3])\n\n\n  0.1  0.16  0.24  0.32  0.48  0.58  0.64  0.65  0.81  0.97  1.03  1.13  1.29 \n    1     7     1     6     4     1     4    10    11    11     1     4    16 \n 1.34  1.37  1.45  1.53  1.62  1.76  1.78  1.94   2.1  2.24  2.26  2.34  2.42 \n    2     1     3     1    12     1     2    10     3     1     5     1     1 \n 2.58  2.74   2.9  3.06  3.22  3.54  3.56  3.87  4.04  4.09  4.19  4.51  4.82 \n    9     1     3     2    10     8     1     6     1     1     2     1     1 \n 4.83  4.84  5.16  5.48  5.64   5.8  6.12  6.44  6.76  7.06  7.09  7.41  7.73 \n    2     5     3     2     1     2     2     1     1     2     2     1     2 \n 7.89  8.06  8.38  8.54  9.66 12.08 12.24 12.56 12.88 13.85 14.66 17.42 \n    1     1     1     1     1     1     1     1     2     1     1     1 \n\n##test BART with token run to ensure installation works\nset.seed(99)\npost &lt;- surv.bart(x.train=x.train, times=times, delta=delta,\n                  nskip=1, ndpost=1, keepevery=1)\n\n*****Calling gbart: type=2\n*****Data:\ndata:n,p,np: 17042, 4, 0\ny1,yn: 1.000000, 0.000000\nx1,x[n*p]: 2.000000, 2.900000\n*****Number of Trees: 50\n*****Number of Cut Points: 100 ... 63\n*****burn,nd,thin: 1,1,1\n*****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.212132,3,1,-2.6383\n*****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,3,0\n*****printevery: 100\n\nMCMC\ndone 0 (out of 2)\ntime: 0s\ntrcnt,tecnt: 1,0\n\npost &lt;- surv.bart(x.train=x.train, times=times, delta=delta,\n                      seed=99)\n\n*****Calling gbart: type=2\n*****Data:\ndata:n,p,np: 17042, 4, 0\ny1,yn: 1.000000, 0.000000\nx1,x[n*p]: 2.000000, 2.900000\n*****Number of Trees: 50\n*****Number of Cut Points: 100 ... 63\n*****burn,nd,thin: 250,10000,10\n*****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.212132,3,1,-2.6383\n*****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,3,0\n*****printevery: 100\n\nMCMC\ndone 0 (out of 10250)\ndone 100 (out of 10250)\ndone 200 (out of 10250)\ndone 300 (out of 10250)\ndone 400 (out of 10250)\ndone 500 (out of 10250)\ndone 600 (out of 10250)\ndone 700 (out of 10250)\ndone 800 (out of 10250)\ndone 900 (out of 10250)\ndone 1000 (out of 10250)\ndone 1100 (out of 10250)\ndone 1200 (out of 10250)\ndone 1300 (out of 10250)\ndone 1400 (out of 10250)\ndone 1500 (out of 10250)\ndone 1600 (out of 10250)\ndone 1700 (out of 10250)\ndone 1800 (out of 10250)\ndone 1900 (out of 10250)\ndone 2000 (out of 10250)\ndone 2100 (out of 10250)\ndone 2200 (out of 10250)\ndone 2300 (out of 10250)\ndone 2400 (out of 10250)\ndone 2500 (out of 10250)\ndone 2600 (out of 10250)\ndone 2700 (out of 10250)\ndone 2800 (out of 10250)\ndone 2900 (out of 10250)\ndone 3000 (out of 10250)\ndone 3100 (out of 10250)\ndone 3200 (out of 10250)\ndone 3300 (out of 10250)\ndone 3400 (out of 10250)\ndone 3500 (out of 10250)\ndone 3600 (out of 10250)\ndone 3700 (out of 10250)\ndone 3800 (out of 10250)\ndone 3900 (out of 10250)\ndone 4000 (out of 10250)\ndone 4100 (out of 10250)\ndone 4200 (out of 10250)\ndone 4300 (out of 10250)\ndone 4400 (out of 10250)\ndone 4500 (out of 10250)\ndone 4600 (out of 10250)\ndone 4700 (out of 10250)\ndone 4800 (out of 10250)\ndone 4900 (out of 10250)\ndone 5000 (out of 10250)\ndone 5100 (out of 10250)\ndone 5200 (out of 10250)\ndone 5300 (out of 10250)\ndone 5400 (out of 10250)\ndone 5500 (out of 10250)\ndone 5600 (out of 10250)\ndone 5700 (out of 10250)\ndone 5800 (out of 10250)\ndone 5900 (out of 10250)\ndone 6000 (out of 10250)\ndone 6100 (out of 10250)\ndone 6200 (out of 10250)\ndone 6300 (out of 10250)\ndone 6400 (out of 10250)\ndone 6500 (out of 10250)\ndone 6600 (out of 10250)\ndone 6700 (out of 10250)\ndone 6800 (out of 10250)\ndone 6900 (out of 10250)\ndone 7000 (out of 10250)\ndone 7100 (out of 10250)\ndone 7200 (out of 10250)\ndone 7300 (out of 10250)\ndone 7400 (out of 10250)\ndone 7500 (out of 10250)\ndone 7600 (out of 10250)\ndone 7700 (out of 10250)\ndone 7800 (out of 10250)\ndone 7900 (out of 10250)\ndone 8000 (out of 10250)\ndone 8100 (out of 10250)\ndone 8200 (out of 10250)\ndone 8300 (out of 10250)\ndone 8400 (out of 10250)\ndone 8500 (out of 10250)\ndone 8600 (out of 10250)\ndone 8700 (out of 10250)\ndone 8800 (out of 10250)\ndone 8900 (out of 10250)\ndone 9000 (out of 10250)\ndone 9100 (out of 10250)\ndone 9200 (out of 10250)\ndone 9300 (out of 10250)\ndone 9400 (out of 10250)\ndone 9500 (out of 10250)\ndone 9600 (out of 10250)\ndone 9700 (out of 10250)\ndone 9800 (out of 10250)\ndone 9900 (out of 10250)\ndone 10000 (out of 10250)\ndone 10100 (out of 10250)\ndone 10200 (out of 10250)\ntime: 122s\ntrcnt,tecnt: 1000,0\n\npre &lt;- surv.pre.bart(times=times, delta=delta, x.train=x.train,\n                     x.test=x.train)\n\nK &lt;- pre$K\nM &lt;- nrow(post$yhat.train)\n\npre$tx.test &lt;- rbind(pre$tx.test, pre$tx.test)\npre$tx.test[ , 2] &lt;- c(rep(1, N*K), rep(2, N*K))\n## sex pushed to col 2, since time is always in col 1\n\npred &lt;- predict(post, newdata=pre$tx.test)\n\n*****In main of C++ for bart prediction\ntc (threadcount): 1\nnumber of bart draws: 1000\nnumber of trees in bart sum: 50\nnumber of x columns: 4\nfrom x,np,p: 4, 67240\n***using serial code\n\npd &lt;- matrix(nrow=M, ncol=2*K)\n\nfor(j in 1:K) {\n  h &lt;- seq(j, N*K, by=K)\n  pd[ , j] &lt;- apply(pred$surv.test[ , h], 1, mean)\n  pd[ , j+K] &lt;- apply(pred$surv.test[ , h+N*K], 1, mean)\n}\n\npd.mu  &lt;- apply(pd, 2, mean)\npd.025 &lt;- apply(pd, 2, quantile, probs=0.025)\npd.975 &lt;- apply(pd, 2, quantile, probs=0.975)\n\nmales &lt;- 1:K\nfemales &lt;- males+K\n\nplot(c(0, pre$times), c(1, pd.mu[males]), type='s', col='blue',\n     ylim=0:1, ylab='S(t, x)', xlab='t (weeks)',\n     main=paste('Melanoma ex. (MASS:: Melanoma)',\n                \"Friedman's partial dependence function\",\n                'Male (blue) vs. Female (red)', sep='\\n'))\nlines(c(0, pre$times), c(1, pd.025[males]), col='blue', type='s', lty=2)\nlines(c(0, pre$times), c(1, pd.975[males]), col='blue', type='s', lty=2)\nlines(c(0, pre$times), c(1, pd.mu[females]), col='red', type='s')\nlines(c(0, pre$times), c(1, pd.025[females]), col='red', type='s', lty=2)\nlines(c(0, pre$times), c(1, pd.975[females]), col='red', type='s', lty=2)\n\n\n\n\n\n\n\n\n\n\n\n8.3.4 CatBoost\nCatBoost is a boosted method designed to handle categorical data without further need preprocessing of the categorical data (one hot encoding of categorical data). It uses symmetric weighted quantile sketch(SWQS) to handle missing values. CatBoost internally scale the data. Installing of catboost requires checking of device to see if it has GPU support using CUDA. Installation is from https://catboost.ai/en/docs/installation/r-installation-binary-installation\n\nlibrary(catboost)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#knn",
    "href": "machine-learning.html#knn",
    "title": "8  Machine learning",
    "section": "8.4 KNN",
    "text": "8.4 KNN\nK nearest neighbour (KNN) uses ’feature similarity based on measure of distance between data points to make prediction. The K in KNN refers to the number of neighbours to define the case for similarity. K nearest neighbour is available from the caret library.\n\nlibrary(caret)\n\ndata(\"BreastCancer\",package = \"mlbench\")\ncolnames(BreastCancer)\n\n [1] \"Id\"              \"Cl.thickness\"    \"Cell.size\"       \"Cell.shape\"     \n [5] \"Marg.adhesion\"   \"Epith.c.size\"    \"Bare.nuclei\"     \"Bl.cromatin\"    \n [9] \"Normal.nucleoli\" \"Mitoses\"         \"Class\"          \n\n#note Class is benign or malignant of class factor\n#column Bare.nuclei removed due to NA\nBreastCancer&lt;-BreastCancer[,-c(1,7)]\n\n#split data\nset.seed(123)\nsplit = caTools::sample.split(BreastCancer$Class, SplitRatio = 0.75)\nTrain = subset(BreastCancer, split == TRUE)\nTest = subset(BreastCancer, split == FALSE)\n\n#grid of values to test in cross-validation.\nknn_Grid &lt;-  expand.grid(k = c(1:15))\n\nknn_Control &lt;- trainControl(method = \"cv\",\n                           number = 10, \n                           # repeats = 10, # uncomment for repeatedcv \n                           ## Estimate class probabilities\n                           classProbs = TRUE,\n                           ## Evaluate performance using \n                           ## the following function\n                           summaryFunction = twoClassSummary)\n\n#scaling data is performed here under preProcess\n\nknn &lt;- caret::train(Class ~ ., \n                    data = Train, \n                  method = \"knn\",\n                 trControl=knn_Control,\n                 tuneGrid=knn_Grid,\n                 #optimise with roc metric\n                 metric=\"ROC\")\n\n\nsummary(knn)\n\n            Length Class      Mode     \nlearn        2     -none-     list     \nk            1     -none-     numeric  \ntheDots      0     -none-     list     \nxNames      71     -none-     character\nproblemType  1     -none-     character\ntuneValue    1     data.frame list     \nobsLevels    2     -none-     character\nparam        0     -none-     list     \n\npred_knn&lt;-predict(knn,Test)\nconfusionMatrix(pred_knn, Test$Class)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       114        25\n  malignant      0        35\n                                          \n               Accuracy : 0.8563          \n                 95% CI : (0.7952, 0.9048)\n    No Information Rate : 0.6552          \n    P-Value [Acc &gt; NIR] : 1.883e-09       \n                                          \n                  Kappa : 0.6472          \n                                          \n Mcnemar's Test P-Value : 1.587e-06       \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.5833          \n         Pos Pred Value : 0.8201          \n         Neg Pred Value : 1.0000          \n             Prevalence : 0.6552          \n         Detection Rate : 0.6552          \n   Detection Prevalence : 0.7989          \n      Balanced Accuracy : 0.7917          \n                                          \n       'Positive' Class : benign          \n                                          \n\nroc_knn&lt;-pROC::roc(Test$Class, as.numeric(pred_knn))\n\nSetting levels: control = benign, case = malignant\n\n\nSetting direction: controls &lt; cases\n\nroc_knn\n\n\nCall:\nroc.default(response = Test$Class, predictor = as.numeric(pred_knn))\n\nData: as.numeric(pred_knn) in 114 controls (Test$Class benign) &lt; 60 cases (Test$Class malignant).\nArea under the curve: 0.7917\n\n#https://plotly.com/r/knn-classification/\n\npdb &lt;- cbind(Test[,-9], Test[,9])\npdb &lt;- cbind(pdb, pred_knn)\n\nfig &lt;- plotly::plot_ly(data = pdb,\n    x = ~as.numeric(Test$Cl.thickness), \n    y = ~as.numeric(Test$Epith.c.size), \n    type = 'scatter', mode = 'markers',color = ~pred_knn, colors = 'RdBu', \n    symbol = ~Test$Class, split = ~Test$Class, \n    symbols = c('square-dot','circle-dot'), \n    marker = list(size = 12, line = list(color = 'black', width = 1)))\n\nfig",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#support-vector-machine",
    "href": "machine-learning.html#support-vector-machine",
    "title": "8  Machine learning",
    "section": "8.5 Support vector machine",
    "text": "8.5 Support vector machine\nIn brief, support vector machine regression (SVR) can be seen as a way to enhance data which may not be easily separated in its native space. It manipulates data from low dimension to higher dimension in feature space and which can reveal relationship not discernible in low dimensional space. It does this around the hyperparameter controlling the margin of the data from a fitted line in a way not dissimilar from fitting a regression line based on minimising least squares. The default setting is radial basis function.\n\nlibrary(e1071)\n\n\nAttaching package: 'e1071'\n\n\nThe following object is masked from 'package:mlr3tuning':\n\n    tune\n\n\nThe following object is masked from 'package:mlr3verse':\n\n    tune\n\n\nThe following objects are masked from 'package:randomForestSRC':\n\n    impute, tune\n\nlibrary(caret)\n\n# The Breast cancer data is used again from knn\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n#scaling data is performed here under preProcess\n\nsvm_Linear &lt;- caret::train(Class ~ ., \n                    data = Train, \n                  method = \"svmLinear\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneLength = 10)\n\n\nsummary(svm_Linear)\n\nLength  Class   Mode \n     1   ksvm     S4 \n\npred&lt;-predict(svm_Linear,BreastCancer)\nconfusionMatrix(pred, BreastCancer$Class)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  benign malignant\n  benign       455        13\n  malignant      3       228\n                                          \n               Accuracy : 0.9771          \n                 95% CI : (0.9631, 0.9869)\n    No Information Rate : 0.6552          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.9488          \n                                          \n Mcnemar's Test P-Value : 0.02445         \n                                          \n            Sensitivity : 0.9934          \n            Specificity : 0.9461          \n         Pos Pred Value : 0.9722          \n         Neg Pred Value : 0.9870          \n             Prevalence : 0.6552          \n         Detection Rate : 0.6509          \n   Detection Prevalence : 0.6695          \n      Balanced Accuracy : 0.9698          \n                                          \n       'Positive' Class : benign          \n                                          \n\nroc_svm&lt;-pROC::roc(BreastCancer$Class, as.numeric(pred))\n\nSetting levels: control = benign, case = malignant\n\n\nSetting direction: controls &lt; cases\n\nroc_svm\n\n\nCall:\nroc.default(response = BreastCancer$Class, predictor = as.numeric(pred))\n\nData: as.numeric(pred) in 458 controls (BreastCancer$Class benign) &lt; 241 cases (BreastCancer$Class malignant).\nArea under the curve: 0.9698",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#non-negative-matrix-factorisation",
    "href": "machine-learning.html#non-negative-matrix-factorisation",
    "title": "8  Machine learning",
    "section": "8.6 Non-negative matrix factorisation",
    "text": "8.6 Non-negative matrix factorisation\nNon-negative matrix factorisation is an unsupervised machine learning method, which seeks to explain the observed clinical features using smaller number of basis components (hidden variables). A matrix V of dimension m x n is factorise to 2 matrices W and H. W has dimensions m x k an H has dimensions n x k. For topic modeling in the chapter of text mining, V matrix is the document term matrix. Each row of H is the word embedding and the columns pf W represent the weight.\nThe interpretation of NMF components is similar to, but often more natural than, related methods such as factor analysis and principal component analysis. The non-negativity constraint in NMF leads to a simple “parts-based” interpretation and has been successfully used in facial recognition, metagene pattern discovery, and market research. For a clinical example, the matrix for NMF decomposition consists of rows of hospitals and their service availability.\nThe example below used the recommended procedure to estimate the factorization rank, based on stability of the cophenetic correlation coefficient and the residual error, prior to performing the NMF analysis. The data were permuted and the factorization rank computed. These data were used as reference for selecting factorization rank to minimize the chance of overfitting.\n\n#BiocManager::install(\"Biobase\")\nlibrary(NMF,quietly = TRUE)\n\nRegistered S3 methods overwritten by 'registry':\n  method               from \n  print.registry_field proxy\n  print.registry_entry proxy\n\n\nNMF - BioConductor layer [OK] | Shared memory capabilities [NO: windows] | Cores 2/2\n\n\n\nAttaching package: 'NMF'\n\n\nThe following object is masked from 'package:nlme':\n\n    coef&lt;-\n\n\nThe following object is masked from 'package:party':\n\n    fit\n\n\nThe following object is masked from 'package:modeltools':\n\n    fit\n\nlibrary(tidyverse)\nedge&lt;- read.csv(\"./Data-Use/Hosp_Network_geocoded.csv\")\ndf&lt;-edge[,c(2:dim(edge)[2])]\nrow.names(df)&lt;-edge[,1] #bipartite matrix\n#select columns#remove distance data\ndf_se&lt;-edge[,c(2:16)]\nrow.names(df_se)&lt;-edge[,1] #bipartite matrix\n#south eastern hospitals\n#select rows\ndf_se&lt;-df_se[c(1,6,7,11,12,13,14,17,19,20,24,31,33,34,35),]\n\n\n#estimate factorisation rank-prevent overfitting\nestim.r &lt;- nmf(df_se, 2:6, nrun = 10, seed = 123456)\nplot(estim.r)\n\n\n\n\n\n\n\nconsensusmap(estim.r)\n\n\n\n\n\n\n\n\nThe optimal number of rank for this data is likely to be 4.\n\n#Using the data above we can use which argument to find the order\n#since the starting point is 2 we just need to add 1\nRank=which(estim.r$measures$cophenetic==max(estim.r$measures$cophenetic))+1\n\nmodel&lt;-nmf(df_se, Rank,nrun=100)\npmodel&lt;-predict(model,prob=TRUE)\ncoefmap(model)\n\n\n\n\n\n\n\nbasismap(model)\n\n\n\n\n\n\n\nconsensusmap(model)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#formal-concept-analysis",
    "href": "machine-learning.html#formal-concept-analysis",
    "title": "8  Machine learning",
    "section": "8.7 Formal concept analysis",
    "text": "8.7 Formal concept analysis\nThis is an unsupervised machine learning method which takes an input matrix of objects and attributes (binary values) and seeks to find the hierarchy of relations. Each concept shares a set of attributes with other objects and each sub-concept shares a smaller set of attributes with a subset of the objects. A Hasse diagram is used to display the hierarchy of relations.\nFirst we will illustrate with a simple relationship among fruit. Note in this example there is no close set for apple and pear, as both share the attribute of green color. There is a close set for the tropical fruit mango and and banana.\nThere are several libraries for FCA. Here we will use multiplex. The fcaR library can also handle fuzzy data.\n\n#BiocManager::install(\"Rgraphviz\")\n\nlibrary(multiplex) #Algebraic Tools for the Analysis of Multiple Social Networks\n\n\nAttaching package: 'multiplex'\n\n\nThe following object is masked from 'package:mlr3tuning':\n\n    ti\n\n\nThe following objects are masked from 'package:mlr3verse':\n\n    flt, ti\n\nlibrary(Rgraphviz) #plot hasse diagram\n\nLoading required package: graph\n\n\n\nAttaching package: 'graph'\n\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\n\nThe following object is masked from 'package:party':\n\n    nodes\n\n\nThe following object is masked from 'package:strucchange':\n\n    boundary\n\n\n\nAttaching package: 'Rgraphviz'\n\n\nThe following object is masked from 'package:NMF':\n\n    name\n\nfr&lt;-data.frame(Fruit=c(\"Apple\", \"Banana\",\"Pear\", \"Mango\"),\n                 round=c(1,0,0,0),\n                 cylindrical=c(0,1,0,0),\n                 yellow=c(0,1,0,1),\n                 red=c(1,0,1,1),\n                 green=c(1,0,1,0), #color when ripe\n                 tropical=c(0,1,0,1),\n               large_seed=c(0,0,0,1)\n)\n\ndf&lt;-fr[,c(2:dim(fr)[2])]\nrow.names(df)&lt;-fr[,1] #bipartite matrix\n\n#perform Galois derivations between partially ordered subsets\n#galois(df_se',labeling = \"full\")\ngf &lt;- galois(df, labeling = \"reduced\")\n\n#partial ordering of concept\npo&lt;-partial.order(gf,type=\"galois\")\ndiagram(po, main=\"Hasse diagram of partial order - Fruit\") \n\n\n\n\n\n\n\n#lattice  diagram with reduced context\ndiagram.levels(po)\n\n$`4`\n[1] \"{round} {Apple}\"        \"{cylindrical} {Banana}\" \"{large_seed} {Mango}\"  \n\n$`3`\n[1] \"{tropical, yellow} {}\" \"{green} {Pear}\"       \n\n$`2`\n[1] \"{red} {}\"\n\n$`5`\n[1] \"7\"\n\n$`1`\n[1] \"8\"\n\n\nNext we illustrate FCA in network of hospitals in South-Eastern Melbourne. The objects are the hospitals and the attributes are the services available in those hospitals.\n\n#library(multiplex) #Algebraic Tools for the Analysis of Multiple Social Networks\n#library(Rgraphviz) #plot hasse diagram\n\n#install BiocManager::install(\"Rgraphviz\")\n\nedge&lt;- read.csv(\"./Data-Use/Hosp_Network_geocoded.csv\")\ndf&lt;-edge[,c(2:dim(edge)[2])]\nrow.names(df)&lt;-edge[,1] #bipartite matrix\n\n#select columns#remove distance data\ndf_se&lt;-edge[,c(2:16)]\nrow.names(df_se)&lt;-edge[,1] #bipartite matrix\n\n#south eastern hospitals\n#select rows\ndf_se&lt;-df_se[c(1,6,7,11,12,13,14,17,19,20,24,31,33,34,35),]\n\n#perform Galois derivations between partially ordered subsets\n#galois(df_se',labeling = \"full\")\ngf &lt;- galois(df_se, labeling = \"reduced\")\n#partial ordering of concept\npo&lt;-partial.order(gf,type=\"galois\")\ndiagram(po, main=\"Hasse diagram of partial order with reduced context\") \n\n\n\n\n\n\n\n#lattice  diagram with reduced context\ndiagram.levels(po)\n\n$`9`\n[1] \"{designated, ECR} {mmc}\"\n\n$`4`\n[1] \"{link_rmh} {}\" \"18\"            \"25\"            \"27\"           \n[5] \"28\"            \"31\"           \n\n$`1`\n[1] \"{CT, link_mmc} {}\"\n\n$`3`\n[1] \"{X99min_rmh} {dandenongvalley, knoxprivate, seprivate}\"\n[2] \"{TPA} {}\"                                              \n[3] \"{CTA} {}\"                                              \n[4] \"{stroke_unit} {}\"                                      \n[5] \"23\"                                                    \n\n$`2`\n[1] \"{X99min_mmc} {}\" \"{public} {}\"    \n\n$`7`\n[1] \"{CTP} {ddh}\"    \"{} {frankston}\" \"{} {latrobe}\"  \n\n$`6`\n[1] \"{MRI} {}\"        \"{} {marroondah}\" \"19\"              \"{} {casey}\"     \n[5] \"{} {bairnsdale}\" \"{} {sale}\"      \n\n$`5`\n[1] \"{VST} {}\"     \"{} {hampton}\" \"22\"           \"24\"           \"30\"          \n\n$`8`\n[1] \"{neurosx} {cabrini}\" \"{} {bhh}\"            \"{} {warragul}\"      \n\n$`10`\n[1] \"14\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#evolutionary-algorithm",
    "href": "machine-learning.html#evolutionary-algorithm",
    "title": "8  Machine learning",
    "section": "8.8 Evolutionary Algorithm",
    "text": "8.8 Evolutionary Algorithm\nEvolutionary algorithm are search method which take the source of inspiration from nature such as evolution and survival of the fittest. These are seen as heuristic based method. The results from evolutionary algorithm shouldn’t be compared unless all conditions set are the same. In essence the findings are similar under the same conditions.\n\n8.8.1 Simulated Annealing\nThis method uses idea in metallurgy whereby metal is heated and then cooled to alter its property.\n\n#SA section is set not to run as the analysis takes a long time.\n# a saved run is provided below\n\ndata(\"BreastCancer\",package = \"mlbench\")\ncolnames(BreastCancer)\n\n#check for duplicates\nsum(duplicated(BreastCancer))\n\n#remove duplicates\n#keep Id to avoid creation of new duplicates\nBreastCancer1&lt;-unique(BreastCancer) #reduce 699 to 691 rows\n\n#convert multiple columns to numeric\n#lapply output a list\nBreastCancer2&lt;-lapply(BreastCancer1[,-c(7,11)], as.numeric) #list\nBreastCancer2&lt;-as.data.frame(BreastCancer2)\nBreastCancer2$Class&lt;-BreastCancer1$Class\n\nx=BreastCancer2[,-10]\ny=BreastCancer2$Class\n\n\nsa_ctrl &lt;- safsControl(functions = rfSA,\n                       method = \"repeatedcv\",\n                       repeats = 3, #default is 5\n                       improve = 50)\n\nset.seed(10)\nglm_sa &lt;- safs(x = x, y = y,\n              iters = 5, #default is 250\n              safsControl = sa_ctrl, method=\"glm\")\n\n#save(glm_sa,file=\"Logistic_SimulatedAnnealing.Rda\")\n\n#############################################\n#\n#Simulated Annealing Feature Selection\n#\n#691 samples\n#9 predictors\n#2 classes: 'benign', 'malignant' \n#\n#Maximum search iterations: 5 \n#Restart after 50 iterations without improvement (0 restarts on average)\n#\n#Internal performance values: Accuracy, Kappa\n#Subset selection driven to maximize internal Accuracy \n#\n#External performance values: Accuracy, Kappa\n#Best iteration chose by maximizing external Accuracy \n#External resampling method: Cross-Validated (10 fold, repeated 3 times) \n\n#During resampling:\n#  * the top 5 selected variables (out of a possible 9):\n#    Bl.cromatin (56.7%), Id (46.7%), Cl.thickness (43.3%), Epith.c.size (43.3%), #Marg.adhesion (43.3%)\n#  * on average, 3.5 variables were selected (min = 2, max = 5)\n#\n#In the final search using the entire training set:\n#   * 2 features selected at iteration 5 including:\n#     Cl.thickness, Cell.size  \n#   * external performance at this iteration is\n#\n#   Accuracy       Kappa \n#     0.9314      0.8479 \n\n\nload(\"./Logistic_SimulatedAnnealing.Rda\")\n\n#plot output of simulated annealing\nplot(glm_sa)\n\n\n\n\n\n\n\n\n\n\n8.8.2 Genetic Algorithm\nGenetic algorithm is a machine learning tool based on ideas from Darwin’s concept of natural selection. It is based on mutation, crossover and selection. Genetic algorithm can be used in any situation. The issue is in finding the fitness function to evaluate the output. Since it does not depend on gradient descent algorithm, it is less likely to be stuck in local minima compared to other machine learning methods. Genetic algorithm is available in R as part of caret and GA libraries. Genetic algorithm can be used to optimise feature selection for regression modelling at the expense of much longer running time.\nOne potential issue with using cross-validation in genetic algorithm for feature selection is that it would be not right to use it again when feeding this data into another machine learning method.\n\n#GA\nlibrary(caret)\n\ndata(\"BreastCancer\",package = \"mlbench\")\ncolnames(BreastCancer)\n\n#check for duplicates\nsum(duplicated(BreastCancer))\n\n#remove duplicates\n#keep Id to avoid creation of new duplicates\nBreastCancer1&lt;-unique(BreastCancer) #reduce 699 to 691 rows\n\n#convert multiple columns to numeric\n#lapply output a list\nBreastCancer2&lt;-lapply(BreastCancer1[,-c(7,11)], as.numeric) #list\nBreastCancer2&lt;-as.data.frame(BreastCancer2)\nBreastCancer2$Class&lt;-BreastCancer1$Class\n\n\n#check for NA\nanyNA(BreastCancer2)\n\nsplit = caTools::sample.split(BreastCancer2$Class, SplitRatio = 0.7)\nTrain = subset(BreastCancer2, split == TRUE)\nTest = subset(BreastCancer2, split == FALSE)\n\nx=Train[,-10]\ny=Train$Class\n\n#cross validation indicates the number of cycle of the procedure from randomly generating new population of chromosome to mutate child chromosome.\n\nga_ctrl &lt;- gafsControl(functions = rfGA,\n                       method = \"cv\",\n                       repeats = 3, # default is 10\n                       genParallel=TRUE, # Use parallel programming\n                       allowParallel = TRUE\n                       )\n\n## Use the same random number seed as the RFE process\n## so that the same CV folds are used for the external\n## resampling. \n\nset.seed(10)\nsystem.time(glm_ga &lt;- gafs(x = x, y = y,\n              iters = 5, #recommended is 200\n              gafsControl = ga_ctrl, method=\"glm\"))\n\n#save(glm_ga,file=\"Logistic_GeneticAlgorithm.Rda\")\n\n################################################################\n# The output of glm_ga\n#Genetic Algorithm Feature Selection\n\n#484 samples\n#9 predictors\n#2 classes: 'benign', 'malignant' \n\n#Maximum generations: 5 \n#Population per generation: 50 \n#Crossover probability: 0.8 \n#Mutation probability: 0.1 \n#Elitism: 0 \n#\n#Internal performance values: Accuracy, Kappa\n#Subset selection driven to maximize internal Accuracy \n#\n#External performance values: Accuracy, Kappa\n#Best iteration chose by maximizing external Accuracy \n#External resampling method: Cross-Validated (10 fold) \n#\n#During resampling:\n#  * the top 5 selected variables (out of a possible 9):\n#    Cell.shape (100%), Cl.thickness (100%), Epith.c.size (100%), \nNormal.nucleoli #(100%), Id (90%)\n#  * on average, 6.7 variables were selected (min = 5, max = 8)\n#\n#In the final search using the entire training set:\n#   * 7 features selected at iteration 2 including:\n#     Cl.thickness, Cell.shape, Marg.adhesion, Epith.c.size, Bl.cromatin ... \n#   * external performance at this iteration is\n#\n#   Accuracy       Kappa \n#     0.9691      0.9328 \n#\n\nThe output from the Genetic Algorithm is plotted as mean fitness by generations. This plot shows the internal and external accuracy estimate from cross validation.\n\nload(\"./Logistic_GeneticAlgorithm.Rda\")\n#plot output of genetic algorithm \nplot(glm_ga)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#manifold-learning",
    "href": "machine-learning.html#manifold-learning",
    "title": "8  Machine learning",
    "section": "8.9 Manifold learning",
    "text": "8.9 Manifold learning\nManifold learning has been described as using geometry information in high dimensional space to map data into cluster in lower dimensional space. This is a non-linear reduction technique. Several manifold learning methods are described below but this list is not exhaustive. It is available through maniTools package.\n\n8.9.1 T-Stochastic Neighbourhood Embedding\nT-Stochastic Neighbourhood Embedding (TSNE) is a manifold learning method which seeks to transform the complex data into low (2) dimensions while maintaining the distance between neighbouring objects. The distance between data points are can be measured using Euclidean distance or other measures of distance. The transformed data points are conditional probabilities that represents similarities. The original description of TSNE used PCA as a first step to speed up computation and reduce noise.\nThis method is listed here as it is a form of data reduction method. This non-linear method is different from PCA in that the low dimensional output of TSNE are not intended for machine learning. TSNE is implemented in R as Rtsne. The perplexity parameter allows tuning of the proximity of the data points. The PCA step can be performed within Rtsne by setting the pca argument. The default number of iterations or max_iter is 1000.\n\nlibrary(Rtsne)\nlibrary(ggplot2)\nlibrary(mice) #impute missing data\n\n\nAttaching package: 'mice'\n\n\nThe following objects are masked from 'package:BiocGenerics':\n\n    cbind, rbind\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\ndata(\"BreastCancer\",package = \"mlbench\")\ncolnames(BreastCancer)\n\n [1] \"Id\"              \"Cl.thickness\"    \"Cell.size\"       \"Cell.shape\"     \n [5] \"Marg.adhesion\"   \"Epith.c.size\"    \"Bare.nuclei\"     \"Bl.cromatin\"    \n [9] \"Normal.nucleoli\" \"Mitoses\"         \"Class\"          \n\n#check for duplicates\nsum(duplicated(BreastCancer))\n\n[1] 8\n\n#remove duplicates\n#keep Id to avoid creation of new duplicates\nBreastCancer1&lt;-unique(BreastCancer) #reduce 699 to 691 rows\n\n#impute missing data\n#m is number of multiple imputation, default is 5\n#output is a list\nimputed_Data &lt;- mice(BreastCancer1, m=5, maxit = 5, method = 'pmm', seed = 500)\n\n\n iter imp variable\n  1   1  Bare.nuclei\n  1   2  Bare.nuclei\n  1   3  Bare.nuclei\n  1   4  Bare.nuclei\n  1   5  Bare.nuclei\n  2   1  Bare.nuclei\n  2   2  Bare.nuclei\n  2   3  Bare.nuclei\n  2   4  Bare.nuclei\n  2   5  Bare.nuclei\n  3   1  Bare.nuclei\n  3   2  Bare.nuclei\n  3   3  Bare.nuclei\n  3   4  Bare.nuclei\n  3   5  Bare.nuclei\n  4   1  Bare.nuclei\n  4   2  Bare.nuclei\n  4   3  Bare.nuclei\n  4   4  Bare.nuclei\n  4   5  Bare.nuclei\n  5   1  Bare.nuclei\n  5   2  Bare.nuclei\n  5   3  Bare.nuclei\n  5   4  Bare.nuclei\n  5   5  Bare.nuclei\n\n#choose among the 5 imputed dataset\ncompleteData &lt;- complete(imputed_Data,2)\n\n#convert multiple columns to numeric\n#lapply output a list\nBreastCancer2&lt;-lapply(completeData[,-c(11)], as.numeric) #list\nBreastCancer2&lt;-as.data.frame(BreastCancer2)\nBreastCancer2$Class&lt;-BreastCancer1$Class\n\nBC_unique &lt;- unique(BreastCancer2) # Remove duplicates\nset.seed(42) # Sets seed for reproducibility\ntsne_out &lt;- Rtsne(as.matrix(BC_unique[,-11]), \n        normalize = T, #normalise data\n        pca=T, dims = 3, #pca before analysis\n        perplexity=20, #tuning\n        verbose=FALSE) # Run TSNE\n#plot(tsne_out$Y,col=BC_unique$Class,asp=1)\n\n# Add a new column with color\nmycolors &lt;- c('red', 'blue')\nBC_unique$color &lt;- mycolors[ as.numeric(BC_unique$Class) ]\n\n#turn off rgl\n#rgl::plot3d(x=tsne_out$Y[,1], y=tsne_out$Y[,2], z=tsne_out$Y[,3], type = 'p', col=BC_unique$color, size=8)\n#rgl::legend3d(\"topright\", legend = names(mycolors), pch = 16, col = colors, cex=1, inset=c(0.02))\n\nThe example with Breast cancer didn’t turn out as well. Let’s try TSNE with the iris dataset.\n\n#TSNE\n\ndata(iris)\n#5 columns\n\nIris_unique &lt;- unique(iris) # Remove duplicates\nset.seed(42) # Sets seed for reproducibility\ntsne_out &lt;- Rtsne(as.matrix(Iris_unique[,-5]), dims = 2, perplexity=10, verbose=FALSE) # Run TSNE\nplot(tsne_out$Y,col=Iris_unique$Species,asp=1)\n\n\n\n\n\n\n\n\n\n\n8.9.2 Self organising map\nSelf organising map is an unsupervised machine learning method and is excellent for viewing complex data in low dimensional space i.e. a data reduction method. SOM is available as part of kohonen library. It uses competitive learning to adjust its weight in contrast to other neural network approaches which use backward propagation or gradient descent to update the weight of the features. Each node is evaluated to participate in the neural network. Input vectors that are close to each other in high dimensional space are mapped to be close to each other in low dimensional space. SOM is a competeitive neural network and has been considered as a deep learning method.\nThe codes below are modified from https://rpubs.com/AlgoritmaAcademy/som for use in analysis of iris data. The first illustration is with unsupervised SOM.\n\nlibrary(kohonen)\n\n\nAttaching package: 'kohonen'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n#unsupervised SOM\n#use iris dataset 150 x 5\n\nset.seed(100)\n\n#convert to numeric matrix\niris.train &lt;- as.matrix(scale(iris[,-5]))\n\n# grid should be smaller than dim(iris) 150 x5\n#xdim =10 and ydim=10 would be &lt; 120\niris.grid &lt;- somgrid(xdim = 10, ydim = 10, topo = \"hexagonal\")\n\n#som model\niris.model &lt;- som(iris.train, iris.grid, rlen = 500, radius = 2.5, keep.data = TRUE, dist.fcts = \"euclidean\")\n\nplot(iris.model, type = \"mapping\", pchs = 19, shape = \"round\")\n\n\n\n\n\n\n\n\nPlot\n\nplot(iris.model, type = \"codes\", main = \"Codes Plot\", palette.name = rainbow)\n\n\n\n\n\n\n\n\nThe plot of training shows that the distance between nodes reached a plateau after 300 iterations.\n\nplot(iris.model, type = \"changes\")\n\n\n\n\n\n\n\n\nSupervised SOM is now performed with the same iris data.\n\n#SOM\n\nset.seed(100)\nint &lt;- sample(nrow(iris), nrow(iris)*0.8)\ntrain &lt;- iris[int,]\ntest &lt;- iris[-int,]\n\n# scaling data\ntrainX &lt;- scale(train[,-5])\ntestX &lt;- scale(test[,-5], center = attr(trainX, \"scaled:center\"))\n\n# make label\n#iris$species is already of class factor\n\ntrain.label &lt;- train[,5]\ntest.label &lt;- test[,5]\ntest[,5] &lt;- 916\ntestXY &lt;- list(independent = testX, dependent = test.label)\n\n# make a train data sets that scaled\n# convert them to be a numeric matrix \niris.train &lt;- as.matrix(scale(train[,-5]))\n\nset.seed(100)\n\n# grid should be smaller than dim(train) 120 x5\n#xdim =10 and ydim=10 would be &lt; 120\niris.grid &lt;- somgrid(xdim = 10, ydim = 10, topo = \"hexagonal\")\n\n#som model\niris.model &lt;- som(iris.train, iris.grid, rlen = 500, radius = 2.5, keep.data = TRUE, dist.fcts = \"euclidean\")\n\nclass &lt;- xyf(trainX, classvec2classmat(train.label), iris.grid, rlen = 500)\n\nplot(class, type = \"changes\")\n\n\n\n\n\n\n\npred &lt;- predict(class, newdata = testXY)\ntable(Predict = pred$predictions[[2]], Actual = test.label)\n\n            Actual\nPredict      setosa versicolor virginica\n  setosa          9          0         0\n  versicolor      0          8         0\n  virginica       0          0         7\n\n\nDetermine number of clusters.\n\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_nbclust(iris.model$codes[[1]], kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nPlot\n\nset.seed(100)\nclust &lt;- kmeans(iris.model$codes[[1]], 6)\nplot(iris.model, type = \"codes\", bgcol = rainbow(9)[clust$cluster], \n     main = \"Cluster SOM\")\nadd.cluster.boundaries(iris.model, clust$cluster)\n\n\n\n\n\n\n\n\n\n\n8.9.3 Multidimensional scaling\nMDS is a method of dimensionality reduction which preserves the distance between variables. This method has been used in geography. It is implemented in IsoplotR package and igraph package as layout.mds.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#deep-learning",
    "href": "machine-learning.html#deep-learning",
    "title": "8  Machine learning",
    "section": "8.10 Deep learning",
    "text": "8.10 Deep learning\nDeep learning is a neural network with many layers: inner, multiple hidden and outer layer. Deep learning methods can be supervised or unsupervised. It uses gradient descent algorithm in search for the solution. One potential issue that it may be stuck in a local minima rather than the global minima.\nThere are several R libraries for performing deep learning. It’s worth checking out the installation requirement as some require installing the library in python and uses the reticulate library to perform analysis. The examples used here are R libraries including RSNNS. The instructions for installing Miniconda from reticulate was provided in the earlier chapter on [data wrangling][Python Minconda environment]. Those instruction include installing torch and kerras.\nFor tabular data, deep learning may not necessarily be better than tree-based machine learning method (Grinsztajn, Oyallon, and Varoquaux 2022) . By contrast, deep learning may be better for unstructured data such as imaging or text. The reasons for this can be due to tabular data having a mixed (continuous and categorical) data structure, sparsity and lack of locality present from pattern in imaging or audio or text. Missing data is poorly handled by deep learning. Newer approach to deep learning such as transformer for tabular data uses approaches such as entity embedding to examine categorical data in vector space after one hot encoding; attentive mechanism to sequentially focus on features. In situations where the data is over 3000 data points transformer method such as TabPFN perform well against tree-based methods (McElfresh et al. 2023)\n\n8.10.0.1 Multiplayer Perceptron\nMultilayer perceptron is a type of deep learning. . It passes information in one direction from inner to hidden and outer layer and hence is referred to as feed forward artificial neural network. It trains the data using a loss function which adapt to the parameter and optimise according to the specified learning rate. Overfitting is minimised by using an L2 regularisation penalty, termed alpha.\n\nlibrary(caret)\nlibrary(RSNNS)\n\nLoading required package: Rcpp\n\n\n\nAttaching package: 'RSNNS'\n\n\nThe following object is masked from 'package:kohonen':\n\n    som\n\n\nThe following objects are masked from 'package:caret':\n\n    confusionMatrix, train\n\ndata(\"BreastCancer\",package = \"mlbench\")\ncolnames(BreastCancer)\n\n [1] \"Id\"              \"Cl.thickness\"    \"Cell.size\"       \"Cell.shape\"     \n [5] \"Marg.adhesion\"   \"Epith.c.size\"    \"Bare.nuclei\"     \"Bl.cromatin\"    \n [9] \"Normal.nucleoli\" \"Mitoses\"         \"Class\"          \n\n#remove ID column\n#remove column a=with NA \n#alternative is to impute\nBreastCancer&lt;-BreastCancer[,-c(1,7)]#remaining 9 columns\n\n#convert multiple columns to numeric\n#lapply output a list\nBreastCancer2&lt;-lapply(BreastCancer[,-c(9)], as.numeric)\nBreastCancer2&lt;-as.data.frame(BreastCancer2)\nBreastCancer2&lt;-merge(BreastCancer2, BreastCancer$Class)\n\n#note Class is benign or malignant of class factor\n#column Bare.nuclei removed due to NA\n\n#split data\nset.seed(123)\n\nBreastCancer2Values &lt;- BreastCancer2[,c(1:8)]\nBreastCancer2Targets &lt;- decodeClassLabels(BreastCancer2[,9])\n\n#this returns the orginal file as a list\nBreastCancer2 &lt;- splitForTrainingAndTest(BreastCancer2Values, BreastCancer2Targets, ratio=0.15) #ratio is percentage for test data\nBreastCancer2 &lt;- normTrainingAndTestSet(BreastCancer2) #put out a list object\n\nmodel &lt;- mlp(BreastCancer2$inputsTrain, \n             BreastCancer2$targetsTrain, \n             size=5, #number of unit in hidden layer \n             learnFuncParams=c(0.1), \n              maxit=50, #number of iteration to learn\n             inputsTest=BreastCancer2$inputsTest, targetsTest=BreastCancer2$targetsTest)\n\nsummary(model)\n\nSNNS network definition file V1.4-3D\ngenerated at Thu Dec 12 13:25:41 2024\n\nnetwork name : RSNNS_untitled\nsource files :\nno. of units : 15\nno. of connections : 50\nno. of unit types : 0\nno. of site types : 0\n\n\nlearning function : Std_Backpropagation\nupdate function   : Topological_Order\n\n\nunit default section :\n\nact      | bias     | st | subnet | layer | act func     | out func\n---------|----------|----|--------|-------|--------------|-------------\n 0.00000 |  0.00000 | i  |      0 |     1 | Act_Logistic | Out_Identity \n---------|----------|----|--------|-------|--------------|-------------\n\n\nunit definition section :\n\nno. | typeName | unitName         | act      | bias     | st | position | act func     | out func | sites\n----|----------|------------------|----------|----------|----|----------|--------------|----------|-------\n  1 |          | Input_1          | -0.14849 | -0.12745 | i  | 1,0,0    | Act_Identity |          | \n  2 |          | Input_2          |  1.59566 |  0.17298 | i  | 2,0,0    | Act_Identity |          | \n  3 |          | Input_3          |  1.61379 | -0.05461 | i  | 3,0,0    | Act_Identity |          | \n  4 |          | Input_4          |  0.76866 |  0.22981 | i  | 4,0,0    | Act_Identity |          | \n  5 |          | Input_5          |  0.35424 |  0.26428 | i  | 5,0,0    | Act_Identity |          | \n  6 |          | Input_6          |  2.69324 | -0.27267 | i  | 6,0,0    | Act_Identity |          | \n  7 |          | Input_7          |  0.37127 |  0.01686 | i  | 7,0,0    | Act_Identity |          | \n  8 |          | Input_8          | -0.35178 |  0.23545 | i  | 8,0,0    | Act_Identity |          | \n  9 |          | Hidden_2_1       |  0.01505 | -4.19609 | h  | 1,2,0    |||\n 10 |          | Hidden_2_2       |  0.01511 | -4.18364 | h  | 2,2,0    |||\n 11 |          | Hidden_2_3       |  0.01446 | -4.17636 | h  | 3,2,0    |||\n 12 |          | Hidden_2_4       |  0.01596 | -4.17513 | h  | 4,2,0    |||\n 13 |          | Hidden_2_5       |  0.01394 | -4.20955 | h  | 5,2,0    |||\n 14 |          | Output_benign    |  0.71619 |  0.92553 | o  | 1,4,0    |||\n 15 |          | Output_malignant |  0.28380 | -0.92463 | o  | 2,4,0    |||\n----|----------|------------------|----------|----------|----|----------|--------------|----------|-------\n\n\nconnection definition section :\n\ntarget | site | source:weight\n-------|------|---------------------------------------------------------------------------------------------------------------------\n     9 |      |  8:-0.00576,  7: 0.00727,  6:-0.00682,  5:-0.01802,  4: 0.01969,  3:-0.08011,  2: 0.09269,  1:-0.00830\n    10 |      |  8:-0.00727,  7: 0.02291,  6:-0.00065,  5:-0.01455,  4: 0.01343,  3: 0.07341,  2:-0.07951,  1:-0.00231\n    11 |      |  8:-0.02047,  7: 0.00020,  6:-0.03410,  5: 0.01364,  4: 0.05111,  3:-0.04571,  2: 0.04224,  1:-0.00804\n    12 |      |  8: 0.00616,  7: 0.00433,  6: 0.01931,  5:-0.03026,  4: 0.00085,  3:-0.02034,  2: 0.02802,  1:-0.00241\n    13 |      |  8:-0.01292,  7: 0.00408,  6: 0.00104,  5: 0.00315,  4: 0.04242,  3:-0.07730,  2: 0.02324,  1: 0.03049\n    14 |      | 13: 0.02422, 12:-0.07496, 11:-0.05541, 10: 0.00556,  9: 0.11299\n    15 |      | 13: 0.00929, 12:-0.09087, 11:-0.07272, 10:-0.00928,  9: 0.09646\n-------|------|---------------------------------------------------------------------------------------------------------------------\n\nweightMatrix(model)\n\n                 Input_1 Input_2 Input_3 Input_4 Input_5 Input_6 Input_7\nInput_1                0       0       0       0       0       0       0\nInput_2                0       0       0       0       0       0       0\nInput_3                0       0       0       0       0       0       0\nInput_4                0       0       0       0       0       0       0\nInput_5                0       0       0       0       0       0       0\nInput_6                0       0       0       0       0       0       0\nInput_7                0       0       0       0       0       0       0\nInput_8                0       0       0       0       0       0       0\nHidden_2_1             0       0       0       0       0       0       0\nHidden_2_2             0       0       0       0       0       0       0\nHidden_2_3             0       0       0       0       0       0       0\nHidden_2_4             0       0       0       0       0       0       0\nHidden_2_5             0       0       0       0       0       0       0\nOutput_benign          0       0       0       0       0       0       0\nOutput_malignant       0       0       0       0       0       0       0\n                 Input_8   Hidden_2_1    Hidden_2_2    Hidden_2_3    Hidden_2_4\nInput_1                0 -0.008297254 -0.0023068141 -0.0080354111 -0.0024115480\nInput_2                0  0.092688218 -0.0795127451  0.0422427431  0.0280199945\nInput_3                0 -0.080105409  0.0734057277 -0.0457132570 -0.0203414690\nInput_4                0  0.019694204  0.0134255355  0.0511073507  0.0008500156\nInput_5                0 -0.018022288 -0.0145457229  0.0136382505 -0.0302589945\nInput_6                0 -0.006821295 -0.0006512092 -0.0341025740  0.0193094723\nInput_7                0  0.007272924  0.0229056627  0.0001989322  0.0043286891\nInput_8                0 -0.005761526 -0.0072661606 -0.0204701889  0.0061648567\nHidden_2_1             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nHidden_2_2             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nHidden_2_3             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nHidden_2_4             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nHidden_2_5             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nOutput_benign          0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nOutput_malignant       0  0.000000000  0.0000000000  0.0000000000  0.0000000000\n                   Hidden_2_5 Output_benign Output_malignant\nInput_1           0.030488508   0.000000000      0.000000000\nInput_2           0.023240086   0.000000000      0.000000000\nInput_3          -0.077304244   0.000000000      0.000000000\nInput_4           0.042423774   0.000000000      0.000000000\nInput_5           0.003147935   0.000000000      0.000000000\nInput_6           0.001040343   0.000000000      0.000000000\nInput_7           0.004078272   0.000000000      0.000000000\nInput_8          -0.012916351   0.000000000      0.000000000\nHidden_2_1        0.000000000   0.112992622      0.096456863\nHidden_2_2        0.000000000   0.005558367     -0.009282685\nHidden_2_3        0.000000000  -0.055406742     -0.072724856\nHidden_2_4        0.000000000  -0.074957542     -0.090871565\nHidden_2_5        0.000000000   0.024224803      0.009294559\nOutput_benign     0.000000000   0.000000000      0.000000000\nOutput_malignant  0.000000000   0.000000000      0.000000000\n\nextractNetInfo(model)\n\n$infoHeader\n                name               value\n1       no. of units                  15\n2 no. of connections                  50\n3  no. of unit types                   0\n4  no. of site types                   0\n5  learning function Std_Backpropagation\n6    update function   Topological_Order\n\n$unitDefinitions\n   unitNo         unitName     unitAct    unitBias        type posX posY posZ\n1       1          Input_1 -0.14849401 -0.12745351  UNIT_INPUT    1    0    0\n2       2          Input_2  1.59566152  0.17298311  UNIT_INPUT    2    0    0\n3       3          Input_3  1.61378837 -0.05461386  UNIT_INPUT    3    0    0\n4       4          Input_4  0.76865852  0.22981048  UNIT_INPUT    4    0    0\n5       5          Input_5  0.35424057  0.26428038  UNIT_INPUT    5    0    0\n6       6          Input_6  2.69324446 -0.27266610  UNIT_INPUT    6    0    0\n7       7          Input_7  0.37127367  0.01686329  UNIT_INPUT    7    0    0\n8       8          Input_8 -0.35178334  0.23545146  UNIT_INPUT    8    0    0\n9       9       Hidden_2_1  0.01505136 -4.19608831 UNIT_HIDDEN    1    2    0\n10     10       Hidden_2_2  0.01510898 -4.18364382 UNIT_HIDDEN    2    2    0\n11     11       Hidden_2_3  0.01445734 -4.17636061 UNIT_HIDDEN    3    2    0\n12     12       Hidden_2_4  0.01596117 -4.17512655 UNIT_HIDDEN    4    2    0\n13     13       Hidden_2_5  0.01393711 -4.20955276 UNIT_HIDDEN    5    2    0\n14     14    Output_benign  0.71619302  0.92553055 UNIT_OUTPUT    1    4    0\n15     15 Output_malignant  0.28380045 -0.92462683 UNIT_OUTPUT    2    4    0\n        actFunc      outFunc sites\n1  Act_Identity Out_Identity      \n2  Act_Identity Out_Identity      \n3  Act_Identity Out_Identity      \n4  Act_Identity Out_Identity      \n5  Act_Identity Out_Identity      \n6  Act_Identity Out_Identity      \n7  Act_Identity Out_Identity      \n8  Act_Identity Out_Identity      \n9  Act_Logistic Out_Identity      \n10 Act_Logistic Out_Identity      \n11 Act_Logistic Out_Identity      \n12 Act_Logistic Out_Identity      \n13 Act_Logistic Out_Identity      \n14 Act_Logistic Out_Identity      \n15 Act_Logistic Out_Identity      \n\n$fullWeightMatrix\n                 Input_1 Input_2 Input_3 Input_4 Input_5 Input_6 Input_7\nInput_1                0       0       0       0       0       0       0\nInput_2                0       0       0       0       0       0       0\nInput_3                0       0       0       0       0       0       0\nInput_4                0       0       0       0       0       0       0\nInput_5                0       0       0       0       0       0       0\nInput_6                0       0       0       0       0       0       0\nInput_7                0       0       0       0       0       0       0\nInput_8                0       0       0       0       0       0       0\nHidden_2_1             0       0       0       0       0       0       0\nHidden_2_2             0       0       0       0       0       0       0\nHidden_2_3             0       0       0       0       0       0       0\nHidden_2_4             0       0       0       0       0       0       0\nHidden_2_5             0       0       0       0       0       0       0\nOutput_benign          0       0       0       0       0       0       0\nOutput_malignant       0       0       0       0       0       0       0\n                 Input_8   Hidden_2_1    Hidden_2_2    Hidden_2_3    Hidden_2_4\nInput_1                0 -0.008297254 -0.0023068141 -0.0080354111 -0.0024115480\nInput_2                0  0.092688218 -0.0795127451  0.0422427431  0.0280199945\nInput_3                0 -0.080105409  0.0734057277 -0.0457132570 -0.0203414690\nInput_4                0  0.019694204  0.0134255355  0.0511073507  0.0008500156\nInput_5                0 -0.018022288 -0.0145457229  0.0136382505 -0.0302589945\nInput_6                0 -0.006821295 -0.0006512092 -0.0341025740  0.0193094723\nInput_7                0  0.007272924  0.0229056627  0.0001989322  0.0043286891\nInput_8                0 -0.005761526 -0.0072661606 -0.0204701889  0.0061648567\nHidden_2_1             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nHidden_2_2             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nHidden_2_3             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nHidden_2_4             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nHidden_2_5             0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nOutput_benign          0  0.000000000  0.0000000000  0.0000000000  0.0000000000\nOutput_malignant       0  0.000000000  0.0000000000  0.0000000000  0.0000000000\n                   Hidden_2_5 Output_benign Output_malignant\nInput_1           0.030488508   0.000000000      0.000000000\nInput_2           0.023240086   0.000000000      0.000000000\nInput_3          -0.077304244   0.000000000      0.000000000\nInput_4           0.042423774   0.000000000      0.000000000\nInput_5           0.003147935   0.000000000      0.000000000\nInput_6           0.001040343   0.000000000      0.000000000\nInput_7           0.004078272   0.000000000      0.000000000\nInput_8          -0.012916351   0.000000000      0.000000000\nHidden_2_1        0.000000000   0.112992622      0.096456863\nHidden_2_2        0.000000000   0.005558367     -0.009282685\nHidden_2_3        0.000000000  -0.055406742     -0.072724856\nHidden_2_4        0.000000000  -0.074957542     -0.090871565\nHidden_2_5        0.000000000   0.024224803      0.009294559\nOutput_benign     0.000000000   0.000000000      0.000000000\nOutput_malignant  0.000000000   0.000000000      0.000000000\n\npar(mfrow=c(2,2))\nplotIterativeError(model)\n\npredictions &lt;- predict(model,BreastCancer2$inputsTest)\n\nplotRegressionError(predictions[,2], BreastCancer2$targetsTest[,2])\n\nconfusionMatrix(BreastCancer2$targetsTrain,fitted.values(model))\n\n       predictions\ntargets      1\n      1 262125\n      2 153185\n\nconfusionMatrix(BreastCancer2$targetsTest,predictions)\n\n       predictions\ntargets     1\n      1 58017\n      2 15274\n\nplotROC(fitted.values(model)[,2], BreastCancer2$targetsTrain[,2])\nplotROC(predictions[,2], BreastCancer2$targetsTest[,2])\n\n\n\n\n\n\n\nprobs &lt;- predictions / rowSums(predictions)\n\n#confusion matrix with 402040-method\nconfusionMatrix(BreastCancer2$targetsTrain, encodeClassLabels(fitted.values(model),                                                      method=\"402040\", l=0.4, h=0.6))\n\n       predictions\ntargets      1\n      1 262125\n      2 153185\n\n\n\n\n8.10.0.2 Deep survival neural network\nThere are several libraries available in survivalmodels which interface with pycox from Python. First, we illustrate the use of deepsurv.\n\nlibrary(survivalmodels)\n\nWarning: package 'survivalmodels' was built under R version 4.3.2\n\ndata(Melanoma, package = \"MASS\")\n\ntimes &lt;- Melanoma$time\ntimes &lt;- ceiling(times/7)  ## weeks\n\n#1 died from melanoma, 2 alive, 3 dead from other causes.\n##delta: 0=censored, 1=dead\ndelta=ifelse(Melanoma$status==2,0,1)\n\n## matrix of observed covariates\nx.train &lt;- cbind(Melanoma$sex, Melanoma$age, Melanoma$thickness)\n\ndeepsurv(data = Melanoma, \n    frac = 0.3, #Fraction of data to use for validation \n    activation = \"relu\",\n    num_nodes = c(4L, 8L, 4L, 2L), \n    dropout = 0.1, \n    early_stopping = TRUE, \n    epochs = 100L, #number of epochs.\n    batch_size = 32L)\n\n\n DeepSurv Neural Network \n\nCall:\n  deepsurv(data = Melanoma, frac = 0.3, activation = \"relu\", num_nodes = c(4L,      8L, 4L, 2L), dropout = 0.1, early_stopping = TRUE, batch_size = 32L,      epochs = 100L)\n\nResponse:\n  Surv(time, status)\nFeatures:\n  {sex, age, year, thickness, ulcer} \n\n\nUsing the same Melanoma dataset, we illustrate DeepHit.\n\nDH&lt;-deephit(data = Melanoma, \n    frac = 0.3, #Fraction of data to use for validation \n    activation = \"relu\",\n    num_nodes = c(4L, 8L, 4L, 2L), \n    dropout = 0.1, \n    early_stopping = TRUE, \n    epochs = 100L, #number of epochs.\n    batch_size = 32L)\n\nsummary(DH)\n\n\n DeepHit Neural Network \n\nCall:\n  deephit(data = Melanoma, frac = 0.3, activation = \"relu\", num_nodes = c(4L,      8L, 4L, 2L), dropout = 0.1, early_stopping = TRUE, batch_size = 32L,      epochs = 100L)\n\nResponse:\n  Surv(time, status)\nFeatures:\n  {sex, age, year, thickness, ulcer} \n\n\nCombining the deepsurv and deephit output.\n\nlibrary(mlr3)\nlibrary(mlr3proba)\n\nWarning: package 'mlr3proba' was built under R version 4.3.2\n\nlibrary(mlr3extralearners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\n\n## get the `whas` task from mlr3proba\nwhas &lt;- tsk(\"whas\")\n\n## create our own task from the Melanoma dataset\nMelanoma_data &lt;- MASS::Melanoma\n\nMelanoma_data$status=ifelse(Melanoma_data$status==2,0,1)\n\n## convert characters to factors\nMelanoma_data$sex &lt;- factor(Melanoma_data$sex)\nMelanomaTS &lt;- TaskSurv$new(\"Melanoma\", Melanoma_data, time = \"time\", event = \"status\")\n\n\n#1 died from melanoma, 2 alive, 3 dead from other causes.\n##delta: 0=censored, 1=dead\ndelta=ifelse(Melanoma_data$status==2,0,1)\n\n## matrix of observed covariates\nx.train &lt;- cbind(Melanoma_data$sex, Melanoma_data$age, Melanoma_data$thickness)\n\n\n## combine in list\ntasks &lt;- list(whas, MelanomaTS)\n\nlibrary(paradox)\n\nsearch_space &lt;- ps(\n ## p_dbl for numeric valued parameters\n dropout = p_dbl(lower = 0, upper = 1),\n weight_decay = p_dbl(lower = 0, upper = 0.5),\n learning_rate = p_dbl(lower = 0, upper = 1),\n \n ## p_int for integer valued parameters\n nodes = p_int(lower = 1, upper = 32),\n k = p_int(lower = 1, upper = 4)\n)\n\nsearch_space$trafo &lt;- function(x, param_set) {\n x$num_nodes = rep(x$nodes, x$k)\n x$nodes = x$k = NULL\n return(x)\n}\n\n\ncreate_autotuner &lt;- function(learner) {\n AutoTuner$new(\n   learner = learner,\n   search_space = search_space,\n   resampling = rsmp(\"holdout\"),\n   measure = msr(\"surv.cindex\"),\n   terminator = trm(\"evals\", n_evals = 2),\n   tuner = tnr(\"random_search\")\n )\n}\n\n## load learners\nlearners &lt;- lrns(\n #paste0(\"surv.\", c(\"deephit\", \"deepsurv\")), #crash when running multiple learners\n paste0(\"surv.\", c( \"deepsurv\")),\n frac = 0.3, early_stopping = TRUE, epochs = 10, optimizer = \"adam\"\n)\n \n# apply our function\nlearners &lt;- lapply(learners, create_autotuner)\n\n\ncreate_pipeops &lt;- function(learner) {\n po(\"encode\") %&gt;&gt;% po(\"scale\") %&gt;&gt;% po(\"learner\", learner)\n}\n\n## apply our function\nlearners &lt;- lapply(learners, create_pipeops)\n\n## select holdout as the resampling strategy\nresampling &lt;- rsmp(\"cv\", folds = 2)\n\n## add KM and CPH\nlearners &lt;- c(learners, lrns(c(\"surv.kaplan\", \"surv.coxph\")))\n#learners &lt;- c(learners, lrns(c(\"surv.coxph\")))\ndesign &lt;- benchmark_grid(tasks, learners, resampling)\nbm &lt;- benchmark(design)\n\nINFO  [13:26:27.729] [mlr3] Running benchmark with 12 resampling iterations\nINFO  [13:26:27.788] [mlr3] Applying learner 'encode.scale.surv.deepsurv.tuned' on task 'whas' (iter 1/2)\nINFO  [13:26:27.955] [bbotk] Starting to optimize 5 parameter(s) with '&lt;OptimizerRandomSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=2, k=0]'\nINFO  [13:26:27.985] [bbotk] Evaluating 1 configuration(s)\nINFO  [13:26:28.002] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [13:26:28.011] [mlr3] Applying learner 'surv.deepsurv' on task 'whas' (iter 1/1)\nINFO  [13:26:28.168] [mlr3] Finished benchmark\nINFO  [13:26:28.269] [bbotk] Result of batch 1:\nINFO  [13:26:28.273] [bbotk]    dropout weight_decay learning_rate nodes k surv.cindex warnings errors\nINFO  [13:26:28.273] [bbotk]  0.6247406    0.4554209     0.8456182    11 3   0.6457419        0      0\nINFO  [13:26:28.273] [bbotk]  runtime_learners                                uhash\nINFO  [13:26:28.273] [bbotk]              0.14 48e68821-91cd-4644-83be-1465ce9a80d6\nINFO  [13:26:28.284] [bbotk] Evaluating 1 configuration(s)\nINFO  [13:26:28.313] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [13:26:28.322] [mlr3] Applying learner 'surv.deepsurv' on task 'whas' (iter 1/1)\nINFO  [13:26:28.428] [mlr3] Finished benchmark\nINFO  [13:26:28.475] [bbotk] Result of batch 2:\nINFO  [13:26:28.478] [bbotk]    dropout weight_decay learning_rate nodes k surv.cindex warnings errors\nINFO  [13:26:28.478] [bbotk]  0.1573471    0.4268247     0.6872897    27 1   0.5627744        0      0\nINFO  [13:26:28.478] [bbotk]  runtime_learners                                uhash\nINFO  [13:26:28.478] [bbotk]              0.09 fb552b92-0921-4c70-8470-bd5b78c80bd1\nINFO  [13:26:28.493] [bbotk] Finished optimizing after 2 evaluation(s)\nINFO  [13:26:28.494] [bbotk] Result:\nINFO  [13:26:28.495] [bbotk]    dropout weight_decay learning_rate nodes k learner_param_vals  x_domain\nINFO  [13:26:28.495] [bbotk]  0.6247406    0.4554209     0.8456182    11 3          &lt;list[8]&gt; &lt;list[4]&gt;\nINFO  [13:26:28.495] [bbotk]  surv.cindex\nINFO  [13:26:28.495] [bbotk]    0.6457419\nINFO  [13:26:28.724] [mlr3] Applying learner 'encode.scale.surv.deepsurv.tuned' on task 'whas' (iter 2/2)\nINFO  [13:26:28.866] [bbotk] Starting to optimize 5 parameter(s) with '&lt;OptimizerRandomSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=2, k=0]'\nINFO  [13:26:28.890] [bbotk] Evaluating 1 configuration(s)\nINFO  [13:26:28.906] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [13:26:28.913] [mlr3] Applying learner 'surv.deepsurv' on task 'whas' (iter 1/1)\nINFO  [13:26:28.993] [mlr3] Finished benchmark\nINFO  [13:26:29.021] [bbotk] Result of batch 1:\nINFO  [13:26:29.023] [bbotk]    dropout weight_decay learning_rate nodes k surv.cindex warnings errors\nINFO  [13:26:29.023] [bbotk]  0.9949267    0.2285935     0.8684118    17 4   0.5124002        0      0\nINFO  [13:26:29.023] [bbotk]  runtime_learners                                uhash\nINFO  [13:26:29.023] [bbotk]              0.07 d46a5e87-b652-4667-b6aa-3c26b93a19a5\nINFO  [13:26:29.029] [bbotk] Evaluating 1 configuration(s)\nINFO  [13:26:29.043] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [13:26:29.050] [mlr3] Applying learner 'surv.deepsurv' on task 'whas' (iter 1/1)\nINFO  [13:26:29.178] [mlr3] Finished benchmark\nINFO  [13:26:29.222] [bbotk] Result of batch 2:\nINFO  [13:26:29.224] [bbotk]    dropout weight_decay learning_rate nodes k surv.cindex warnings errors\nINFO  [13:26:29.224] [bbotk]  0.8363856    0.2299571     0.9908099    13 2   0.5128205        0      0\nINFO  [13:26:29.224] [bbotk]  runtime_learners                                uhash\nINFO  [13:26:29.224] [bbotk]              0.13 3faeca40-4635-4b4d-92dd-ae25f9f2eb61\nINFO  [13:26:29.240] [bbotk] Finished optimizing after 2 evaluation(s)\nINFO  [13:26:29.241] [bbotk] Result:\nINFO  [13:26:29.242] [bbotk]    dropout weight_decay learning_rate nodes k learner_param_vals  x_domain\nINFO  [13:26:29.242] [bbotk]  0.8363856    0.2299571     0.9908099    13 2          &lt;list[8]&gt; &lt;list[4]&gt;\nINFO  [13:26:29.242] [bbotk]  surv.cindex\nINFO  [13:26:29.242] [bbotk]    0.5128205\nINFO  [13:26:29.489] [mlr3] Applying learner 'surv.kaplan' on task 'whas' (iter 1/2)\nINFO  [13:26:29.512] [mlr3] Applying learner 'surv.kaplan' on task 'whas' (iter 2/2)\nINFO  [13:26:29.534] [mlr3] Applying learner 'surv.coxph' on task 'whas' (iter 1/2)\nINFO  [13:26:29.570] [mlr3] Applying learner 'surv.coxph' on task 'whas' (iter 2/2)\nINFO  [13:26:29.599] [mlr3] Applying learner 'encode.scale.surv.deepsurv.tuned' on task 'Melanoma' (iter 1/2)\nINFO  [13:26:29.723] [bbotk] Starting to optimize 5 parameter(s) with '&lt;OptimizerRandomSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=2, k=0]'\nINFO  [13:26:29.743] [bbotk] Evaluating 1 configuration(s)\nINFO  [13:26:29.756] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [13:26:29.762] [mlr3] Applying learner 'surv.deepsurv' on task 'Melanoma' (iter 1/1)\nINFO  [13:26:29.825] [mlr3] Finished benchmark\nINFO  [13:26:29.850] [bbotk] Result of batch 1:\nINFO  [13:26:29.852] [bbotk]    dropout weight_decay learning_rate nodes k surv.cindex warnings errors\nINFO  [13:26:29.852] [bbotk]  0.4570555    0.2741379     0.8601232    20 1   0.7037037        0      0\nINFO  [13:26:29.852] [bbotk]  runtime_learners                                uhash\nINFO  [13:26:29.852] [bbotk]              0.06 8e4322ea-b826-4134-8200-d013111aae8d\nINFO  [13:26:29.858] [bbotk] Evaluating 1 configuration(s)\nINFO  [13:26:29.870] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [13:26:29.876] [mlr3] Applying learner 'surv.deepsurv' on task 'Melanoma' (iter 1/1)\nINFO  [13:26:29.939] [mlr3] Finished benchmark\nINFO  [13:26:30.027] [bbotk] Result of batch 2:\nINFO  [13:26:30.029] [bbotk]    dropout weight_decay learning_rate nodes k surv.cindex warnings errors\nINFO  [13:26:30.029] [bbotk]  0.7608218     0.479834     0.7939875    20 3    0.462963        0      0\nINFO  [13:26:30.029] [bbotk]  runtime_learners                                uhash\nINFO  [13:26:30.029] [bbotk]              0.06 6897696f-9efb-4441-8c53-cf90370145aa\nINFO  [13:26:30.044] [bbotk] Finished optimizing after 2 evaluation(s)\nINFO  [13:26:30.045] [bbotk] Result:\nINFO  [13:26:30.047] [bbotk]    dropout weight_decay learning_rate nodes k learner_param_vals  x_domain\nINFO  [13:26:30.047] [bbotk]  0.4570555    0.2741379     0.8601232    20 1          &lt;list[8]&gt; &lt;list[4]&gt;\nINFO  [13:26:30.047] [bbotk]  surv.cindex\nINFO  [13:26:30.047] [bbotk]    0.7037037\nINFO  [13:26:30.260] [mlr3] Applying learner 'encode.scale.surv.deepsurv.tuned' on task 'Melanoma' (iter 2/2)\nINFO  [13:26:30.384] [bbotk] Starting to optimize 5 parameter(s) with '&lt;OptimizerRandomSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=2, k=0]'\nINFO  [13:26:30.404] [bbotk] Evaluating 1 configuration(s)\nINFO  [13:26:30.417] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [13:26:30.423] [mlr3] Applying learner 'surv.deepsurv' on task 'Melanoma' (iter 1/1)\nINFO  [13:26:30.489] [mlr3] Finished benchmark\nINFO  [13:26:30.514] [bbotk] Result of batch 1:\nINFO  [13:26:30.516] [bbotk]    dropout weight_decay learning_rate nodes k surv.cindex warnings errors\nINFO  [13:26:30.516] [bbotk]  0.1140623    0.2846605       0.22349    10 4   0.5548387        0      0\nINFO  [13:26:30.516] [bbotk]  runtime_learners                                uhash\nINFO  [13:26:30.516] [bbotk]              0.06 2097e901-dc16-4498-8fe4-319409f51b73\nINFO  [13:26:30.522] [bbotk] Evaluating 1 configuration(s)\nINFO  [13:26:30.534] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [13:26:30.540] [mlr3] Applying learner 'surv.deepsurv' on task 'Melanoma' (iter 1/1)\nINFO  [13:26:30.602] [mlr3] Finished benchmark\nINFO  [13:26:30.630] [bbotk] Result of batch 2:\nINFO  [13:26:30.632] [bbotk]    dropout weight_decay learning_rate nodes k surv.cindex warnings errors\nINFO  [13:26:30.632] [bbotk]  0.2337101    0.3132217     0.5829765     8 3   0.6967742        0      0\nINFO  [13:26:30.632] [bbotk]  runtime_learners                                uhash\nINFO  [13:26:30.632] [bbotk]              0.04 bbfab361-fd16-4d28-af25-973a48eea689\nINFO  [13:26:30.641] [bbotk] Finished optimizing after 2 evaluation(s)\nINFO  [13:26:30.641] [bbotk] Result:\nINFO  [13:26:30.642] [bbotk]    dropout weight_decay learning_rate nodes k learner_param_vals  x_domain\nINFO  [13:26:30.642] [bbotk]  0.2337101    0.3132217     0.5829765     8 3          &lt;list[8]&gt; &lt;list[4]&gt;\nINFO  [13:26:30.642] [bbotk]  surv.cindex\nINFO  [13:26:30.642] [bbotk]    0.6967742\nINFO  [13:26:30.904] [mlr3] Applying learner 'surv.kaplan' on task 'Melanoma' (iter 1/2)\nINFO  [13:26:30.928] [mlr3] Applying learner 'surv.kaplan' on task 'Melanoma' (iter 2/2)\nINFO  [13:26:30.952] [mlr3] Applying learner 'surv.coxph' on task 'Melanoma' (iter 1/2)\nINFO  [13:26:30.979] [mlr3] Applying learner 'surv.coxph' on task 'Melanoma' (iter 2/2)\nINFO  [13:26:30.999] [mlr3] Finished benchmark\n\n## Aggreggate with Harrell's C and Integrated Graf Score\nmsrs &lt;- msrs(c(\"surv.cindex\", \"surv.graf\"))\nbm$aggregate(msrs)[, c(3, 4, 7, 8)]\n\n    task_id                       learner_id surv.cindex surv.graf\n1:     whas encode.scale.surv.deepsurv.tuned   0.4969695       Inf\n2:     whas                      surv.kaplan   0.5000000       Inf\n3:     whas                       surv.coxph   0.7502472       Inf\n4: Melanoma encode.scale.surv.deepsurv.tuned   0.4938226       Inf\n5: Melanoma                      surv.kaplan   0.5000000       Inf\n6: Melanoma                       surv.coxph   0.7383489       Inf\n\nlibrary(mlr3benchmark)\n\nWarning: package 'mlr3benchmark' was built under R version 4.3.2\n\n\n\nAttaching package: 'mlr3benchmark'\n\n\nThe following object is masked from 'package:survivalmodels':\n\n    requireNamespaces\n\n## create mlr3benchmark object\nbma &lt;- as.BenchmarkAggr(bm)\n\nWarning: 'as.BenchmarkAggr' is deprecated.\nUse 'as_benchmark_aggr' instead.\nSee help(\"Deprecated\")\n\n## run global Friedman test\nbma$friedman_test()\n\n\n    Friedman rank sum test\n\ndata:  cindex and learner_id and task_id\nFriedman chi-squared = 4, df = 2, p-value = 0.1353\n\n\n\n\n8.10.1 Transformer\nTransformer is a deep neural network architecture that uses entity embedding and attentive mechanism.\n\n8.10.1.1 Tabnet\nTabnet can be run as part of mlrverse.\n\nlibrary(tidyverse)\nlibrary(mlr3verse)\nlibrary(tabnet)\nlibrary(recipes)\nlibrary(yardstick)\ndata(\"BreastCancer\",package = \"mlbench\")\n#The Breast Cancer data contains NA as well as factors\n#note Class is benign or malignant of class factor\n#column Bare.nuclei removed due to NA\nBreastCancer&lt;-BreastCancer[,-c(1,7)] #%&gt;% \n  #mutate(Class=ifelse(Class==\"malignant\",1,0))\n\n#split data using caTools. \n#The next example will use createDataPartition from caret\nset.seed(123)\nsplit = caTools::sample.split(BreastCancer$Class, SplitRatio = 0.75)\nTrain = subset(BreastCancer, split == TRUE)\nTest = subset(BreastCancer, split == FALSE)\n\nrec &lt;- recipe(Class ~ ., data = Train) %&gt;% \n  step_normalize(all_numeric(), -all_outcomes())\n\n#epoch is number of epoch\nfit &lt;- tabnet_fit(rec, Train, epochs = 30, valid_split=0.1, learn_rate = 5e-3)\nautoplot(fit)\n\nChecking performance\n\nmetrics &lt;- metric_set(accuracy, precision, recall)\ncbind(Test, predict(fit, Test)) %&gt;% \n  metrics(Class, estimate = .pred_class)\n\nPlot\n\ncbind(Test, predict(fit, Test, type = \"prob\")) %&gt;% \n  #check the data frame as the argument is taken from the truth factor Class \n  #and contains benign and malignant\n  roc_auc(Class, .pred_benign)\n\nExplain model on Test data with with attention heatmap\n\nexplain &lt;- tabnet_explain(fit, Test)\nautoplot(explain)\n\n\n\n8.10.1.2 Prior data fitted network\nTabPFN is a transformer neural network. It does not need preprocessing of the data. It performs its own preprocessing, including z-score normalization and outlier handling. There is no R implementation of TabPFN at this stage.\n\n\n\n8.10.2 CNN\nConvolution neural network or CNN is an artifical neural network method that is well suited to classification of image data. CNN is able to develop an internal representation of the image.\n\n\n8.10.3 RNN\nRecurrent neural network or RNN is an artifical neural network method that is well suited to data with repeated patterns such as natural language processing. However, this architecture is less suited for tabular or imaging data.\n\n\n8.10.4 Reinforcement learning\nReinforcement learning is an unsupervised method which uses trial and error for agent to learn and adapt.\n\nlibrary(ReinforcementLearning)\n\n\n\n\n\nGrinsztajn, Léo, Edouard Oyallon, and Gaël Varoquaux. 2022. “Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data?” ArXiv abs/2207.08815. https://doi.org/10.48550/arXiv.2207.08815.\n\n\nMcElfresh, Duncan C., Sujay Khandagale, Jonathan Valverde, C. VishakPrasad, Ben Feuer, Chinmay Hegde, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. 2023. “When Do Neural Nets Outperform Boosted Trees on Tabular Data?” ArXiv abs/2305.02997. https://doi.org/10.48550/arXiv.2305.02997.\n\n\nMolnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018. “Iml: An r Package for Interpretable Machine Learning.” JOSS 3 (26): 786. https://doi.org/10.21105/joss.00786.\n\n\nPhan, T. G., J. Chen, S. Singhal, H. Ma, B. B. Clissold, J. Ly, and R. Beare. 2018. “Exploratory Use of Decision Tree Analysis in Classification of Outcome in Hypoxic-Ischemic Brain Injury.” Front Neurol 9: 126.\n\n\nPhan, T. G., T. Kooblal, C. Matley, S. Singhal, B. Clissold, J. Ly, A. G. Thrift, V. Srikanth, and H. Ma. 2019. “Stroke Severity Versus Dysphagia Screen as Driver for Post-stroke Pneumonia.” Front Neurol 10: 16.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "graph-theory.html",
    "href": "graph-theory.html",
    "title": "9  Graph Theory",
    "section": "",
    "text": "9.1 Special graphs",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Graph Theory</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#special-graphs",
    "href": "graph-theory.html#special-graphs",
    "title": "9  Graph Theory",
    "section": "",
    "text": "9.1.1 Laplacian matrix\nThe Laplacian matrix is the difference between the degree and adjacency matrix. The degree matrix represents the number of direct connection of a node. The diagonal of Laplacian matrix retains the diagonal values of the degree matrix (since the diagonal of the adjacency matrix consists of zeroes). The smallest eigenvalue of the Laplacian describes whether the graph is connected or not. The second eigenvalue of the Laplacian matrix is the algebraic connectivity or Fiedler value. High Fiedler value indicates greater number of connected components and consequently, resilience to breakdown in flow of information between members(Jamakovic and Mieghem 2008) .\n\n\n9.1.2 Bimodal (bipartite) graph\nBimodal graphs are of interest in social network and in analysis of food ecosystem in nature. A well studied example is the Zachary karate club network.This dataset is included in the igraphdata package.\n\nlibrary(latentnet)\n\nLoading required package: network\n\n\n\n'network' 1.18.1 (2023-01-24), part of the Statnet Project\n* 'news(package=\"network\")' for changes since last version\n* 'citation(\"network\")' for citation information\n* 'https://statnet.org' for help, support, and other information\n\n\n\nAttaching package: 'network'\n\n\nThe following objects are masked from 'package:igraph':\n\n    %c%, %s%, add.edges, add.vertices, delete.edges, delete.vertices,\n    get.edge.attribute, get.edges, get.vertex.attribute, is.bipartite,\n    is.directed, list.edge.attributes, list.vertex.attributes,\n    set.edge.attribute, set.vertex.attribute\n\n\nLoading required package: ergm\n\n\n\n'ergm' 4.5.0 (2023-05-27), part of the Statnet Project\n* 'news(package=\"ergm\")' for changes since last version\n* 'citation(\"ergm\")' for citation information\n* 'https://statnet.org' for help, support, and other information\n\n\n'ergm' 4 is a major update that introduces some backwards-incompatible\nchanges. Please type 'news(package=\"ergm\")' for a list of major\nchanges.\n\n\n\n'latentnet' 2.10.6 (2022-05-11), part of the Statnet Project\n* 'news(package=\"latentnet\")' for changes since last version\n* 'citation(\"latentnet\")' for citation information\n* 'https://statnet.org' for help, support, and other information\nNOTE: BIC calculation prior to latentnet 2.7.0 had a bug in the calculation of the effective number of parameters. See help(summary.ergmm) for details.\nNOTE: Prior to version 2.8.0, handling of fixed effects for directed networks had a bug. See help(\"ergmm-terms\") for details.\n\n#davis of social network\ndata(davis)\ndavis.fit&lt;-ergmm(davis~bilinear(d=2)+rsociality)\nplot(davis.fit,pie=TRUE,rand.eff=\"sociality\",labels=TRUE)\n\n\n\n\n\n\n\n#davis[,1:10]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Graph Theory</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#centrality-measures",
    "href": "graph-theory.html#centrality-measures",
    "title": "9  Graph Theory",
    "section": "9.2 Centrality Measures",
    "text": "9.2 Centrality Measures\nThe nodes in this case are variables such as ASPECTS regions, demographic and risk factors. This graph below uses data from a paper on the use of PageRank where a graph of 4 variables were used to illustrate the PageRank method for searching brain regions related to disability (Beare et al. 2015).\n\n#the network data is provided above on ASPECTS regions\n\n#degree is available in igraph, sna\nd&lt;-igraph::degree(aspects)\n\n#closeness is available in igraph, sna\ncl&lt;-igraph::closeness(aspects)\n\n#betweenness\nb&lt;-igraph::betweenness(aspects)\n\n#page rank\np&lt;-page.rank(aspects)\ndf&lt;-data.frame(\"degree\"=round(d,2),\"closeness\"=round(cl,2),\"betweenness\" =round(b,2),\"PageRank\" =round(p$vector,2))\nknitr::kable(df)\n\n\n\n\n\ndegree\ncloseness\nbetweenness\nPageRank\n\n\n\n\nM1\n3\n0.33\n0\n0.29\n\n\nM2\n4\n0.50\n2\n0.37\n\n\nDisability\n1\nNaN\n0\n0.25\n\n\ntpa\n2\n0.25\n0\n0.09\n\n\n\n\n\n\n9.2.1 Local centrality measures\nCentrality measures assign a measure of “importance” to nodes and can therefore indicate whether some nodes are more critical than others in a given network. When network nodes represent variables, centrality measures may indicate relevance of variables to a model.\nThe simplest centrality measure, degree centrality, is the count of links for each node and is a purely local measure of importance. Node strength, used in weighted networks, is the sum of weights of edges entering or leaving (or both) the node. Other measures, such as betweenness centrality, describe more global structure - the degree of participation of a node as conduit of information between other nodes in a network.\n\n\n9.2.2 Global centrality measures\n\n9.2.2.1 Page Rank\nPageRank is one member of a family of graph eigenvector centrality measures, all of which incorporate the idea that the score of a node depends, at least in part, on the scores of neighbors connecting to the node. Thus a page may have a high PageRank score if many pages link to it, or if a few important or authoritative pages link to it.\nOthers include eigenvector centrality (which works best with undirected graphs), alpha centrality and Katz centrality. PageRank uses a different scaling for connections (by the number of links leaving the node) and importance is based on incoming connections rather than outgoing connections(Brin and Page 1998) . Eigenvector centrality measure a node’s centrality in terms of node parameters and centrality of neighboring nodes.\nPageRank has several differences with respect to other eigenvector centrality methods, expanded below, which make it better suited for digraphs. PageRank was originally described in terms of a web user/surfer randomly clicking links, and the PageRank of a web page corresponds to the probability of the random surfer arriving at the page of interest. The model of the random surfer used in the PageRank computation includes a damping factor, which represents the chance of the random surfer becoming bored and selecting a completely different page at random (teleporting to a random page). Similarly, if a page is a sink (i.e. has no outgoing links), then the random web surfer may click on to a random page.\nA number of different approaches are available for computing the PageRank for nodes in a network. The conceptually simplest is to assign an equal initial score to each node, and then iteratively update PageRank scores. This is easy to perform algebraically for a small number of nodes but can take a long time with larger data. In practice, it is performed using eigenvector methode. PageRank analysis can be performed using igraph package. (Beare et al. 2015)\nGraph based methods have emerged as tools for interpreting and analysing connected network structures and in this case network structures associated with disability.These types of analysis are attractive because they assess the connectedness of each region of interest (ROI) with respect to other ROIs over the entire brain network.Eigenvector centrality methods have been used to explore connectedness of brain regions. PageRank is a variant of eigenvector centrality and is an ideal method for analysis ofdirected graph (the edges between adjacent nodes(regions) have direction). This method was initially developed as the basis of the search engine for Google. PageRank offered a considerable improvement over pure text based methods in ranking search results,and had the advantage of being content independent (i.e. the search is based on links between the web pages). PageRank emphasises web pages based on the number of links directed to a page and the importance of the sources of those links. Thus a small number of links from influential pages can greatly enhance the importance of the destination page.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Graph Theory</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#community",
    "href": "graph-theory.html#community",
    "title": "9  Graph Theory",
    "section": "9.3 Community",
    "text": "9.3 Community",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Graph Theory</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#visualising-graph",
    "href": "graph-theory.html#visualising-graph",
    "title": "9  Graph Theory",
    "section": "9.4 Visualising graph",
    "text": "9.4 Visualising graph\nThere are many packages for visualising graph such as igraph, sna, ggraph, _Rgraphviz, visnetwork, networkD3.\n\n9.4.1 Visnetwork\n\nlibrary(igraph)\nlibrary(RColorBrewer)\nlibrary(visNetwork)\nedge&lt;- read.csv(\"./Data-Use/TPA_edge.csv\") #3 columns:V1 V2 time \nnode&lt;-read.csv(\"./Data-Use/TPA_node.csv\")\ndf&lt;-graph_from_data_frame(d=edge[,c(1,2)],vertices = node,directed=F)\nV(df)$type&lt;-V(df)$name %in% edge[,1]\n#assign color\nvertex.label&lt;-V(df)$membership\n# Generate colors based on type:\ncolrs &lt;- c(\"gray50\", \"gold\")\nV(df)$color &lt;- colrs[V(df)$membership]\n#assign shape\nshape &lt;- c(\"circle\", \"square\")\nV(df)$shape&lt;-shape[V(df)$membership] \n\n#extract data for visNetwork\ndata&lt;-toVisNetworkData(df)\nvisNetwork(nodes=data$nodes,edges=data$edges, main=\"TPA ECR Network\")%&gt;% visNodes(color = list(hover = \"red\")) %&gt;% visInteraction(hover = TRUE)\n\n\n\n\n\n\n\n9.4.2 Large graph\nThere are very few softwares capable of handling very large graph with the exception of Gephi , Cytoscape and Neo4J.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Graph Theory</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#social-media-and-network-analysis",
    "href": "graph-theory.html#social-media-and-network-analysis",
    "title": "9  Graph Theory",
    "section": "9.5 Social Media and Network Analysis",
    "text": "9.5 Social Media and Network Analysis\nSocial media platform such as twitter, youtube, facebook and instagram are rich source of information for graph theory analysis as well as textmining. The following section only covers twitter and youtube as both are accessible to the public. There’s restricted access for facebook and instagram.\n\n9.5.1 Twitter\nAnalysis of Twitter requires creating an account on Twitter. This step will generate a series of keys listed below. These keys should be stored in a secret location. There are several different ways to access Twitter data. It should be noted that the data covers a range of 9 days and a maximum of 18000 tweets can be downloaded each day. The location of the tweeter can also be accessed if you have created an account with Google Maps API.\n\nlibrary(rtweet)\ncreate_token(\n  app= \"\",\n  consumer_key=\"\",\n  consumer_secret = \"\",\n  access_token = \"\",\n  access_secret = \"\"\n)\n# searching for tweets on WHO\nCW &lt;- search_tweets(\"WHO\", n = 18000, include_rts = TRUE)\n# searching for tweets confined y location\nsearchTerm_t= (geocode= (\"-37.81363,144.9631,5km\"))\nmyTwitterData &lt;- Authenticate(\"twitter\", \n                              apiKey=myapikey, apiSecret=myapisecret, \n                              accessToken=myaccesstoken, accessTokenSecret=myaccesstokensecret) %&gt;% \n  Collect(searchTerm=searchTerm_t, numTweets=100, writeToFile=FALSE,verbose=TRUE)\n#alternately confined search for tweets on MS from Australia\nMStw &lt;- search_tweets(\n  \"multiple sclerosis\", geocode = lookup_coords(\"AUS\"),n = 18000, include_rts = FALSE\n)\nrt &lt;- lat_lng(MStw)#extract lat and lon from tweets\n## plot lat and lng points onto  map\nwith(rt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))\nleaflet:: leaflet(data=rt) %&gt;% addTiles () %&gt;% addCircles(lat=~lat,lng=~lng)\n## plot time series of tweets\nts_plot(MStw, \"weeks\") +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(plot.title = ggplot2::element_text(face = \"bold\"))   ggplot2::labs(\n    x = NULL, y = NULL,\n    title = \"Frequency of #MS Twitter statuses from past 9 days\",\n    subtitle = \"Twitter status (tweet) counts aggregated using three-hour intervals\",\n    caption = \"\\nSource: Data collected from Twitter's REST API via rtweet\"\n  )\n\n\n\n9.5.2 Youtube\nTo perform analysis of comments on Youtube, a Google Developer’s account should be created. The apikey shoud be saved in a secret location. The analysis can be done by identifying the video of interest.\n\nlibrary(SocialMediaLab)\napiKey &lt;- \"\"\nvideoIDs&lt;-c(\"YHzz2cXBlGk\") #123 comments #406,253 views 2/2/18\n#extract\ng_youtube_actor &lt;- Authenticate(\"youtube\", apiKey= apiKey) %&gt;%\n  Collect(videoIDs = videoIDs, writeToFile=TRUE) %&gt;%\n  Create(\"Actor\")\n\n\n\n\n\nBeare, R., J. Chen, T. G. Phan, R. K. Lees, M. Ali, A. Alexandrov, P. M. Bath, et al. 2015. “Googling Stroke ASPECTS to Determine Disability: Exploratory Analysis from VISTA-Acute Collaboration.” PLoS ONE 10 (5): e0125687.\n\n\nBrin, Sergey, and Lawrence Page. 1998. “The Anatomy of a Large-Scale Hypertextual Web Search Engine.” Computer Networks and ISDN Systems 30 (1-7): 107–17. https://doi.org/10.1016/S0169-7552(98)00110-X.\n\n\nFornito, Alex. 2016. “Graph Theoretic Analysis of Human Brain Networks.” In Neuromethods, edited by Massimo Filippi, 2nd ed., 119:283–314. Neuromethods 119. United States of America: Humana Press. https://doi.org/10.1007/978-1-4939-5611-1_10.\n\n\nJamakovic, A., and Piet Van Mieghem. 2008. “On the Robustness of Complex Networks by Using the Algebraic Connectivity.” In Networking, edited by Amitabha Das, Hung Keng Pung, Francis Bu-Sung Lee, and Lawrence Wai-Choong Wong, 4982:183–94. Lecture Notes in Computer Science. Springer. http://dblp.uni-trier.de/db/conf/networking/networking2008.html#JamakovicM08.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Graph Theory</span>"
    ]
  },
  {
    "objectID": "natural-language-processing.html",
    "href": "natural-language-processing.html",
    "title": "10  Natural language processing",
    "section": "",
    "text": "10.1 Bag of words\nBag of words or unigram analysis describe data in which words in a sentence were separated or tokenised. Within this bag of words the order of words within the document is not retained. Depending on how this process is performed the negative connotation may be loss. Consider “not green” and after cleaning of the document, only the color “green” remain.\nThe following codes illustrate the processing steps to clean up a document. These include turning words to lower case as R is case sensitive. Next stop word filter is used to remove phrases like “I”, “he”, “she”, “they” etc.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "natural-language-processing.html#bag-of-words",
    "href": "natural-language-processing.html#bag-of-words",
    "title": "10  Natural language processing",
    "section": "",
    "text": "10.1.1 TFIDF\nTerm frequency defines the frequency of a term in a document. The document frequency defines how often a term is used across document. The inverse document frequency can be seen as a weight to decrease the importance of commonly words used across documents. Term frequency inverse document frequency is a process used to down weight common terms and highlight important terms in the document.\nIn the example under topic modeling, an example of creating tfidf is shown. Other packages like tidytext, textmineR have functions for creating tfidf\n\n\n10.1.2 Extracting data from web\nThis is an example using RISmed library to extract data from PubMed on electronic medcical record and text mining for 2021.\n\n#library(adjutant)\nlibrary(RISmed)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(SnowballC)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(lattice)\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary (dplyr)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(stringr)\n\nThe function to extract data from PubMed.\nData such as tear of publications can be easily extracted.\nCreate list for high and low impact factor journals\nPlot of journal publications normalised by year.\nPlot of journal publications normalised by journal.\nCorpus\nThe steps in processing and creating a Corpus from tm library is illustrated.\nHere, the same preprocessing steps are performed using tidytext.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "natural-language-processing.html#wordcloud",
    "href": "natural-language-processing.html#wordcloud",
    "title": "10  Natural language processing",
    "section": "10.2 Wordcloud",
    "text": "10.2 Wordcloud\nA trick with wordcloud is setting the right number of words, the range of size of the words to be plotted.\nPlot Wordcloud with negative and positive sentiment from Bing library. Other sentiment libraries include afinn, loughran and nrc.\ngraph analysis of word relationship",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "natural-language-processing.html#bigram-analysis",
    "href": "natural-language-processing.html#bigram-analysis",
    "title": "10  Natural language processing",
    "section": "10.3 Bigram analysis",
    "text": "10.3 Bigram analysis\nThe relationship among the bigrams are illustrated here.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "natural-language-processing.html#trigram",
    "href": "natural-language-processing.html#trigram",
    "title": "10  Natural language processing",
    "section": "10.4 Trigram",
    "text": "10.4 Trigram\nThe relationship among the trigrams are illustrated here.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "natural-language-processing.html#topic-modeling-or-thematic-analysis",
    "href": "natural-language-processing.html#topic-modeling-or-thematic-analysis",
    "title": "10  Natural language processing",
    "section": "10.5 Topic modeling or thematic analysis",
    "text": "10.5 Topic modeling or thematic analysis\nTwo methods for unsupervised thematic analysis, NMF and probabilistic topic model, are illustrated.\n\n10.5.1 Probabilistic topic model\nProbabilistic topic modelling is a machine learning method that generates topics or discovers themes among a collection of documents. This step was performed using the Latent Dirichlet Allocation algorithm via the topicmodels package in R. An issue with topic modeling is that the number of topics are not known. It can be estimated empirically or by examining the harmonic means of the log likelihood .\nEstimate the number of topics based on the log likelihood of P(topics|documents) at each iterations\nThe previous analysis show that there are 40 topics. For ease of illustrations LDA is perform with 5 topics.\nCompare differences in words between topics.\n\n\n10.5.2 NMF\nIn the previous chapter, NMF was used as a method to cluster data. Here, it can be framed as a method for topic modeling. The term document matrix is used.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "natural-language-processing.html#deep-learning-for-text",
    "href": "natural-language-processing.html#deep-learning-for-text",
    "title": "10  Natural language processing",
    "section": "10.6 Deep Learning for text",
    "text": "10.6 Deep Learning for text\n\n10.6.1 transformer\nWe can use reticulate package to install transformers from Hugging Face. The example below uses DistilBERT transformer from Hugging Face.\n\n##########\n#the following installation files have been commented out\n#the files are stored under huggingfaceR envs\n#library(reticulate)\n#devtools::install_github(\"farach/huggingfaceR\")\n#hf_python_depends('transformers') \n\n##########\n\nlibrary(huggingfaceR)\n\ndistilBERT &lt;- hf_load_pipeline(\n  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\", \n  task = \"text-classification\"\n  )\n#&gt; \n#&gt; \n#&gt; distilbert-base-uncased-finetuned-sst-2-english is ready for text-classification\n\ndistilBERT\n#&gt; &lt;transformers.pipelines.text_classification.TextClassificationPipeline object at 0x000001D0A8F71510&gt;",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "bayesian-analysis.html",
    "href": "bayesian-analysis.html",
    "title": "11  Bayesian Analysis",
    "section": "",
    "text": "11.1 Baysian belief\nBayes Theorem deals with the impact of information update on current belief expresses in terms of probability. P(A|B) is the posterior probability of A given knowledge of B. P(A) is the prior probability. P(B|A) is the conditional probability of B given A. P(A|B)=P(B|A) x P(A)/P(B)\nBayes’ theorem can be expressed in terms of odds and post-test probabilities. This concept had been used in the analysis of the post-test probabilities of stroke related to the use of ABCD2. In this case the likelihood ratio and pretest probability of stroke in patients with TIA is used to calculate the post-test probability.\nTable 13. 2 x 2 table Disease + Disease - Test + eg ABCD2≥4 TP FP Test – eg ABCD2&lt;4 FN TN\nThe true positive rate (TPR) can be expressed as conditional probability P(T+|D+). TP is the joint distribution of T+ and D+ while TN is the joint distribution of T- and D-.\nThe true negative rate (TNR) can be expressed as conditional probability P(T-|D-).\nIn the setting of ABCD2, the pre-test odds is derived from cases of TIA and stroke outcome at 90 days.\nThe positive likelihood ratio (PLR) in this case is derived from the sensitivity of ABCD2 for stroke and the one minus specificity of ABCD for stroke. One can interpret the likelihood ratio in terms of Bayes theorem.\nTo derive the post-test odds, the PLR is multiplied by the pre-test odds at the voxel level. The post-test odds is given by the product of the pre-test odds and the likelihood ratio.\nIn turn, the post-test probabilities is calculated from the post-test odds by:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "bayesian-analysis.html#baysian-belief",
    "href": "bayesian-analysis.html#baysian-belief",
    "title": "11  Bayesian Analysis",
    "section": "",
    "text": "11.1.1 Conditional probability\nTwo events are independent if\nOr\nIt follows that the events A and B occur simultaneously P(A∩B) is given by the probability of event A and probability of event B given that A has occurred.\nIf events A and B are independent then P(A∩B) is given by the probability of event A and probability of event B.\n\n11.1.1.1 Bayes Rule\nFor a sample space (A) containing n different subspaces (A1, A2..An) and A is a subset of the larger sample space B+ and B-, the probability that A is a member of B+an be given by P(A|B+). This can be divised as a tree structure with one branch given by the product of P(B+) and P(A|B+) and the other P(B-) and P(A|B-). The probability of B is given by the sum of P(B+)P(A|B+) and P(B-)P(A|B).\nTo make a decision on which of An events to choose, one evaluate the conditional probability of each subset e.g. P(A1∩B)/P(B), P(A2∩B)/P(B)…P(An∩B)/P(B). The probability of P(B) is given by the sum of P(A1∩B), P(A2∩B)…P(An∩B). Here P(An∩B) is the same as P(An)P(B|An). The subspace with the highest conditional probability may yield the optimal result.\n\n\n11.1.1.2 Conditional independence\nThe conditional distribution is given by\nConditional independence states that two events (A, B) are independent on a third set (C) if those two sets are independent in their conditional probability distribution given C782. This is stated as",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "bayesian-analysis.html#markov-model",
    "href": "bayesian-analysis.html#markov-model",
    "title": "11  Bayesian Analysis",
    "section": "11.2 Markov model",
    "text": "11.2 Markov model\nThe Markov chain describes a chain of memoryless states which transit from one state to another without dependency on previous states. The Markov property The transition matrix describes the probabilities of changing from one state to another. A property of this Markov matrix is that the column data sum to one. An example is provided here \\(\\left[\\begin{array}{cc}.8 & .7\\\\.2 &.3\\end{array}\\right]\\). The column probabilities sum to 1. The PageRank method that we discuss in chapter on Graph Theory is a special form of Markov chain.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "bayesian-analysis.html#inla-stan-and-bugs",
    "href": "bayesian-analysis.html#inla-stan-and-bugs",
    "title": "11  Bayesian Analysis",
    "section": "11.3 INLA, Stan and BUGS",
    "text": "11.3 INLA, Stan and BUGS\nIn the example here, the GBD 2016 life time risk of stroke is re-used. The model is shown next to the simple linear regression. INLA is used to infer the probability of a set of data given the defined parameters. In the regression example below the linear regression return coefficient of 0.788 and the INLA version returns a mean value of 0.789 as well. Note that Bayesian methods do not provide p-values.\n\n11.3.1 Linear regression\n\nlibrary(INLA)\n\nLoading required package: Matrix\n\n\nLoading required package: sp\n\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\n\n\nThis is INLA_23.09.09 built 2023-10-16 17:29:11 UTC.\n - See www.r-inla.org/contact-us for how to get help.\n\nload(\"./Data-Use/world_stroke.Rda\")\n\n#perform ordinary linear regression for comparison\nfit&lt;-lm(LifeExpectancy~MeanLifetimeRisk, data =world_sfdf)\nsummary(fit)\n\n\nCall:\nlm(formula = LifeExpectancy ~ MeanLifetimeRisk, data = world_sfdf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5141  -4.8413   0.1315   5.3673  10.5146 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      56.27490    1.59843   35.21   &lt;2e-16 ***\nMeanLifetimeRisk  0.78852    0.07655   10.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.223 on 173 degrees of freedom\n  (71 observations deleted due to missingness)\nMultiple R-squared:  0.3802,    Adjusted R-squared:  0.3766 \nF-statistic: 106.1 on 1 and 173 DF,  p-value: &lt; 2.2e-16\n\n\nHere the output of Bayesian analysis contrasts with the frequentist output above.\n\n#need to subset data as world_sfdf is of class \"sf\"         \"tbl_df\"     \"tbl\" \"data.frame\"\nfitINLA&lt;-inla(LifeExpectancy~MeanLifetimeRisk, family = \"gaussian\", data =world_sfdf[,c(23,12)])\n\nsummary(fitINLA)\n\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" keep = keep, working.directory = working.directory, \n   silent = silent, \", \" inla.mode = inla.mode, safe = FALSE, debug = \n   debug, .parent.frame = .parent.frame)\" ) \nTime used:\n    Pre = 0.566, Running = 0.391, Post = 0.0458, Total = 1 \nFixed effects:\n                   mean    sd 0.025quant 0.5quant 0.975quant   mode kld\n(Intercept)      56.275 1.595     53.143   56.275     59.407 56.275   0\nMeanLifetimeRisk  0.789 0.076      0.639    0.789      0.938  0.789   0\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.026 0.003      0.021    0.026\n                                        0.975quant  mode\nPrecision for the Gaussian observations      0.032 0.026\n\nMarginal log-Likelihood:  -587.97 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nPerforming Bayesian analysis with rstan can be tricky as the text file for the stan code require an empty line at the end. The brms library contains the same precompiled stan code and is easier to run.\n\n#This may not run if there is incompatibility between Rtools and version of R\n#check  system(\"g++ -v\")\nlibrary(brms)\n\nfitBRM&lt;-brm(LifeExpectancy~MeanLifetimeRisk,  data =world_sfdf[,c(23,12)])\n\nsummary(fitBRM)\n\nExtract the posterior samples for plotting.\n\n\n11.3.2 Logistic regression\nThis example illustrates the use of INLA for logistic regression.\n\n#library(INLA)\ndata(\"BreastCancer\",package = \"mlbench\")\ncolnames(BreastCancer)\n\n [1] \"Id\"              \"Cl.thickness\"    \"Cell.size\"       \"Cell.shape\"     \n [5] \"Marg.adhesion\"   \"Epith.c.size\"    \"Bare.nuclei\"     \"Bl.cromatin\"    \n [9] \"Normal.nucleoli\" \"Mitoses\"         \"Class\"          \n\n#note Class is benign or malignant of class factor\n#need to convert this to numeric values\n\n\n#first convert to character\nBreastCancer$Class&lt;-as.character(BreastCancer$Class)\n\nBreastCancer$Class[BreastCancer$Class==\"benign\"]&lt;-0\nBreastCancer$Class[BreastCancer$Class==\"malignant\"]&lt;-1\n\n\n\n#convert factors to numeric\nBreastCancer2&lt;-lapply(BreastCancer[,-c(1,7)], as.numeric)\nBreastCancer2&lt;-as.data.frame(BreastCancer2)\n\n#return Class to data\n#convert character back to numeric\nBreastCancer2$Class&lt;-as.numeric(BreastCancer$Class)\n\nDx&lt;-inla(Class ~Epith.c.size+Cl.thickness+Cell.shape, family=\"binomial\", \n         data = BreastCancer2)\n\nsummary(Dx)\n\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" keep = keep, working.directory = working.directory, \n   silent = silent, \", \" inla.mode = inla.mode, safe = FALSE, debug = \n   debug, .parent.frame = .parent.frame)\" ) \nTime used:\n    Pre = 0.383, Running = 0.527, Post = 0.0541, Total = 0.964 \nFixed effects:\n               mean    sd 0.025quant 0.5quant 0.975quant   mode kld\n(Intercept)  -8.496 0.709     -9.886   -8.496     -7.106 -8.496   0\nEpith.c.size  0.483 0.123      0.242    0.483      0.724  0.483   0\nCl.thickness  0.644 0.094      0.459    0.644      0.829  0.644   0\nCell.shape    0.941 0.121      0.703    0.941      1.178  0.941   0\n\nMarginal log-Likelihood:  -114.89 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nThe same data is used for logistic regression with stan.\n\nDxBRM&lt;-brm(Class ~Epith.c.size+Cl.thickness+Cell.shape, \n           family=bernoulli(link = \"logit\"), data = BreastCancer2)\n\nsummary(DxBRM)\n\nExtract the posterior sample from logistic regression.\n\npost_samples_DxBRM &lt;- brms::posterior_samples(DxBRM)\n\npost_samples_DxBRM %&gt;% \n  select(-lp__) %&gt;% \n  pivot_longer(cols = everything()) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_density() +\n  facet_wrap(~name, scales = \"free\")\n\n\n\n11.3.3 Mixed model\nPlotting world_sfdf to identify characteristics of the data\n\nlibrary(ggplot2)\nggplot(data=world_sfdf, aes(x=LifeExpectancy, y=MeanLifetimeRisk,\n              color=Income, shape=Continent))+                                               \n  geom_point()+\n  geom_jitter()\n\n\n\n\n\n\n\n\nIntercept model with INLA\n\n# Set prior on precision\nprec.prior &lt;- list(prec = list(param = c(0.001, 0.001)))\n\nInla_Income&lt;-inla(MeanLifetimeRisk~1+LifeExpectancy+f(Income,  \n    model = \"iid\",\n    hyper = prec.prior), \n    data =world_sfdf[,c(23,12, 15, 20)], \n    control.predictor = list(compute = TRUE))\n\nsummary(Inla_Income)\n\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" keep = keep, working.directory = working.directory, \n   silent = silent, \", \" inla.mode = inla.mode, safe = FALSE, debug = \n   debug, .parent.frame = .parent.frame)\" ) \nTime used:\n    Pre = 0.45, Running = 0.649, Post = 0.0788, Total = 1.18 \nFixed effects:\n                 mean    sd 0.025quant 0.5quant 0.975quant   mode   kld\n(Intercept)    17.948 2.190     13.536   17.968     22.250 17.966 0.001\nLifeExpectancy  0.027 0.019     -0.010    0.027      0.064  0.027 0.000\n\nRandom effects:\n  Name    Model\n    Income IID model\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.047 0.004      0.039    0.047\nPrecision for Income                    0.097 0.089      0.012    0.071\n                                        0.975quant  mode\nPrecision for the Gaussian observations      0.056 0.046\nPrecision for Income                         0.332 0.033\n\nMarginal log-Likelihood:  -760.68 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nIntercept model with BRMS\n\nBRMS_Income &lt;- brm(MeanLifetimeRisk ~ 1 + LifeExpectancy+(1 | Income), \n                              data   = world_sfdf, \n                              warmup = 100, \n                              iter   = 200, \n                              chains = 2, \n                              inits  = \"random\",\n                              cores  = 2)  #use 2 CPU cores simultaneously instead of just 1.\n\nsummary(BRMS_Income)\n\n\npost_samples_BRMS_Income &lt;- brms::posterior_samples(BRMS_Income)\n\npost_samples_BRMS_Income %&gt;% \n  select(-lp__) %&gt;% \n  pivot_longer(cols = everything()) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_density() +\n  facet_wrap(~name, scales = \"free\")\n\n\n\n11.3.4 Bayesian Metaanalysis\nA Bayesian approach towards metaanalysis is provided below using the package meta4diag (Guo and Riebler 2015). This approach uses the Integrated Nested Laplacian Approximations (INLA). This package takes a has an advantage over the mada package which does not provide a bivariate method for performing summary sensitivity and specificity.\n\n#install.packages(\"INLA\",repos=c(getOption(\"repos\"),INLA=\"https://inla.r-inla-download.org/R/stable\"), dep=TRUE)\nlibrary(meta4diag)\n\nLoading required package: grid\n\n\nLoading required package: shiny\n\n\nLoading required package: shinyBS\n\n\nLoading required package: caTools\n\nlibrary(INLA)\n\n#data from spot sign metaanalysis\nDat&lt;-read.csv(\"./Data-Use/ss150718.csv\")\n\n#remove duplicates\ndat&lt;-subset(Dat, Dat$retain==\"yes\") \n\n#the data can be viewed under res$data\nres &lt;- meta4diag(data = dat) \n\n#perform SROC\nSROC(res, crShow = T)\n\n\n\n\n\n\n\n\nForest plot of sensitivity using the meta4diag package.\n\n#sensitivity is specified under accuracy.type\n#note that there are several different forest plot: mada, metafor, meta4diag\nmeta4diag::forest(res, accuracy.type=\"sens\", est.type=\"mean\", p.cex=\"scaled\", p.pch=15, p.col=\"black\",\n    nameShow=\"right\", dataShow=\"center\", estShow=\"left\", text.cex=1,\n    shade.col=\"gray\", arrow.col=\"black\", arrow.lty=1, arrow.lwd=1,\n    cut=TRUE, intervals=c(0.025,0.975),\n    main=\"Forest plot of Sensitivity\", main.cex=1.5, axis.cex=1)\n\n\n\n\n\n\n\n\nForest plot of specificity using the meta4diag package.\n\n#specificity is specified under accuracy.type\n#note that there are several different forest plot: mada, metafor, meta4diag\nmeta4diag::forest(res, accuracy.type=\"spec\", est.type=\"mean\", p.cex=\"scaled\", p.pch=15, p.col=\"black\",\n    nameShow=\"right\", dataShow=\"center\", estShow=\"left\", text.cex=1,\n    shade.col=\"gray\", arrow.col=\"black\", arrow.lty=1, arrow.lwd=1,\n    cut=TRUE, intervals=c(0.025,0.975),\n    main=\"Forest plot of Specificity\", main.cex=1.5, axis.cex=1)\n\n\n\n\n\n\n\n\n\n\n11.3.5 Cost\nThe example below is provided by hesim.\n\nlibrary(\"hesim\")\n\n\nAttaching package: 'hesim'\n\n\nThe following object is masked from 'package:Matrix':\n\n    expand\n\nlibrary(\"data.table\")\nstrategies &lt;- data.table(strategy_id = c(1, 2))\nn_patients &lt;- 1000\npatients &lt;- data.table(patient_id = 1:n_patients,\n          age = rnorm(n_patients, mean = 70, sd = 10),\n          female = rbinom(n_patients, size = 1, prob = .4))\nstates &lt;- data.table(state_id = c(1, 2),\n                     state_name = c(\"Healthy\", \"Sick\")) \n# Non-death health states\ntmat &lt;- rbind(c(NA, 1, 2),\n              c(3, NA, 4),\n              c(NA, NA, NA))\ncolnames(tmat) &lt;- rownames(tmat) &lt;- c(\"Healthy\", \"Sick\", \"Dead\")\ntransitions &lt;- create_trans_dt(tmat)\ntransitions[, trans := factor(transition_id)]\nhesim_dat &lt;- hesim_data(strategies = strategies,\n                        patients = patients, \n                        states = states,\n                        transitions = transitions)\nprint(hesim_dat)\n\n$strategies\n   strategy_id\n1:           1\n2:           2\n\n$patients\n      patient_id      age female\n   1:          1 73.31048      0\n   2:          2 90.66601      0\n   3:          3 71.06716      1\n   4:          4 77.01498      1\n   5:          5 62.29037      1\n  ---                           \n 996:        996 56.70158      0\n 997:        997 63.26972      1\n 998:        998 62.28144      0\n 999:        999 63.38752      0\n1000:       1000 61.63177      0\n\n$states\n   state_id state_name\n1:        1    Healthy\n2:        2       Sick\n\n$transitions\n   transition_id from to from_name to_name trans\n1:             1    1  2   Healthy    Sick     1\n2:             2    1  3   Healthy    Dead     2\n3:             3    2  1      Sick Healthy     3\n4:             4    2  3      Sick    Dead     4\n\nattr(,\"class\")\n[1] \"hesim_data\"\n\n\n\n\n\n\nGuo, Jingyi, and Andrea Riebler. 2015. “meta4diag: Bayesian Bivariate Meta-analysis of Diagnostic Test Studies for Routine Practice.” arXiv e-Prints, December, arXiv:1512.06220. https://arxiv.org/abs/1512.06220.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "operational-research.html",
    "href": "operational-research.html",
    "title": "12  Operational Research",
    "section": "",
    "text": "12.1 Queueing theory\nQueueing theory describes the movement of a queue such as customer arrival in bank, shop or emergency department. It seeks to balance supply and demand for a service. It begun with the study queue waiting on Danish telephones in 1909.\nLittle’s theorem describes the linear relationship between the number of customer L to the customer arrival rate \\(\\lambda\\) and the customer served per time peiod, \\(\\mu\\). This can also be used to determine the number of beds needed for coronary care unit given 4 patients being admitted to cardiology unit, one of whom needs to be admitted to coronary care unit and would stay for an average of 3 days.\nQueueing system is described in terms of Kendall’s notation, M/M/c/k, with exponential arrival time. Using this terminology, MM1 system has 1 server and infinite queue. An MM/2/3 system has 2 (c) servers and 1 (k-c) position in the queue. The M refers to Markov chain.\nAn example of a single server providing full service is a car wash. Example of a single multiphase server include different single stations in bank of withdrawing, deposit, information A counter at the airport or train station for economy and business passengers is considered multiserver single phase queue. A laundromat with different queues for washing and drying is an example of multiphase multiservers.\nA traditional queue at a shop can also be seen as first in first out with the first customer served first and leave first. An issue with FIFO is that people may queue early such as overnight queue for the latest iPhone. Alternatives include last in first out queue and priority queueing in emergency department.\nLet’s create a simple queue with 2 customers arriving per minute and 3 customers served per minute. The PO is the probability that the server is idle.\nlibrary(queueing)\n\nlambda &lt;- 2 # 2 customers arriving per minute\nmu &lt;- 3 # 3 customers served per minute\n\n# MM1 \n\nmm1 &lt;- NewInput.MM1(lambda = 2, mu = 3, n = 0)\n\n# Create queue class object\nmm1_out &lt;- QueueingModel(mm1)\n\n# Report\nReport(mm1_out)\n\nThe inputs of the M/M/1 model are:\nlambda: 2, mu: 3, n: 0\n\nThe outputs of the M/M/1 model are:\n\nThe probability (p0, p1, ..., pn) of the n = 0 clients in the system are:\n0.3333333\nThe traffic intensity is: 0.666666666666667\nThe server use is: 0.666666666666667\nThe mean number of clients in the system is: 2\nThe mean number of clients in the queue is: 1.33333333333333\nThe mean number of clients in the server is: 0.666666666666667\nThe mean time spend in the system is: 1\nThe mean time spend in the queue is: 0.666666666666667\nThe mean time spend in the server is: 0.333333333333333\nThe mean time spend in the queue when there is queue is: 1\nThe throughput is: 2\n\n# Summary\nsummary(mm1_out)\n\n  lambda mu c  k  m        RO        P0       Lq        Wq X L W Wqq Lqq\n1      2  3 1 NA NA 0.6666667 0.3333333 1.333333 0.6666667 2 2 1   1   3\nLets examine M/M/3 queue with exponential inter-arrival times, exponential service times and 3 servers.\nlibrary(queuecomputer)\n\nn &lt;- 100\narrivals &lt;- cumsum(rexp(n, 1.9))\nservice &lt;- rexp(n)\n\nmm3 &lt;- queue_step(arrivals = arrivals, service = service, servers = 3)\nPlot the arrival and departure times\nplot(mm3)[1]\n\n[[1]]\nPlot waiting time\nplot(mm3)[2]\n\n[[1]]\nPlot customer in queue\nplot(mm3)[3]\n\n[[1]]\nPlot customer and server status\nplot(mm3)[4]\n\n[[1]]\nPlot arrival and departure time\nplot(mm3)[5]\n\n[[1]]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Operational Research</span>"
    ]
  },
  {
    "objectID": "operational-research.html#queueing-theory",
    "href": "operational-research.html#queueing-theory",
    "title": "12  Operational Research",
    "section": "",
    "text": "{r operational-research-1-1, warning=F}\n\ncurve(dpois(x, mm1$lambda),\n      from = 0, \n      to = 20, \n      type = \"b\", \n      lwd = 2,\n      xlab = \"Number of customers\",\n      ylab = \"Probability\",\n      main = \"Poisson Distribution for Arrival Process\",\n      ylim = c(0, 0.4),\n      n = 21)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Operational Research</span>"
    ]
  },
  {
    "objectID": "operational-research.html#discrete-event-simulations",
    "href": "operational-research.html#discrete-event-simulations",
    "title": "12  Operational Research",
    "section": "12.2 Discrete Event Simulations",
    "text": "12.2 Discrete Event Simulations\nDiscrete event simulation can be consider as modeling a complexity of system with multiple processes over time. This is different from continuous modeling of a system which evolve continuously with time. Discrete event simulation can be apply to the study of queue such as bank teller with a first in first out system.\n\n12.2.1 Simulate capacity of system\nThe example below is a based on examples provided in the simmer website for laundromat.\n\nlibrary(simmer)\nlibrary(parallel)\nlibrary(simmer.plot)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'simmer.plot'\n\n\nThe following objects are masked from 'package:simmer':\n\n    get_mon_arrivals, get_mon_attributes, get_mon_resources\n\nNUM_ANGIO &lt;- 1  # Number of machines for performing ECR\nECRTIME &lt;- 1     # hours it takes to perform ECR~ 90/60\nT_INTER &lt;- 13 # new patient every ~365*24/700 hours\nSIM_TIME &lt;- 24*30     # Simulation time over 30 days\n\n# setup\nset.seed(42)\nenv &lt;- simmer()\npatient &lt;- trajectory() %&gt;%\n  log_(\"arrives at the ECR\") %&gt;%\n  seize(\"removeclot\", 1) %&gt;%\n  log_(\"enters the ECR\") %&gt;%\n  timeout(ECRTIME) %&gt;%\n  set_attribute(\"clot_removed\", function() sample(50:99, 1)) %&gt;%\n  log_(function() \n    paste0(get_attribute(env, \"clot_removed\"), \"% of clot was removed\")) %&gt;%\n  release(\"removeclot\", 1) %&gt;%\n  log_(\"leaves the ECR\")\nenv %&gt;%\n  add_resource(\"removeclot\", NUM_ANGIO) %&gt;%\n  # feed the trajectory with 4 initial patients\n  add_generator(\"patient_initial\", patient, at(rep(0, 4))) %&gt;%\n  # new patient approx. every T_INTER minutes\n  add_generator(\"patient\", patient, function() sample((T_INTER-2):(T_INTER+2), 1)) %&gt;%\n  # start the simulation\n  run(SIM_TIME)\n\n0: patient_initial0: arrives at the ECR\n0: patient_initial0: enters the ECR\n0: patient_initial1: arrives at the ECR\n0: patient_initial2: arrives at the ECR\n0: patient_initial3: arrives at the ECR\n1: patient_initial0: 86% of clot was removed\n1: patient_initial0: leaves the ECR\n1: patient_initial1: enters the ECR\n2: patient_initial1: 50% of clot was removed\n2: patient_initial1: leaves the ECR\n2: patient_initial2: enters the ECR\n3: patient_initial2: 74% of clot was removed\n3: patient_initial2: leaves the ECR\n3: patient_initial3: enters the ECR\n4: patient_initial3: 59% of clot was removed\n4: patient_initial3: leaves the ECR\n11: patient0: arrives at the ECR\n11: patient0: enters the ECR\n12: patient0: 67% of clot was removed\n12: patient0: leaves the ECR\n25: patient1: arrives at the ECR\n25: patient1: enters the ECR\n26: patient1: 98% of clot was removed\n26: patient1: leaves the ECR\n37: patient2: arrives at the ECR\n37: patient2: enters the ECR\n38: patient2: 74% of clot was removed\n38: patient2: leaves the ECR\n51: patient3: arrives at the ECR\n51: patient3: enters the ECR\n52: patient3: 95% of clot was removed\n52: patient3: leaves the ECR\n66: patient4: arrives at the ECR\n66: patient4: enters the ECR\n67: patient4: 75% of clot was removed\n67: patient4: leaves the ECR\n80: patient5: arrives at the ECR\n80: patient5: enters the ECR\n81: patient5: 96% of clot was removed\n81: patient5: leaves the ECR\n92: patient6: arrives at the ECR\n92: patient6: enters the ECR\n93: patient6: 90% of clot was removed\n93: patient6: leaves the ECR\n105: patient7: arrives at the ECR\n105: patient7: enters the ECR\n106: patient7: 76% of clot was removed\n106: patient7: leaves the ECR\n116: patient8: arrives at the ECR\n116: patient8: enters the ECR\n117: patient8: 86% of clot was removed\n117: patient8: leaves the ECR\n130: patient9: arrives at the ECR\n130: patient9: enters the ECR\n131: patient9: 54% of clot was removed\n131: patient9: leaves the ECR\n145: patient10: arrives at the ECR\n145: patient10: enters the ECR\n146: patient10: 83% of clot was removed\n146: patient10: leaves the ECR\n159: patient11: arrives at the ECR\n159: patient11: enters the ECR\n160: patient11: 89% of clot was removed\n160: patient11: leaves the ECR\n173: patient12: arrives at the ECR\n173: patient12: enters the ECR\n174: patient12: 82% of clot was removed\n174: patient12: leaves the ECR\n186: patient13: arrives at the ECR\n186: patient13: enters the ECR\n187: patient13: 73% of clot was removed\n187: patient13: leaves the ECR\n198: patient14: arrives at the ECR\n198: patient14: enters the ECR\n199: patient14: 64% of clot was removed\n199: patient14: leaves the ECR\n211: patient15: arrives at the ECR\n211: patient15: enters the ECR\n212: patient15: 57% of clot was removed\n212: patient15: leaves the ECR\n223: patient16: arrives at the ECR\n223: patient16: enters the ECR\n224: patient16: 53% of clot was removed\n224: patient16: leaves the ECR\n237: patient17: arrives at the ECR\n237: patient17: enters the ECR\n238: patient17: 94% of clot was removed\n238: patient17: leaves the ECR\n249: patient18: arrives at the ECR\n249: patient18: enters the ECR\n250: patient18: 54% of clot was removed\n250: patient18: leaves the ECR\n263: patient19: arrives at the ECR\n263: patient19: enters the ECR\n264: patient19: 83% of clot was removed\n264: patient19: leaves the ECR\n277: patient20: arrives at the ECR\n277: patient20: enters the ECR\n278: patient20: 84% of clot was removed\n278: patient20: leaves the ECR\n289: patient21: arrives at the ECR\n289: patient21: enters the ECR\n290: patient21: 75% of clot was removed\n290: patient21: leaves the ECR\n300: patient22: arrives at the ECR\n300: patient22: enters the ECR\n301: patient22: 55% of clot was removed\n301: patient22: leaves the ECR\n312: patient23: arrives at the ECR\n312: patient23: enters the ECR\n313: patient23: 52% of clot was removed\n313: patient23: leaves the ECR\n324: patient24: arrives at the ECR\n324: patient24: enters the ECR\n325: patient24: 51% of clot was removed\n325: patient24: leaves the ECR\n339: patient25: arrives at the ECR\n339: patient25: enters the ECR\n340: patient25: 59% of clot was removed\n340: patient25: leaves the ECR\n351: patient26: arrives at the ECR\n351: patient26: enters the ECR\n352: patient26: 82% of clot was removed\n352: patient26: leaves the ECR\n366: patient27: arrives at the ECR\n366: patient27: enters the ECR\n367: patient27: 88% of clot was removed\n367: patient27: leaves the ECR\n377: patient28: arrives at the ECR\n377: patient28: enters the ECR\n378: patient28: 94% of clot was removed\n378: patient28: leaves the ECR\n391: patient29: arrives at the ECR\n391: patient29: enters the ECR\n392: patient29: 58% of clot was removed\n392: patient29: leaves the ECR\n403: patient30: arrives at the ECR\n403: patient30: enters the ECR\n404: patient30: 61% of clot was removed\n404: patient30: leaves the ECR\n418: patient31: arrives at the ECR\n418: patient31: enters the ECR\n419: patient31: 58% of clot was removed\n419: patient31: leaves the ECR\n432: patient32: arrives at the ECR\n432: patient32: enters the ECR\n433: patient32: 84% of clot was removed\n433: patient32: leaves the ECR\n445: patient33: arrives at the ECR\n445: patient33: enters the ECR\n446: patient33: 65% of clot was removed\n446: patient33: leaves the ECR\n460: patient34: arrives at the ECR\n460: patient34: enters the ECR\n461: patient34: 77% of clot was removed\n461: patient34: leaves the ECR\n475: patient35: arrives at the ECR\n475: patient35: enters the ECR\n476: patient35: 77% of clot was removed\n476: patient35: leaves the ECR\n490: patient36: arrives at the ECR\n490: patient36: enters the ECR\n491: patient36: 67% of clot was removed\n491: patient36: leaves the ECR\n502: patient37: arrives at the ECR\n502: patient37: enters the ECR\n503: patient37: 67% of clot was removed\n503: patient37: leaves the ECR\n513: patient38: arrives at the ECR\n513: patient38: enters the ECR\n514: patient38: 95% of clot was removed\n514: patient38: leaves the ECR\n528: patient39: arrives at the ECR\n528: patient39: enters the ECR\n529: patient39: 85% of clot was removed\n529: patient39: leaves the ECR\n543: patient40: arrives at the ECR\n543: patient40: enters the ECR\n544: patient40: 85% of clot was removed\n544: patient40: leaves the ECR\n554: patient41: arrives at the ECR\n554: patient41: enters the ECR\n555: patient41: 67% of clot was removed\n555: patient41: leaves the ECR\n566: patient42: arrives at the ECR\n566: patient42: enters the ECR\n567: patient42: 62% of clot was removed\n567: patient42: leaves the ECR\n579: patient43: arrives at the ECR\n579: patient43: enters the ECR\n580: patient43: 68% of clot was removed\n580: patient43: leaves the ECR\n594: patient44: arrives at the ECR\n594: patient44: enters the ECR\n595: patient44: 78% of clot was removed\n595: patient44: leaves the ECR\n608: patient45: arrives at the ECR\n608: patient45: enters the ECR\n609: patient45: 93% of clot was removed\n609: patient45: leaves the ECR\n619: patient46: arrives at the ECR\n619: patient46: enters the ECR\n620: patient46: 70% of clot was removed\n620: patient46: leaves the ECR\n630: patient47: arrives at the ECR\n630: patient47: enters the ECR\n631: patient47: 97% of clot was removed\n631: patient47: leaves the ECR\n643: patient48: arrives at the ECR\n643: patient48: enters the ECR\n644: patient48: 87% of clot was removed\n644: patient48: leaves the ECR\n658: patient49: arrives at the ECR\n658: patient49: enters the ECR\n659: patient49: 62% of clot was removed\n659: patient49: leaves the ECR\n669: patient50: arrives at the ECR\n669: patient50: enters the ECR\n670: patient50: 58% of clot was removed\n670: patient50: leaves the ECR\n684: patient51: arrives at the ECR\n684: patient51: enters the ECR\n685: patient51: 92% of clot was removed\n685: patient51: leaves the ECR\n696: patient52: arrives at the ECR\n696: patient52: enters the ECR\n697: patient52: 91% of clot was removed\n697: patient52: leaves the ECR\n708: patient53: arrives at the ECR\n708: patient53: enters the ECR\n709: patient53: 78% of clot was removed\n709: patient53: leaves the ECR\n719: patient54: arrives at the ECR\n719: patient54: enters the ECR\n\n\nsimmer environment: anonymous | now: 720 | next: 720\n{ Monitor: in memory }\n{ Resource: removeclot | monitored: TRUE | server status: 1(1) | queue status: 0(Inf) }\n{ Source: patient_initial | monitored: 1 | n_generated: 4 }\n{ Source: patient | monitored: 1 | n_generated: 56 }\n\n\nPlot the schematics of the simulation.\n\nplot(patient)\n\n\n\n\n\nPlot resource usage\n\nresource &lt;- get_mon_resources(env)\nplot(resource)\n\n\n\n\n\n\n\n\nTotal queue size\n\nsum(resource$queue) #total queue size\n\n[1] 9\n\n\nNumber of people in queue of size 1\n\nsum(resource$queue==1) #number of people in queue\n\n[1] 2\n\n\nNumber of peope in queue of size 2\n\nsum(resource$queue==2)\n\n[1] 2\n\n\nPlot arrival versus flow\n\n#View(get_mon_arrivals(env))\nplot(env,what=\"arrivals\",metric=\"flow_time\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext we simulate the process of running a stroke code. In this simulation the activities occur sequentially.\n\n#simulate ST6\npatient &lt;- trajectory(\"patients' path\") %&gt;%\n  ## add an intake activity - nurse arrive first on scene in ED to triage\n  #seize specify priority\n  seize(\"ed nurse\", 1) %&gt;%\n  #timeout return one random value from 5 mean+/-5 SD \n  timeout(function() rnorm(1,5,5)) %&gt;%\n  release(\"ed nurse\", 1) %&gt;%\n  \n  ## add a registrar activity - stroke registrar arrive after stroke code\n  seize(\"stroke reg\", 1) %&gt;%\n  timeout(function() rnorm(1, 10,5)) %&gt;%\n  release(\"stroke reg\", 1) %&gt;%\n  \n  #add CT scanning - the process takes 15 minutes\n  seize(\"CT scan\", 1) %&gt;%\n  timeout(function() rnorm(1, 15,15)) %&gt;%\n  release(\"CT scan\", 1) %&gt;%\n  \n    #stroke reg re-enter\n    #seize(\"stroke reg\", 1) %&gt;%\n    #timeout(function() rnorm(1, 10,5)) %&gt;%\n    #release(\"stroke reg\", 1) %&gt;%\n  #branch\n  \n  ## add stroke consultant - to review scan and makes decision\n  \n  seize(\"stroke consultant\", 1) %&gt;%\n  timeout(function() rnorm(1, 5,10)) %&gt;%\n  release(\"stroke consultant\", 1) %&gt;%\n  \n  ## add a thrombectomy decision activity\n  seize(\"inr\", 1) %&gt;%\n  timeout(function() rnorm(1, 5,5)) %&gt;%\n  release(\"inr\", 1) \n\nenvs &lt;- mclapply(1:100, function(i) {\n  simmer(\"SuperDuperSim\") %&gt;%\n    add_resource(\"ed nurse\", 1) %&gt;%\n    add_resource(\"stroke reg\", 1) %&gt;%\n    add_resource(\"CT scan\", 1) %&gt;%\n    add_resource(\"stroke consultant\", 1) %&gt;%\n    add_resource(\"inr\", 1) %&gt;%\n    add_generator(\"patient\", patient, function() rnorm(1, 10, 2)) %&gt;%\n    run(100) %&gt;%\n    wrap()\n})\n\nPlot patient flow\n\nplot(patient)\n\n\n\n\n\n\n#plot.simmer\nresources &lt;- get_mon_resources(envs)\n\n#\np1&lt;-plot(resources, metric = \"usage\", \n         c(\"ed nurse\",\"stroke reg\",\"CT scan\", \"stroke                         consultant\",\"inr\"), \n         items = \"serve\")\n\n#resource usage\np2&lt;-plot(get_mon_resources(envs[[6]]), metric = \"usage\", \"stroke consultant\", items = \"server\", steps = TRUE)\n\n#resource utilisation\np3&lt;-plot(resources, metric=\"utilization\", c(\"ed nurse\", \"stroke reg\",\"CT scan\"))\n\n#Flow time evolution\narrivals &lt;- get_mon_arrivals(envs)\np4&lt;-plot(arrivals, metric = \"flow_time\")\n\n#combine plot\ngridExtra::grid.arrange(p1,p2, p3,p4)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n#\nactivity&lt;-get_mon_arrivals(envs)\n\nplot(activity,metric=\"activity_time\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow we will fork another path in the patient flow\n\n\n12.2.2 Queuing network\n\nmean_pkt_size &lt;- 100        # bytes\nlambda1 &lt;- 2                # pkts/s\nlambda3 &lt;- 0.5              # pkts/s\nlambda4 &lt;- 0.6              # pkts/s\nrate &lt;- 2.2 * mean_pkt_size # bytes/s\n\n# set an exponential message size of mean mean_pkt_size\nset_msg_size &lt;- function(.)\n  set_attribute(., \"size\", function() rexp(1, 1/mean_pkt_size))\n\n# seize an M/D/1 queue by id; the timeout is function of the message size\nmd1 &lt;- function(., id)\n  seize(., paste0(\"md1_\", id), 1) %&gt;%\n  timeout(function() get_attribute(env, \"size\") / rate) %&gt;%\n  release(paste0(\"md1_\", id), 1)\n\n\nto_queue_1 &lt;- trajectory() %&gt;%\n  set_msg_size() %&gt;%\n  md1(1) %&gt;%\n  leave(0.25) %&gt;%\n  md1(2) %&gt;%\n  branch(\n    function() (runif(1) &gt; 0.65) + 1, continue=c(F, F),\n    trajectory() %&gt;% md1(3),\n    trajectory() %&gt;% md1(4)\n  )\n\nto_queue_3 &lt;- trajectory() %&gt;%\n  set_msg_size() %&gt;%\n  md1(3)\n\nto_queue_4 &lt;- trajectory() %&gt;%\n  set_msg_size() %&gt;%\n  md1(4)\n\n\nenv &lt;- simmer()\nfor (i in 1:4) env %&gt;% \n  add_resource(paste0(\"md1_\", i))\nenv %&gt;%\n  add_generator(\"arrival1_\", to_queue_1, function() rexp(1, lambda1), mon=2) %&gt;%\n  add_generator(\"arrival3_\", to_queue_3, function() rexp(1, lambda3), mon=2) %&gt;%\n  add_generator(\"arrival4_\", to_queue_4, function() rexp(1, lambda4), mon=2) %&gt;%\n  run(4000)\n\nsimmer environment: anonymous | now: 4000 | next: 4000.01785654125\n{ Monitor: in memory }\n{ Resource: md1_1 | monitored: TRUE | server status: 1(1) | queue status: 15(Inf) }\n{ Resource: md1_2 | monitored: TRUE | server status: 1(1) | queue status: 1(Inf) }\n{ Resource: md1_3 | monitored: TRUE | server status: 1(1) | queue status: 0(Inf) }\n{ Resource: md1_4 | monitored: TRUE | server status: 0(1) | queue status: 0(Inf) }\n{ Source: arrival1_ | monitored: 2 | n_generated: 8023 }\n{ Source: arrival3_ | monitored: 2 | n_generated: 2031 }\n{ Source: arrival4_ | monitored: 2 | n_generated: 2399 }\n\n\n\nres &lt;- get_mon_arrivals(env, per_resource = TRUE) %&gt;%\n  subset(resource %in% c(\"md1_3\", \"md1_4\"), select=c(\"name\", \"resource\"))\n\narr &lt;- get_mon_arrivals(env) %&gt;%\n  transform(waiting_time = end_time - (start_time + activity_time)) %&gt;%\n  transform(generator = regmatches(name, regexpr(\"arrival[[:digit:]]\", name))) %&gt;%\n  merge(res)\n\naggregate(waiting_time ~ generator + resource, arr, function(x) sum(x)/length(x))\n\n  generator resource waiting_time\n1  arrival1    md1_3    6.8729025\n2  arrival3    md1_3    0.8427541\n3  arrival1    md1_4    6.7479945\n4  arrival4    md1_4    0.4598295\n\nget_n_generated(env, \"arrival1_\") + get_n_generated(env, \"arrival4_\")\n\n[1] 10422\n\naggregate(waiting_time ~ generator + resource, arr, length)\n\n  generator resource waiting_time\n1  arrival1    md1_3         3824\n2  arrival3    md1_3         2030\n3  arrival1    md1_4         2188\n4  arrival4    md1_4         2398",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Operational Research</span>"
    ]
  },
  {
    "objectID": "operational-research.html#linear-programming",
    "href": "operational-research.html#linear-programming",
    "title": "12  Operational Research",
    "section": "12.3 Linear Programming",
    "text": "12.3 Linear Programming\nLinear programming is an optimisation process to maximise profit and minimise cost with multiple parts of the model having linear relationship. There are several different libraries useful for linear programming. The lpSolve library is used here as illustration.\n\nlibrary(lpSolve)\n\n#solve using linear programming\nn &lt;-2.5 # Numbers of techs. 1 EFT means a person is employed for 40 hours a week and 0.5 EFT means a person is employed for 20 hours a week.\nset_up_eeg &lt;- 40 # 40 minutes\nto_do_eeg &lt;- 30 # 30 minutes\nclean_equipment &lt;- 20 # 20 minutes\nannotate_eeg &lt;- 10 # 10 minutes\n\n# put some error for EEG time\n# if error=1 that mean NO errors happen\nerror &lt;- 0.8 #change from 0.93\n\n#Calculate time for EEG in hour\neeg_case_time &lt;- ((set_up_eeg+to_do_eeg+clean_equipment+annotate_eeg)/60)*error\n\n# limit for EEG per day\n# we can put different limits for EEGs\nlimit_eeg &lt;- round(8*eeg_case_time, digits = 0)\n\n#s[i] - numbers of cases for each i-EEG's machines \n#Setting the coefficients of s[i]-decision variables\n#In a future can put some efficiency or some cost\nobjective.in=c(1,1,1,1,1)\n\n#Constraint Matrix\nconst.mat=matrix(c(1,0,0,0,0,\n                   0,1,0,0,0,\n                   0,0,1,0,0,\n                   0,0,0,1,0,\n                   0,0,0,0,1,\n                   1,1,1,1,1),nrow = 6,byrow = T)\n\n#defining constraints\nconst_num_1=limit_eeg  #in cases\nconst_num_2=limit_eeg  #in cases\nconst_num_3=limit_eeg  #in cases\nconst_num_4=limit_eeg  #in cases\nconst_num_5=limit_eeg  #in cases\nconst_res= n*7 # limit per sessions\n\n#RHS for constraints\nconst.rhs=c(const_num_1,const_num_2,const_num_3,const_num_4,const_num_5, const_res)\n\n#Direction for constraints\nconstr.dir &lt;- rep(\"&lt;=\",6)\n\n#Finding the optimum solution\nopt=lp(direction = \"max\",objective.in,const.mat,constr.dir,const.rhs)\n#summary(opt)\n\n#Objective values of s[i]\n\nopt$solution \n\n[1] 11.0  6.5  0.0  0.0  0.0\n\n\nEstimate for day (Value of objective function at optimal point)\n\n\n[1] 17.5\n\n\nEstimate EEG per month based on staff EFT- only 2.5\n\n\n[1] 366\n\n\nAssuming that the time spend on a report by neurologists (1 report = 30 min) then in a 3.5 hour session a neurologist can report 7 EEG.\n\nneurologist_session=estimate_week/7\n\nneurologist_session\n\n[1] 13.07143",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Operational Research</span>"
    ]
  },
  {
    "objectID": "operational-research.html#forecasting",
    "href": "operational-research.html#forecasting",
    "title": "12  Operational Research",
    "section": "12.4 Forecasting",
    "text": "12.4 Forecasting\nForecasting is useful in predicting trends. In health care it can be used for estimating seasonal trends and bed requirement. Below is a forecast of mortality from COVID-19 in 2020. This forecast is an example and is not meant to be used in practice as mortality from COVID depends on the number of factors including infected cases, age, socioeconomic group, and comorbidity.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()       masks stats::filter()\n✖ dplyr::lag()          masks stats::lag()\n✖ lubridate::now()      masks simmer::now()\n✖ lubridate::rollback() masks simmer::rollback()\n✖ dplyr::select()       masks simmer::select()\n✖ tidyr::separate()     masks simmer::separate()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(prophet)\n\nLoading required package: Rcpp\nLoading required package: rlang\n\nAttaching package: 'rlang'\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\nlibrary(hrbrthemes)\nlibrary(lubridate)\nlibrary(readr) #use read_csv to read csv rather than base R\n\ncovid&lt;-read_csv(\"./Data-Use/Covid_Table100420.csv\") \n\nNew names:\nRows: 27 Columns: 12\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(8): ...1, Year, Week, COVID-19 Deaths...6, Pneumonia Deaths*...8, Infl... num\n(3): Total Deaths, COVID-19 Deaths...5, Pneumonia Deaths*...7 date (1): Date\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n• `COVID-19 Deaths` -&gt; `COVID-19 Deaths...5`\n• `COVID-19 Deaths` -&gt; `COVID-19 Deaths...6`\n• `Pneumonia Deaths*` -&gt; `Pneumonia Deaths*...7`\n• `Pneumonia Deaths*` -&gt; `Pneumonia Deaths*...8`\n• `Influenza Deaths` -&gt; `Influenza Deaths...9`\n• `Influenza Deaths` -&gt; `Influenza Deaths...10`\n\ncolnames(covid)\n\n [1] \"...1\"                  \"Year\"                  \"Week\"                 \n [4] \"Total Deaths\"          \"COVID-19 Deaths...5\"   \"COVID-19 Deaths...6\"  \n [7] \"Pneumonia Deaths*...7\" \"Pneumonia Deaths*...8\" \"Influenza Deaths...9\" \n[10] \"Influenza Deaths...10\" \"Total.Deaths\"          \"Date\"                 \n\n# A data frame with columns ds & y (datetimes & metrics)\ncovid&lt;-rename(covid, ds =Date, y=Total.Deaths)\ncovid2 &lt;- covid[c(1:12),]\n\nm&lt;-prophet(covid2)#create prophet object\n\nDisabling yearly seasonality. Run prophet with yearly.seasonality=TRUE to override this.\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\nn.changepoints greater than number of observations. Using 8\n\n# Extend dataframe 12 weeks into the future\nfuture &lt;- make_future_dataframe(m, freq=\"week\" , periods = 26)\n# Generate forecast for next 500 days\nforecast &lt;- predict(m, future)\n\n# What's the forecast for July 2020?\nforecasted_rides &lt;- forecast %&gt;%\n  arrange(desc(ds)) %&gt;%\n  dplyr::slice(1) %&gt;%\n  pull(yhat) %&gt;%\n  round()\nforecasted_rides\n\n[1] 67488\n\n# Visualize\nforecast_p &lt;- plot(m, forecast) + \n  labs(x = \"\", \n       y = \"mortality\", \n       title = \"Projected COVID-19 world mortality\", \n       subtitle = \"based on data truncated in January 2020\") +\n        ylim(20000,80000)+\n  theme_ipsum_rc()\n#forecast_p\n\n\n12.4.1 Bed requirement\n\n\n12.4.2 Length of stay\n\n\n12.4.3 Customer churns\nCustomer churns or turnover is an issue of interest in marketing. The corollary within healthcare is patients attendance at outpatient clinics, Insurance. The classical method used is GLM.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Operational Research</span>"
    ]
  },
  {
    "objectID": "operational-research.html#process-mapping",
    "href": "operational-research.html#process-mapping",
    "title": "12  Operational Research",
    "section": "12.5 Process mapping",
    "text": "12.5 Process mapping\n\nlibrary(DiagrammeR)\na.plot&lt;-mermaid(\"\n        graph TB\n        \n        A((Triage))\n        A--&gt;|2.3 hr|B(Imaging-No Stroke Code)\n        A--&gt;|0.6 hr|B1(Imaging-Stroke Code)\n        \n        B--&gt;|14.6 hr|B2(Dysphagia Screen)\n        B1--&gt;|no TPA 10.7 hr|B2(Dysphagia Screen)\n\n        C(Stop NBM)\n        B2--&gt;|0 hr|C\n      \n        C--&gt;|Oral route 1.7 hr|E{Antithrombotics}\n        \n        D1--&gt;|7.5 hr|E\n        B--&gt;|PR route 6.8 hr|E\n        B1--&gt;|PR route 3.8 hr|E\n    \n        B1--&gt;|TPA 24.7 hr|D1(Post TPA Scan)\n    \n        style A fill:#ADF, stroke:#333, stroke-width:2px\n        style B fill:#9AA, stroke:#333, stroke-width:2px\n        style B2 fill:#9AA, stroke:#333, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:2px\n        style C fill:#9AA, stroke:#333, stroke-width:2px\n        style D1 fill:#879, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n        \") \na.plot\n\n\n\n\n\n\nlibrary(bupaR)\n\n\nAttaching package: 'bupaR'\n\n\nThe following object is masked from 'package:simmer':\n\n    select\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Operational Research</span>"
    ]
  },
  {
    "objectID": "operational-research.html#supply-chains",
    "href": "operational-research.html#supply-chains",
    "title": "12  Operational Research",
    "section": "12.6 Supply chains",
    "text": "12.6 Supply chains",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Operational Research</span>"
    ]
  },
  {
    "objectID": "operational-research.html#health-economics",
    "href": "operational-research.html#health-economics",
    "title": "12  Operational Research",
    "section": "12.7 Health economics",
    "text": "12.7 Health economics\n\n12.7.1 Cost\n\nlibrary(\"hesim\")\n\n\nAttaching package: 'hesim'\n\n\nThe following object is masked from 'package:tidyr':\n\n    expand\n\nlibrary(\"data.table\")\n\n\nAttaching package: 'data.table'\n\n\nThe following object is masked from 'package:rlang':\n\n    :=\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nstrategies &lt;- data.table(strategy_id = c(1, 2))\nn_patients &lt;- 1000\npatients &lt;- data.table(patient_id = 1:n_patients,\n          age = rnorm(n_patients, mean = 70, sd = 10),\n          female = rbinom(n_patients, size = 1, prob = .4))\nstates &lt;- data.table(state_id = c(1, 2),\n                     state_name = c(\"Healthy\", \"Sick\")) \n# Non-death health states\ntmat &lt;- rbind(c(NA, 1, 2),\n              c(3, NA, 4),\n              c(NA, NA, NA))\ncolnames(tmat) &lt;- rownames(tmat) &lt;- c(\"Healthy\", \"Sick\", \"Dead\")\ntransitions &lt;- create_trans_dt(tmat)\ntransitions[, trans := factor(transition_id)]\nhesim_dat &lt;- hesim_data(strategies = strategies,\n                        patients = patients, \n                        states = states,\n                        transitions = transitions)\nprint(hesim_dat)\n\n$strategies\n   strategy_id\n1:           1\n2:           2\n\n$patients\n      patient_id      age female\n   1:          1 61.99932      1\n   2:          2 65.95321      0\n   3:          3 56.97051      0\n   4:          4 89.82190      1\n   5:          5 48.45879      0\n  ---                           \n 996:        996 73.50196      1\n 997:        997 81.33720      0\n 998:        998 52.63094      1\n 999:        999 80.48849      0\n1000:       1000 35.97193      0\n\n$states\n   state_id state_name\n1:        1    Healthy\n2:        2       Sick\n\n$transitions\n   transition_id from to from_name to_name trans\n1:             1    1  2   Healthy    Sick     1\n2:             2    1  3   Healthy    Dead     2\n3:             3    2  1      Sick Healthy     3\n4:             4    2  3      Sick    Dead     4\n\nattr(,\"class\")\n[1] \"hesim_data\"\n\n\nData from WHO on mortality rate can be extracted directly from WHO or by calling get_who_mr in heemod library.\n\nlibrary(heemod)\n\n\nAttaching package: 'heemod'\n\n\nThe following object is masked from 'package:purrr':\n\n    modify\n\n\nThe following object is masked from 'package:simmer':\n\n    join\n\n\nThere are several data in BCEA library such as Vaccine.\n\nlibrary(BCEA)\n\nThe BCEA version loaded is: 2.4.4\n\n\n\nAttaching package: 'BCEA'\n\n\nThe following object is masked from 'package:graphics':\n\n    contour\n\n#use Vaccine data from BCEA\ndata(Vaccine)\n\nints=c(\"Standard care\",\"Vaccination\")\n\n# Runs the health economic evaluation using BCEA\nm &lt;- bcea(\n      e=eff,\n      c=cost,               # defines the variables of \n                            #  effectiveness and cost\n      ref=2,                # selects the 2nd row of (e, c) \n                            #  as containing the reference intervention\n      interventions=treats, # defines the labels to be associated \n                            #  with each intervention\n      Kmax=50000,           # maximum value possible for the willingness \n                            #  to pay threshold; implies that k is chosen \n                            #  in a grid from the interval (0, Kmax)\n      plot=TRUE             # plots the results\n)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Operational Research</span>"
    ]
  },
  {
    "objectID": "geospatial-analysis.html",
    "href": "geospatial-analysis.html",
    "title": "13  Geospatial analysis",
    "section": "",
    "text": "13.1 Geocoding\nThere are several packages avalable for obtaining geocode or longitude and latitude of location. The tmaptools package provide free geocoding using OpenStreetMap or OSM overpass API. Both ggmap and googleway access Google Maps API and will require a key and payment for access.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Geospatial analysis</span>"
    ]
  },
  {
    "objectID": "geospatial-analysis.html#geocoding",
    "href": "geospatial-analysis.html#geocoding",
    "title": "13  Geospatial analysis",
    "section": "",
    "text": "13.1.1 OpenStreetMap\nThis is a simple example to obtain the geocode of Monash Medical Centre. However, such a simple example does not always work without the full address. There are other libraries for accessing other data from OSM such as parks, restaurants etc.\n\nlibrary(tmaptools)\nmmc&lt;-geocode_OSM (\"monash medical centre, clayton\")\nmmc\n\n$query\n[1] \"monash medical centre, clayton\"\n\n$coords\n        x         y \n145.12072 -37.92088 \n\n$bbox\n     xmin      ymin      xmax      ymax \n145.12067 -37.92094 145.12077 -37.92083 \n\n\nThe osmdata library includes function opg for extracting data from Overpass query. The list is available at https://wiki.openstreetmap.org/wiki/Map_features#Transportation.\n\nlibrary(tidyverse )\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(osmdata)\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\n\nNote that getbb now returns an error “HTTP 405 Method Not Allowed”. It seems that there is an error with getbb function.\n\n# build a query\nquery &lt;- getbb(bbox = \"Brisbane, QLD, australia\") %&gt;% opq() %&gt;%\n  add_osm_feature(key = \"amenity\", value = \"community_centre\")\n\nVector for bbox can be obtained using nominatimlite.\n\nnominatim_polygon &lt;- nominatimlite::geo_lite_sf(address = \"Fairfield, NSW, Australia\", points_only = FALSE)\nbboxsf &lt;- sf::st_bbox(nominatim_polygon)\nbboxsf\n\n     xmin      ymin      xmax      ymax \n150.81120 -33.91416 150.99060 -33.82074 \n\n\nExtracting data on community centre in Brisbane.\n\nquery &lt;- bboxsf %&gt;% opq() %&gt;%\n  add_osm_feature(key = \"amenity\", value = \"community_centre\")\nquery\n\n$bbox\n[1] \"-33.9141628,150.8112007,-33.8207365,150.9906033\"\n\n$prefix\n[1] \"[out:xml][timeout:25];\\n(\\n\"\n\n$suffix\n[1] \");\\n(._;&gt;;);\\nout body;\"\n\n$features\n[1] \"[\\\"amenity\\\"=\\\"community_centre\\\"]\"\n\n$osm_types\n[1] \"node\"     \"way\"      \"relation\"\n\nattr(,\"class\")\n[1] \"list\"           \"overpass_query\"\nattr(,\"nodes_only\")\n[1] FALSE\n\n\nExtracting data on fast food eateries in Brisbane.\n\nquery_FF &lt;- bboxsf %&gt;% opq() %&gt;%\n  add_osm_feature(key = \"amenity\", value = \"fast_food\")\nquery_FF\n\n$bbox\n[1] \"-33.9141628,150.8112007,-33.8207365,150.9906033\"\n\n$prefix\n[1] \"[out:xml][timeout:25];\\n(\\n\"\n\n$suffix\n[1] \");\\n(._;&gt;;);\\nout body;\"\n\n$features\n[1] \"[\\\"amenity\\\"=\\\"fast_food\\\"]\"\n\n$osm_types\n[1] \"node\"     \"way\"      \"relation\"\n\nattr(,\"class\")\n[1] \"list\"           \"overpass_query\"\nattr(,\"nodes_only\")\n[1] FALSE\n\n\nGet information on tags for highway. Wikipedia has a page OpenStreetMap Wiki on highway.\n\navailable_tags(\"highway\") %&gt;% head()\n\n# A tibble: 6 × 2\n  Key     Value       \n  &lt;chr&gt;   &lt;chr&gt;       \n1 highway bridleway   \n2 highway bus_guideway\n3 highway bus_stop    \n4 highway busway      \n5 highway construction\n6 highway corridor    \n\n\nUse the tags from above to define wide street. Plot the wide street using ggplot2.\n\nwide_streets &lt;- bboxsf%&gt;%\n  opq()%&gt;%\n  add_osm_feature(key = \"highway\", \n        value = c(\"motorway\", \"primary\", \"motorway_link\", \"primary_link\", \"secondary\", \"secondary_link\")) %&gt;%\n  osmdata_sf()\n\nggplot() +\n  geom_sf(data = wide_streets$osm_lines,\n          inherit.aes = FALSE,\n          color = \"black\")\n\n\n\n\n\n\n\n\nService or lane way can be defined by service.\n\nnarrow_streets &lt;- bboxsf%&gt;%\n  opq()%&gt;%\n  add_osm_feature(key = \"highway\", \n        value = c(\"service\")) %&gt;%\n  osmdata_sf()\n\n\nggplot() +\n  geom_sf(data = narrow_streets$osm_lines,\n          inherit.aes = FALSE,\n          color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n13.1.2 Google Maps API\nThe equivalent code in ggmap is provided below. Note that a key is required from Google Maps API.\n\nlibrary(ggmap)\nregister_google(key=\"Your Key\")\n#geocode\ngeocode (\"monash medical centre, clayton\")\n\n#trip\n#mapdist(\"5 stud rd dandenong\",\"monash medical centre\")\n\nThe next demonstration is the extraction of geocodes from multiple addresses embedded in a column of data within a data frame. This is more efficient compared to performing geocoding line by line. An example is provided on how to create your own icon on the leaflet document as well as taking a picture for publication.\n\nlibrary(dplyr)\nlibrary(tidyr) #unite verb from tidyr\nlibrary(readr)\nlibrary(tmaptools)\nlibrary(leaflet)\nlibrary(sf)\n\nclinic&lt;-read_csv(\"./Data-Use/TIA_clinics.csv\")\n\nRows: 25 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): City, Country, Setting, Pre-COVID-Assessment, Post-COVID-Assessmen...\ndbl  (1): COVID Alert Level\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclinic2&lt;-clinic %&gt;%  \n  unite (\"address\",City:Country,sep = \",\")%&gt;%   filter(!is.na(`Clinic-Status`)) \n\nload(\"./Data-Use/TIAclinics_geo.Rda\")\n\nclinics_geo&lt;-left_join(clinics_geo,clinic2, by=c(\"query\"=\"address\"))\n#create icon markers\n#icon markers\nicons_blue &lt;- awesomeIcons(\n  icon= 'medkit',\n  iconColor = 'black',\n  library = 'ion',\n  markerColor = \"blue\"\n)\nicons_red &lt;- awesomeIcons(\n  icon= 'medkit',\n  iconColor = 'black',\n  library = 'ion',\n  markerColor = \"red\"\n)\n\n#subset \nclinics_geo_active&lt;-clinics_geo %&gt;%filter(`Clinic-Status`==\"Active\")\nclinics_geo_inactive&lt;-clinics_geo %&gt;%filter(`Clinic-Status` !=\"Active\")\nm&lt;-leaflet(data=clinics_geo) %&gt;% \n  addTiles() %&gt;% #default is OSM\n    addAwesomeMarkers(lat=clinics_geo_active$lat,lng=clinics_geo_active$lon,\n          icon=icons_blue,label = ~as.character(clinics_geo_active$query) ) %&gt;%\n  addAwesomeMarkers(lat=clinics_geo_inactive$lat,lng=clinics_geo_inactive$lon,\n        icon=icons_red,label = ~as.character(clinics_geo_inactive$query)) \n\n#make pics using mapshot\nmapview::mapshot(m, url = paste0(getwd(),\n  file=\"./Data-Use/TIAclinic_world.html\"), file = paste0(getwd(), \"./Data-Use/TIAclinic_world.png\"))\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\n\nm\n\n\n\n\n\nGoogleway has the flexibility of easily interrogating Google Maps API for time of trip and traffic condition.\n\nlibrary(googleway)\nkey=\"Your Key\"\n#trip to MMC\n#traffic model can be optimistic, best guess, pessimistic\ngoogle_distance(\"5 stud rd dandenong\",\"monash medical centre\", key=key, departure_time=as.POSIXct(\"2019-12-03 08:15:00 AEST\"),traffic_model = \"optimistic\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Geospatial analysis</span>"
    ]
  },
  {
    "objectID": "geospatial-analysis.html#sp-and-sf-objects",
    "href": "geospatial-analysis.html#sp-and-sf-objects",
    "title": "13  Geospatial analysis",
    "section": "13.2 Sp and sf objects",
    "text": "13.2 Sp and sf objects\nThere are several methods for reading the shapefile data. Previously rgdal library was used. This approach creates files which can be described as S4 object in that there are slots for different data. The spatial feature sf approach is much easier to handle and the data can easily be subset and merged if needed. An example of conversion between sp and sf is provided.\nHere, base R plot is used to illustrate the shapefile of Melbourne and after parcellation into Voronois, centred by the hospital location.\n\nlibrary(dismo)\n\nLoading required package: raster\n\n\nLoading required package: sp\n\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(ggvoronoi)\n\nrgeos version: 0.6-3, (SVN revision 696)\n GEOS runtime version: 3.11.2-CAPI-1.17.2 \n Please note that rgeos will be retired during October 2023,\nplan transition to sf or terra functions using GEOS at your earliest convenience.\nSee https://r-spatial.org/r/2023/05/15/evolution4.html for details.\n GEOS using OverlayNG\n Linking to sp version: 2.0-0 \n Polygon checking: TRUE \n\nlibrary(tidyverse)\nlibrary(sf)\n\npar(mfrow=c(1,2)) #plot 2 objects in 1 row\nmsclinic&lt;-read.csv(\"./Data-Use/msclinic.csv\") %&gt;% filter(clinic==1, metropolitan==1)\n\n#convert to spatialpointdataframe\ncoordinates(msclinic) &lt;- c(\"lon\", \"lat\")\n#proj4string(msclinic) &lt;- CRS(\"+proj=longlat +datum=WGS84\")\n\nproj4string(msclinic) &lt;- CRS(\"+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs\")\n\n#create voronoi from msclinic data\n#msclinic object is in sp\nmsv&lt;-voronoi(msclinic)\n\n#create voronoi from msclinic data\n#object is in sp\nmsv&lt;-voronoi(msclinic)\n\n#subset Greater Melbourne\nMelb&lt;-st_read(\"./Data-Use/GCCSA_2016_AUST.shp\") %&gt;% filter(STE_NAME16==\"Victoria\",GCC_NAME16==\"Greater Melbourne\")\n\nReading layer `GCCSA_2016_AUST' from data source \n  `C:\\Users\\phant\\Documents\\Book\\Healthcare-R-Book\\Data-Use\\GCCSA_2016_AUST.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 34 features and 5 fields (with 18 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.81694 ymin: -43.74051 xmax: 167.998 ymax: -9.142176\nGeodetic CRS:  GDA94\n\n#transform sf to sp object\nMelb&lt;-as(Melb,Class=\"Spatial\")\nplot(Melb)\n\n#voronoi bound by greater melbourne\nvor &lt;- dismo::voronoi(msclinic, ext=extent(Melb))\n\n#intersect is present in base R, dplyr, raster, lubridate etc\nr &lt;- raster::intersect(vor, Melb)\n\n#r&lt;-intersect(msv,gccsa_fsp)\n\n#\nmsclinic$hospital&lt;-as.character(msclinic$hospital)\nplot(r, col=rainbow(length(r)), lwd=3)\n\n\n\n\n\n\n\n#sp back to sf\n#error with epsg conversion back\nmsclinic_sf&lt;-st_as_sf(msclinic,crs=4283)\n\n\n13.2.1 \nObtain the Brisbane data as sf object.\n\nComCentre &lt;- osmdata::osmdata_sf(query)\nnames(ComCentre$osm_points)\n\n [1] \"osm_id\"           \"name\"             \"addr:city\"        \"addr:housenumber\"\n [5] \"addr:postcode\"    \"addr:state\"       \"addr:street\"      \"amenity\"         \n [9] \"community_centre\" \"level\"            \"opening_hours\"    \"operator\"        \n[13] \"phone\"            \"website\"          \"geometry\"        \n\n\nExtract the centroid and polygons. Check that the coordinate reference system is EPSG 4326 or World Geodetic System (WGS) 84.\n\nComPoint&lt;-ComCentre$osm_points %&gt;% filter(amenity==\"community_centre\")\n\nComPoly &lt;- ComCentre$osm_polygons %&gt;%\n  st_centroid()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nCom &lt;- bind_rows(ComPoly, ComPoint)\nst_crs(Com)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nPlot the map of community centres in Ballarat with tmap. The view argument in tmap_mode set this plotting to interactive viewing with tmap with leaflet. The leaflet library can be called directly using tmap_leaflet\n\n#extract bounding box for Brisbane\n\n#Brisbane &lt;- osmdata::getbb(\"Brisbane, australia\", format_out = \"sf_polygon\")$multipolygon \n\nBrisbane&lt;-nominatim_polygon$geometry\n\nlibrary(tmap)\n\n#set interactive view\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n#plot outline of Ballarat\n  tm_shape(Brisbane)+\n tm_borders()+\n\n  #plot community centres\ntm_shape(Com) +\n  tm_dots()",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Geospatial analysis</span>"
    ]
  },
  {
    "objectID": "geospatial-analysis.html#thematic-map",
    "href": "geospatial-analysis.html#thematic-map",
    "title": "13  Geospatial analysis",
    "section": "13.3 Thematic map",
    "text": "13.3 Thematic map\nIn the first chapter we provided a thematic map example with ggplot2. here we will illustrate with mapview library using open data on Finland.\n\nlibrary(geofi)\nlibrary(ggplot2)\nlibrary(tmap)\nlibrary(tidyverse)\n\nlibrary(tidyr)\nlibrary(pxweb)\n\npxweb 0.16.2: R tools for the PX-WEB API.\nhttps://github.com/ropengov/pxweb\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following object is masked from 'package:raster':\n\n    crosstab\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n#data  on Finland\n#https://pxnet2.stat.fi/PXWeb/pxweb/en/Kuntien_avainluvut/\n#Kuntien_avainluvut__2021/kuntien_avainluvut_2021_viimeisin.px/table/\n#tableViewLayout1/\nFinPop&lt;-readxl::read_xlsx(\"./Data-Use/Kuntien avainluvut_20210704-153529.xlsx\", skip = 2)\n\nNew names:\n• `` -&gt; `...1`\n\nFinPop2&lt;-FinPop[-c(1),] #remove data for entire Finland\n\n#get shapefile for municipality\nmunicipalities &lt;- get_municipalities(year = 2020, scale = 4500) #sf object)\n\nRequesting response from: http://geo.stat.fi/geoserver/wfs?service=WFS&version=1.0.0&request=getFeature&typename=tilastointialueet%3Akunta4500k_2020\n\n\nWarning: Coercing CRS to epsg:3067 (ETRS89 / TM35FIN)\n\n\nData is licensed under: Attribution 4.0 International (CC BY 4.0)\n\n#join shapefile and population data\nmunicipalities2&lt;-right_join(municipalities, FinPop2, by=c(\"name\"=\"...1\")) %&gt;%\n  rename(Pensioner=`Proportion of pensioners of the population, %, 2019`,Age65=`Share of persons aged over 64 of the population, %, 2020`) %&gt;% na.omit()\n\n#plot map using mapview\nmapview::mapview(municipalities2[\"Age65\"],layer.name=\"Age65\")\n\n\n\n\n\nThis example illustrates how to add arguments in mapview\n\nlibrary(mapview)\nlibrary(sf)\nlibrary(tmaptools)\n\n#NY Shape file\nNYsf&lt;-st_read(\"./Data-Use/Borough_Boundaries/geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0.shp\")\n\nReading layer `geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0' from data source `C:\\Users\\phant\\Documents\\Book\\Healthcare-R-Book\\Data-Use\\Borough_Boundaries\\geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49612 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS84(DD)\n\n#NY subway -does not go to Staten Island\nNYsubline&lt;-st_read(\"./Data-Use/NYsubways/geo_export_147781bc-e472-4c12-8cd2-5f9859f90706.shp\")\n\nReading layer `geo_export_147781bc-e472-4c12-8cd2-5f9859f90706' from data source `C:\\Users\\phant\\Documents\\Book\\Healthcare-R-Book\\Data-Use\\NYsubways\\geo_export_147781bc-e472-4c12-8cd2-5f9859f90706.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 742 features and 6 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.03088 ymin: 40.57559 xmax: -73.75541 ymax: 40.90312\nGeodetic CRS:  WGS84(DD)\n\n#NY subway stations\nNYsubstation&lt;-st_read(\"./Data-Use/NYsubways/geo_export_0dab2fcf-79b8-409a-b940-7c98778a4418.shp\")\n\nReading layer `geo_export_0dab2fcf-79b8-409a-b940-7c98778a4418' from data source `C:\\Users\\phant\\Documents\\Book\\Healthcare-R-Book\\Data-Use\\NYsubways\\geo_export_0dab2fcf-79b8-409a-b940-7c98778a4418.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 473 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.03088 ymin: 40.57603 xmax: -73.7554 ymax: 40.90313\nGeodetic CRS:  WGS84(DD)\n\n#list of NY hospitals\nHosp&lt;-c(\"North Shore University Hospital\",\"Long Island Jewish Medical Centre\",\"Staten Island University Hospital\",\"Lennox Hill Hospital\",\"Long Island Jewish Forest Hills\",\"Long Island Jewish Valley Stream\",\"Plainview Hospital\",\"Cohen Children's Medical Center\",\"Glen Cove Hospital\",\"Syosset Hospital\")\n\n#geocode NY hospitals and return sf object\nHosp_geo&lt;-geocode_OSM(paste0(Hosp,\",\",\"New York\",\",\",\"USA\"),as.sf = TRUE)\n\nNo results found for \"Long Island Jewish Medical Centre,New York,USA\".\n\n\nNo results found for \"Lennox Hill Hospital,New York,USA\".\n\n#data from jama paper on variation in mortality from covid\nmapview(NYsf, zcol=\"boro_name\")+\n  mapview(NYsubline, zol=\"name\")+\n    #cex is the circle size default=6\n    mapview(NYsubstation, zol=\"line\",cex=1)+\n      mapview(Hosp_geo, zcol=\"query\", cex=3)\n\n\n\n\n\nThe example shown under Data wrangling on how to extract data from pdf is now put to use to create thematic map of stroke number per region in Denmark.\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(eurostat)\nlibrary(leaflet)\nlibrary(mapview)\nlibrary(tmaptools)\n\n#####################code to generate HospLocations.Rda\nload (\"./Data-Use/europeRGDK.Rda\")\n\n#create data on hospitals\n#hosp_addresses &lt;- c(\n#  AarhusHospital = \"aarhus university hospital, aarhus, Denmark\",\n#  AalborgHospital = \"Hobrovej 18-22 9100 aalborg, Denmark\",\n#  HolstebroHospital = \"Lægårdvej 12 7500 Holstebro, Holstebro, Denmark\",   VejleHospital=\"Vejle Sygehus,Beriderbakken 4, Vejle, Denmark\",\n#  EsbjergHospital=\"Esbjerg Sygehus, Esbjerg, Denmark\",\n#SoenderborgHospital=\"Sydvang 1C 6400 Sønderborg, Denmark\",\n#OdenseHospital=\"Odense Sygehus, Odense, Denmark\",   \n#RoskildeHospital=\"Roskilde Sygehus,  Roskilde, Denmark\",  \n#BlegdamsvejHospital=\"Rigshospitalet blegdamsvej, 9 Blegdamsvej, København, Denmark\",  \n#GlostrupHospital=\"Rigshospitalet Glostrup, Glostrup, Denmark\")\n\n#geocode hospitals using OpenStreetMap. This function works better with street addresses\n#HospLocations &lt;- tmaptools::geocode_OSM(hosp_addresses, as.sf=TRUE)\n\n#convert data into sf object\n#HospLocations &lt;- sf::st_transform(HospLocations,           sf::st_crs(europeRGDK)) \n\n#CSC comprehensive stroke centre\n#PSC primary stroke centre\n#HospLocations$Center&lt;-c(\"CSC\", \"PSC\", \"PSC\", \"PSC\", \"PSC\", \"PSC\", \"CSC\", \"PSC\", \"CSC\", \"PSC\")\n\n#save HospLocations\n#save(HospLocations, \"HospLocations.Rda\")\n\nload(\"./Data-Use/HospLocations.Rda\")\n\n##https://ec.europa.eu/eurostat/web/nuts/background\nload(\"./Data-Use/euro_nuts2_sf.Rda\")\nDKnuts2_sf&lt;- euro_nuts2_sf%&gt;% filter(str_detect(NUTS_ID,\"^DK\"))\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n#convert pdf to csv file\ndk&lt;-read.csv(\"./Data-Use/denmarkstrokepdf.csv\")\n\n#extract only data on large regions=NUTS2\ndk2&lt;-dk[c(4:8),]\n\n#clean up column X.1 containing stroke data\n#remove numerator before back slash then remove number before slash sign\ndk2$strokenum&lt;-str_replace(dk2$X.1,\"[0-9]*\",\"\") %&gt;%\n  str_replace(\"/\",\"\\\\\") %&gt;% as.numeric()\ndk2$Uoplyst&lt;-str_replace(dk2$Uoplyst,\"SjÃ¦lland\",\"Sjælland\")\n\n#merge sf file for DK nuts2 with stroke number\nDKnuts2_sf2&lt;-right_join(DKnuts2_sf,dk2,by=c(\"NUTS_NAME\"=\"Uoplyst\"))\n\n#add population from Statistics Denmark 2018 \nDKnuts2_sf2$pop&lt;-c(589148,1822659,835024, 1313596,1220763)\nDKnuts2_sf2$male&lt;-c(297679,894553,416092,657817,610358)\nDKnuts2_sf2$maleper&lt;-round(with(DKnuts2_sf2,pop/male),2)\n\n#plot map\nmapview(DKnuts2_sf2[\"strokenum\"], layer.name=\"Stroke Number\") +mapview(HospLocations, zcol=\"Center\", layer.name=\"Hospital Designation\")\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n13.3.1 Calculate distance to Hospital-OpenStreetMap\nDetermine distance hospital to centroid of kommunes rather than the larger regions of Denmark. This can be performed with OpenStreetMap or Google Maps API.\n\ndist_to_loc &lt;- function (geometry, location){\n    units::set_units(st_distance(st_centroid (geometry), location)[,1], km)\n}\n\n#set distance 10 km\n#change to 100 km\ndist_range &lt;- units::set_units(100, km)\n\n##\neuropeRGDK &lt;- mutate(europeRGDK,\n       DirectDistanceToAarhus = dist_to_loc(geometry,HospLocations[\"AarhusHospital\", ]),\n       DirectDistanceToAalborg     = dist_to_loc(geometry,HospLocations[\"AalborgHospital\", ]),\n       DirectDistanceToHolstebro     = dist_to_loc(geometry,HospLocations[\"HolstebroHospital\", ]),\n       DirectDistanceToVejle     = dist_to_loc(geometry,HospLocations[\"VejleHospital\", ]),\n       DirectDistanceToEsbjerg     = dist_to_loc(geometry,HospLocations[\"EsbjergHospital\", ]),\n       DirectDistanceToSoenderborg = dist_to_loc(geometry,HospLocations[\"SoenderborgHospital\", ]),\n       DirectDistanceToOdense     = dist_to_loc(geometry,HospLocations[\"OdenseHospital\", ]),\n       DirectDistanceToRoskilde     = dist_to_loc(geometry,HospLocations[\"RoskildeHospital\", ]),\n       DirectDistanceToBlegdamsvej     = dist_to_loc(geometry,HospLocations[\"BlegdamsvejHospital\", ]),\n       DirectDistanceToGlostrup     = dist_to_loc(geometry,HospLocations[\"GlostrupHospital\", ]),\n       #\n       DirectDistanceToNearest   = pmin(DirectDistanceToAarhus,\n      DirectDistanceToAalborg,DirectDistanceToHolstebro,\n      DirectDistanceToVejle,DirectDistanceToEsbjerg, DirectDistanceToSoenderborg,DirectDistanceToOdense,\n      DirectDistanceToRoskilde,DirectDistanceToBlegdamsvej,\n      DirectDistanceToGlostrup))\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\nStrokeHosp &lt;- filter(europeRGDK,                                               DirectDistanceToNearest &lt; dist_range) %&gt;%\n        mutate(Postcode = as.numeric(COMM_ID)) \n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\nhead(StrokeHosp)\n\nSimple feature collection with 6 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 8.709358 ymin: 56.83344 xmax: 8.940572 ymax: 56.98483\nGeodetic CRS:  +proj=longlat +ellps=GRS80 +no_defs\n        COMM_ID Shape_Leng   Shape_Area                       geometry\n1 DK10817738668  0.3605216 0.0030530589 MULTIPOLYGON (((8.940572 56...\n2 DK10817738669  0.1539856 0.0012336619 MULTIPOLYGON (((8.908073 56...\n3 DK10817738670  0.1630955 0.0009363543 MULTIPOLYGON (((8.886687 56...\n4 DK10817738671  0.2010890 0.0019239206 MULTIPOLYGON (((8.822215 56...\n5 DK10817738672  0.1926252 0.0011456026 MULTIPOLYGON (((8.750229 56...\n6 DK10817738673  0.1793186 0.0011254477 MULTIPOLYGON (((8.83318 56....\n  DirectDistanceToAarhus DirectDistanceToAalborg DirectDistanceToHolstebro\n1          117.7155 [km]           68.11356 [km]             64.41291 [km]\n2          115.7763 [km]           65.96041 [km]             64.48420 [km]\n3          114.0335 [km]           67.75456 [km]             60.72506 [km]\n4          118.1121 [km]           73.63410 [km]             59.09770 [km]\n5          119.4927 [km]           76.87186 [km]             57.33305 [km]\n6          114.0855 [km]           72.38971 [km]             55.92370 [km]\n  DirectDistanceToVejle DirectDistanceToEsbjerg DirectDistanceToSoenderborg\n1         140.9514 [km]           163.4109 [km]               230.7453 [km]\n2         139.8147 [km]           163.2912 [km]               229.6966 [km]\n3         136.7127 [km]           159.5629 [km]               226.5046 [km]\n4         138.2621 [km]           158.3096 [km]               227.7350 [km]\n5         138.0619 [km]           156.6019 [km]               227.3109 [km]\n6         134.1065 [km]           154.9979 [km]               223.6369 [km]\n  DirectDistanceToOdense DirectDistanceToRoskilde DirectDistanceToBlegdamsvej\n1          195.3515 [km]            245.3869 [km]               266.3579 [km]\n2          193.8254 [km]            243.2411 [km]               264.1433 [km]\n3          191.2311 [km]            242.1564 [km]               263.3819 [km]\n4          193.8937 [km]            246.9101 [km]               268.4274 [km]\n5          194.3217 [km]            248.7312 [km]               270.4758 [km]\n6          189.6643 [km]            243.1370 [km]               264.8318 [km]\n  DirectDistanceToGlostrup DirectDistanceToNearest Postcode\n1            258.4642 [km]           64.41291 [km]    28310\n2            256.2729 [km]           64.48420 [km]    28311\n3            255.3919 [km]           60.72506 [km]    28312\n4            260.3345 [km]           59.09770 [km]    28313\n5            262.3009 [km]           57.33305 [km]    28314\n6            256.6704 [km]           55.92370 [km]    28315",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Geospatial analysis</span>"
    ]
  },
  {
    "objectID": "geospatial-analysis.html#spatial-regression",
    "href": "geospatial-analysis.html#spatial-regression",
    "title": "13  Geospatial analysis",
    "section": "13.4 Spatial regression",
    "text": "13.4 Spatial regression\nThis is data published in Jama 29/4/2020 on COVD-19 in New York. The New York borough shapefiles were obtained from New York Open Data at https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm. For those wishing to evaluate other datasets, there’s lung and lip cancer data in SpatialEpi library, leukemia in DClusterm library. Key aspect of spatial regression is that neighbouring regions are similar and distant regions are less so. It uses the polyn2nb in spdep library to create the neighbourhood weight. The map of residual for the New York data does not suggest spatial association of residuals.\nThe Moran’s I is used to test for global spatial autocorrelation among adjacent regions in multidimensional space. Moran’s I requires calculation of neighbourhood. It is related to the number of regions and sum of all spatial weights.\n\n13.4.1 New York COVID-19 mortality\nThe example below illustrate the need to look at the data. It is not possible to perform spatial regression given the small size of the areal data (4 connected boroughs); Staten Island does not have adjoining border with the others nor share subway system. An intriguing possibility is that areal data analysis at the neighbourhood level would allow a granular examination of socioeconomic effect of COVID-19 on mortality data. To plot the railway lines with tmap the argument tm_lines is required whereas the argument tm_polygons is better suited for plotting the shape file. It\n\nlibrary(leaflet)\nlibrary(SpatialEpi)\nlibrary(spdep)\n\nLoading required package: spData\n\n\nTo access larger datasets in this package, install the spDataLarge\npackage with: `install.packages('spDataLarge',\nrepos='https://nowosad.github.io/drat/', type='source')`\n\nlibrary(spatialreg) #some of spdep moved to spatialreg\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n\nAttaching package: 'spatialreg'\n\n\nThe following objects are masked from 'package:spdep':\n\n    get.ClusterOption, get.coresOption, get.mcOption,\n    get.VerboseOption, get.ZeroPolicyOption, set.ClusterOption,\n    set.coresOption, set.mcOption, set.VerboseOption,\n    set.ZeroPolicyOption\n\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following objects are masked from 'package:raster':\n\n    area, select\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndfj&lt;-data.frame(\nBorough=c(\"Bronx\",\"Brooklyn\",\"Manhattan\",\"Queens\",\"Staten Island\"),\nPop=c(1432132,2582830,1628701,2278906,476179),\nAge65=c(12.8,13.9,16.5,15.7,16.2),\nWhite=c(25.1,46.6,59.2,39.6,75.1),\nHispanic=c(56.4,19.1,25.9,28.1,18.7),\nAfro.American=c(38.3,33.5,16.9,19.9,11.5),\nAsian=c(4.6,13.4,14,27.5,11),\nOthers=c(36.8,10.4,15.4,17,5.2),\nIncome=c(38467,61220,85066,69320,82166),\nBeds=c(336,214,534,144,234),\nCOVIDtest=c(4599,2970,2844,3800,5603),\nCOVIDhosp=c(634,400,331,560,370),\nCOVIDdeath=c(224,181,122,200,143),\nCOVIDdeathlab=c(173,132,91,154,117),\nDiabetes=c(16,27,15,22,25),\nObesity=c(32,12,8,11,8),\nHypertension=c(36,29,23,28,25)) %&gt;% \n  #reverse prevalence per 100000 to raw\n  mutate(Age65raw=round(Age65/100*Pop,0),\n               Bedsraw=round(Beds/100000*Pop,0),\n               COVIDtestraw=round(COVIDtest/100000*Pop,0),\n               COVIDhospraw=round(COVIDhosp/100000*Pop,0),\n               COVIDdeathraw=round(COVIDdeath/100000*Pop),0)\n#Expected\nrate&lt;-sum(dfj$COVIDdeathraw)/sum(dfj$Pop)\ndfj$Expected&lt;-with(dfj, Pop*rate )\n\n#SMR standardised mortality ratio\ndfj$SMR&lt;-with(dfj, COVIDdeathraw/Expected)\n\n#NY Shape file - see this file open from above chunk\nNYsf&lt;-st_read(\"./Data-Use/Borough_Boundaries/geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0.shp\")\n\nReading layer `geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0' from data source `C:\\Users\\phant\\Documents\\Book\\Healthcare-R-Book\\Data-Use\\Borough_Boundaries\\geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49612 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS84(DD)\n\n#join dataset\nNYsf&lt;-left_join(NYsf, dfj,by=c(\"boro_name\"=\"Borough\"))\n\n#contiguity based neighbourhood\nNY.nb&lt;-poly2nb(NYsf) \nis.symmetric.nb(NY.nb) # TRUE\n\n[1] TRUE\n\n#NY subway \nNYsubline&lt;-st_read(\"./Data-Use/NYsubways/geo_export_147781bc-e472-4c12-8cd2-5f9859f90706.shp\")\n\nReading layer `geo_export_147781bc-e472-4c12-8cd2-5f9859f90706' from data source `C:\\Users\\phant\\Documents\\Book\\Healthcare-R-Book\\Data-Use\\NYsubways\\geo_export_147781bc-e472-4c12-8cd2-5f9859f90706.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 742 features and 6 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.03088 ymin: 40.57559 xmax: -73.75541 ymax: 40.90312\nGeodetic CRS:  WGS84(DD)\n\n#raw data\ntm_shape(NYsf) + \n  tm_polygons(col='SMR',title='COVID raw') +\n  tm_shape(NYsubline)+tm_lines(col='name')\n\n\n\n\n\nThe standardized mortality ratio (ratio of mortality divided by expected value for each borough) for the boroughs were: Bronx (1.245), Brooklyn (1.006), Manhattan (0.678) Queens (1.111) and Staten Island (0.794). The Figure shows a strong relationship between standardized mortality ratio and Income (R2=0.816).\n\n#plot regression lines linear vs robust linear\nggplot(data=NYsf,aes(x=Income,y=COVIDdeath)) + geom_point() + geom_smooth(method='lm',col='darkblue',fill='blue') + geom_smooth(method='rlm',col='darkred',fill='red')\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nStandardized mortality ratio and Income among different ethnic groups in Boroughs of New York.\n\n#varies by ethinicty\ndfj_long&lt;-pivot_longer(data=dfj, names_to = \"Ethnicity\", values_to = \"Popsize\",cols=c(White,Afro.American,Asian,Hispanic,Others))\n\ncommonplot&lt;-list(\n  scale_size_continuous(name = \"Population size\"),xlab(\"Income (Thousand)\"),facet_wrap(~Ethnicity,nrow=2)\n)\nggplot(data=dfj_long, mapping=aes(x=Income/1000, y=SMR, size=Popsize,colour=Borough))+geom_point()+commonplot\n\n\n\n\n\n\n\n\nRobust regression to obtain residual for plotting with thematic maps.\n\n#robust linear models\nNYsf$resids &lt;- rlm(COVIDdeathraw~Pop+Age65raw,data=NYsf)$res\n\n#tmap robust linear model-residual\n#plot using color blind argument\npar(mfrow=c(2,1))\ntm_shape(NYsf) + tm_polygons(col='resids',title='Residuals')\n\nVariable(s) \"resids\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\ntm_shape(NYsf) + tm_polygons(col='resids',title='Residuals')+\n  tm_style(\"col_blind\")\n\nVariable(s) \"resids\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n#create spatial weights for neighbour lists\nr.id&lt;-attr(NYsf,\"region.id\")\nlw &lt;- nb2listw(NY.nb,zero.policy = TRUE) #W=row standardised\n\nIn this example, the Moran’s I test suggest that the null test can be rejected. This implies random distribution of stroke across the regions of Denmark.\n\n#globaltest spatial autocorrelation using Moran I test from spdep\ngm&lt;-moran.test(NYsf$SMR,listw = lw , na.action = na.omit, zero.policy = T)\ngm\n\n\n    Moran I test under randomisation\n\ndata:  NYsf$SMR  \nweights: lw  n reduced by no-neighbour observations\n  \n\nMoran I statistic standard deviate = 0.12086, p-value = 0.4519\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      -0.31010080       -0.33333333        0.03694814 \n\n\nThere’s no evidence of spatial autocorrelation at local level. Note that this data is too small and may not be the ideal to evaluate local Moran’s.\n\n#local test of autocorrelation\nlm&lt;-localmoran(NYsf$SMR,listw = nb2listw(NY.nb, zero.policy = TRUE, \n          style = \"C\") , na.action = na.omit, zero.policy = T)\n\nlm\n\n           Ii         E.Ii      Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.37737978 -0.362800450 0.264360322 -0.02835563      0.9773785\n2  0.00000000  0.000000000 0.000000000         NaN            NaN\n3 -0.05282102 -0.007107618 0.009392627 -0.47168273      0.6371533\n4  0.03765408 -0.147207581 0.086099458  0.63000884      0.5286888\n5 -1.25295769 -0.588823897 0.199930608 -1.48530599      0.1374628\nattr(,\"call\")\nlocalmoran(x = NYsf$SMR, listw = nb2listw(NY.nb, zero.policy = TRUE, \n    style = \"C\"), zero.policy = T, na.action = na.omit)\nattr(,\"class\")\n[1] \"localmoran\" \"matrix\"     \"array\"     \nattr(,\"quadr\")\n       mean    median     pysal\n1  High-Low  High-Low  High-Low\n2   Low-Low   Low-Low   Low-Low\n3  High-Low   Low-Low  High-Low\n4 High-High High-High High-High\n5  Low-High  Low-High  Low-High\n\n\n\n\n13.4.2 Danish Stroke Registry\nWe now return to spatial regression with the data from Danish stroke registry described above. First we will calculate the SMR\n\n#Expected\nrate&lt;-sum(DKnuts2_sf2$strokenum)/sum(DKnuts2_sf2$pop)\nDKnuts2_sf2$Expected&lt;-with(DKnuts2_sf2, pop*rate )\n\n#SMR standardised mortality ratio\nDKnuts2_sf2$SMR&lt;-with(DKnuts2_sf2, strokenum/Expected)\n\ntm_shape(DKnuts2_sf2) + \n  tm_polygons(col='SMR',title='Stroke') \n\n\n\n\n\nNow we plot the neighborhood weight to see if the Zealand island is linked to Jutland peninsula. The data is in the .nb file. It looks like we may need to recalculate the neighborhood as there are bridges between Zealand and Zealand. This is different situation from Staten Island and the other boroughs of New York.\n\n#contiguity based neighbourhood\nDKnut2.nb&lt;-poly2nb(DKnuts2_sf2) \nDKnut2.nb\n\nNeighbour list object:\nNumber of regions: 5 \nNumber of nonzero links: 6 \nPercentage nonzero weights: 24 \nAverage number of links: 1.2 \n\nis.symmetric.nb(DKnut2.nb) \n\n[1] TRUE\n\n\nPlotting in base R with sf object requires extracting the geometry and coordinates. Need to modify the link between Zealand and Jutland.\n\nplot(st_geometry(DKnuts2_sf2))\nplot(DKnut2.nb,coords=st_coordinates(st_centroid(st_geometry(DKnuts2_sf2))),\n     add=TRUE,pch=16,col='darkred')\n\n\n\n\n\n\n\n\nThe solution is provided in stack overflow. https://gis.stackexchange.com/questions/413159/how-to-assign-a-neighbour-status-to-unlinked-polygons\n\nDKconnect &lt;- function(polys, nb, distance=\"centroid\"){\n    \n    if(distance == \"centroid\"){\n        coords = sf::st_coordinates(sf::st_centroid(sf::st_geometry(polys)))\n        dmat = as.matrix(dist(coords))\n    }else if(distance == \"polygon\"){\n        dmat = sf::st_distance(polys) + 1 # offset for adjacencies\n        diag(dmat) = 0 # no self-intersections\n    }else{\n        stop(\"Unknown distance method\")\n    }\n    \n    gfull = igraph::graph.adjacency(dmat, weighted=TRUE, mode=\"undirected\")\n    gmst = igraph::mst(gfull)\n    edgemat = as.matrix(igraph::as_adj(gmst))\n    edgelistw = spdep::mat2listw(edgemat)\n    edgenb = edgelistw$neighbour\n    attr(edgenb,\"region.id\") = attr(nb, \"region.id\")\n    allnb = spdep::union.nb(nb, edgenb)\n    allnb\n}\n\n#run function\nDKnut2_connected.nb = DKconnect(DKnuts2_sf2,DKnut2.nb)\n\nWarning in spdep::mat2listw(edgemat): style is M (missing); style should be set\nto a valid value\n\nplot(st_geometry(DKnuts2_sf2))\nplot(DKnut2_connected.nb,\n     coords=st_coordinates(st_centroid(st_geometry(DKnuts2_sf2))),\n     add=TRUE,pch=16,col='darkred')\n\n\n\n\n\n\n\n#create spatial weights for neighbour lists\nr.id&lt;-attr(DKnuts2_sf2,\"id\")\nlw &lt;- nb2listw(DKnut2_connected.nb,zero.policy = TRUE) #W=row standardised\n\nPerform robust regression\n\n#robust linear models\nDKnuts2_sf2$resids &lt;- MASS::rlm(SMR~maleper,data=DKnuts2_sf2)$res\n\n#tmap robust linear model-residual\n#plot using color blind argument\ntm_shape(DKnuts2_sf2) + tm_polygons(col='resids',title='Residuals')+\n  tm_style(\"col_blind\")\n\nVariable(s) \"resids\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\nCheck for spatial autocorrelation\n\n#globaltest spatial autocorrelation using Moran I test from spdep\ngm&lt;-moran.test(DKnuts2_sf2$SMR,listw = lw , \n               na.action = na.omit, zero.policy = T)\ngm\n\n\n    Moran I test under randomisation\n\ndata:  DKnuts2_sf2$SMR  \nweights: lw    \n\nMoran I statistic standard deviate = -0.98413, p-value = 0.8375\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n       -0.7354200        -0.2500000         0.2432931 \n\n\nLocal test of autocorrelation\n\n#local test of autocorrelation\nlm&lt;-localmoran(DKnuts2_sf2$SMR,\n    listw = nb2listw(DKnut2_connected.nb, zero.policy = TRUE, \n          style = \"C\") , na.action = na.omit, zero.policy = T)\nlm\n\n          Ii        E.Ii    Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.4777173 -0.20238585 0.4276556 -0.4210259      0.6737362\n2 -0.9220960 -0.21612919 0.4418446 -1.0620611      0.2882079\n3 -1.3492564 -0.49175508 0.6214513 -1.0877554      0.2767031\n4 -0.2490245 -0.14095220 0.2605379 -0.2117284      0.8323189\n5 -0.1984676 -0.09276265 0.1789140 -0.2499040      0.8026616\nattr(,\"call\")\nlocalmoran(x = DKnuts2_sf2$SMR, listw = nb2listw(DKnut2_connected.nb, \n    zero.policy = TRUE, style = \"C\"), zero.policy = T, na.action = na.omit)\nattr(,\"class\")\n[1] \"localmoran\" \"matrix\"     \"array\"     \nattr(,\"quadr\")\n       mean   median    pysal\n1  High-Low High-Low High-Low\n2   Low-Low  Low-Low Low-High\n3 High-High High-Low High-Low\n4  Low-High Low-High Low-High\n5  Low-High Low-High Low-High\n\n\nSpatial regression with spdep. The spatial filtering removes spatial dependency for regression analysis.\n\n##spdep & spatialreg\nfit.ols&lt;-lm(SMR~maleper, data=DKnuts2_sf2, \n            listw=lw,zero.policy=T, type=\"lag\", method=\"spam\")\n\nsummary(fit.ols)\n\nSAR - Lag model\n\nfit.lag&lt;-lagsarlm(SMR~maleper, data=DKnuts2_sf2, \n                  listw=lw,zero.policy=T, type=\"lag\", method=\"spam\")\n\nsummary(fit.lag, Nagelkerke=T)\n\nSpatial Durbin Model\n\n#spatialreg\n\nfit.durb&lt;-lagsarlm(SMR~maleper,data=DKnuts2_sf2, \n                   listw=lw,zero.policy=T, type=\"mixed\", method=\"spam\")\n\nsummary(fit.durb, Nagelkerke=T)\n\nSpatial Durbin Error Model\n\nfit.errdurb&lt;-errorsarlm(SMR~maleper, data=DKnuts2_sf2, listw=lw,zero.policy=T,etype=\"emixed\", method=\"spam\")\n\nsummary(fit.errdurb, Nagelkerke=T)\n\nSAC Model\n\nfit.sac&lt;-sacsarlm(SMR~maleper,data=DKnuts2_sf2, \n                  listw=lw,zero.policy=T, type=\"sac\", method=\"MC\")\n\nsummary(fit.sac, Nagelkerke=T)\n\nspatial filtering\n\n#function from spatialreg\n#Set ExactEV=TRUE to use exact expectations and variances rather than the expectation and variance of Moran's I from the previous iteration, default FALSE\n\nDKFilt&lt;-SpatialFiltering(SMR~maleper, \n        data=DKnuts2_sf2,\n        nb=DKnut2_connected.nb,\n        #ExactEV = TRUE,\n        zero.policy = TRUE,style=\"W\")\n\n\n\n13.4.3 INLA\nThis section uses Bayesian modeling for regression with fitting of the model by Integrated Nested Lapace Approximation (INLA). https://www.r-bloggers.com/spatial-data-analysis-with-inla/. For those wanting to analyse leukemia in New York instead of COVID-19, the dataset NY8 is available from DClusterm. INLA approximates the posterior distribution as latent Gaussian Markov random field. In this baseline analysis, the poisson model is performed without any random effect terms.\n\nlibrary(INLA)\nnb2INLA(\"DKnut2.graph\", DKnut2_connected.nb)\n#This create a file called ``LDN-INLA.adj'' with the graph for INLA\n\nDK.adj &lt;- paste(getwd(),\"/DK.graph\",sep=\"\")\n\n#Poisson model with no random latent effect-ideal baseline model\nm1&lt;-inla(SMR~ 1+maleper, data=DKnuts2_sf2, family=\"poisson\",\n         E=DKnuts2_sf2$Expected,control.predictor = list(compute = TRUE),\n  control.compute = list(dic = TRUE, waic = TRUE), verbose = T )\n\nR1&lt;-summary(m1)\n\nIn this next analysis, the Poisson model was repeated with random effect terms. This step was facilitated by adding the index term.\n\n#Poisson model with random effect \n#index to identify random effect ID\nDKnuts2_sf2$ID &lt;- 1:nrow(DKnuts2_sf2)\nm2&lt;-inla(SMR~ 1+ maleper +f(ID, model = \"iid\"), data=DKnuts2_sf2, family=\"poisson\",\n         E=DKnuts2_sf2$Expected,control.predictor = list(compute = TRUE),\n  control.compute = list(dic = TRUE, waic = TRUE) )\nR2&lt;-summary(m2)\nDKnut2_sf2$FIXED.EFF &lt;- m1$summary.fitted[, \"mean\"]\nDKnut2_sf2$IID.EFF &lt;- m2$summary.fitted[, \"mean\"]\n\n#plot regression on map\ntSMR&lt;-tm_shape(DKnuts2_sf2)+tm_polygons(\"SMR\")\ntFIXED&lt;-tm_shape(DKnuts2_sf2)+tm_polygons(\"FIXED.EFF\")\ntIID&lt;-tm_shape(DKnuts2_sf2)+tm_polygons(\"IID.EFF\")\n\nThis next paragraph involves the use of spatial random effects in regression models. Examples include conditional autoregressive (CAR) and intrinsic CAR (ICAR).\n\n# Create sparse adjacency matrix\nDK.mat &lt;- as(nb2mat(DKnut2_connected.nb, \n                    style = \"B\",zero.policy = TRUE),\"Matrix\") \n\n#S=variance stabilise\n# Fit model\nm.icar &lt;- inla(SMR ~ 1+maleper+   \n    f(ID, model = \"besag\", graph = DK.mat),\n  data = DKnuts2_sf2, E = DKnuts2_sf2$Expected, family =\"poisson\",\n  control.predictor = list(compute = TRUE),\n  control.compute = list(dic = TRUE, waic = TRUE))\n\nR3&lt;-summary(m.icar)\n\nThe Besag-York-Mollie (BYM) now accounts for spatial dependency of neighbours. It includes random effect from ICA and index.\n\nm.bym = inla(SMR ~ -1+ maleper+   \n    f(ID, model = \"bym\", graph = DK.mat),\n  data = DKnuts2_sf2, E = DKnuts2_sf2$Expected, family =\"poisson\",\n  control.predictor = list(compute = TRUE),\n  control.compute = list(dic = TRUE, waic = TRUE))\n\nR4&lt;-summary(m.bym)\n\n\nICARmatrix &lt;- Diagonal(nrow(DK.mat), apply(DK.mat, 1, sum)) - DK.mat\nCmatrix &lt;- Diagonal(nrow(DKnuts2_sf2), 1) -  ICARmatrix\nmax(eigen(Cmatrix)$values)\nm.ler = inla(SMR ~ -1+maleper+ \n    f(ID, model = \"generic1\", Cmatrix = Cmatrix),\n  data = DKnuts2_sf2, E = DKnuts2_sf2$Expected, family =\"poisson\",\n  control.predictor = list(compute = TRUE),\n  control.compute = list(dic = TRUE, waic = TRUE))\nR5&lt;-summary(m.ler)\n\nSpatial econometric model usch as spatial lag model includes covariates and autoregres on the response variable.\n\n#X\nmmatrix &lt;- model.matrix(SMR ~ 1, DKnuts2_sf2)\n#W\nW &lt;- as(nb2mat(DKnut2_connected.nb, style = \"W\", zero.policy = TRUE), \"Matrix\")\n#Q\nQ.beta = Diagonal(n = ncol(mmatrix), x = 0.001)\n#Range of rho\nrho.min&lt;- -1\nrho.max&lt;- 1\n#Arguments for 'slm'\nargs.slm = list(\n   rho.min = rho.min ,\n   rho.max = rho.max,\n   W = W,\n   X = mmatrix,\n   Q.beta = Q.beta\n)\n#Prior on rho\nhyper.slm = list(\n   prec = list(\n      prior = \"loggamma\", param = c(0.01, 0.01)),\n      rho = list(initial=0, prior = \"logitbeta\", param = c(1,1))\n)\n\n#SLM model\nm.slm &lt;- inla( SMR ~ -1+maleper+\n     f(ID, model = \"slm\", args.slm = args.slm, hyper = hyper.slm),\n   data = DKnuts2_sf2, family = \"poisson\",\n   E = DKnuts2_sf2$Expected,\n   control.predictor = list(compute = TRUE),\n   control.compute = list(dic = TRUE, waic = TRUE)\n)\n\nR6&lt;-summary(m.slm)\nmarg.rho.internal &lt;- m.slm$marginals.hyperpar[[\"Rho for ID\"]]\nmarg.rho &lt;- inla.tmarginal( function(x) {\n  rho.min + x * (rho.max - rho.min)\n}, marg.rho.internal)\ninla.zmarginal(marg.rho, FALSE)\nplot(marg.rho, type = \"l\", main = \"Spatial autocorrelation\")\n\nSpatial model selection\n\nDKnut2_sf2$ICAR &lt;- m.icar$summary.fitted.values[, \"mean\"]\nDKnut2_sf2$BYM &lt;- m.bym$summary.fitted.values[, \"mean\"]\nDKnut2_sf2$LEROUX &lt;- m.ler$summary.fitted.values[, \"mean\"]\nDKnut2_sf2$SLM &lt;- m.slm$summary.fitted.values[, \"mean\"]\n\nlabels&lt;-c(\"Fixed\",\"IID\", \"ICAR\",\"BYM\",\"LEROUX\",\"SLM\")\nMarginal_Likelihood&lt;-c(R1$mlik[1],R2$mlik[1],R3$mlik[1],R4$mlik[1],R5$mlik[1],\n                       R6$mlik[1])\nMarginal_Likelihood&lt;-round(Marginal_Likelihood,2)\nWAIC&lt;-c(R1$waic[[1]],R2$waic[[1]],R3$waic[[1]],R4$waic[[1]],R5$waic[[1]],\n        R6$waic[[1]])\nWAIC&lt;-round(WAIC,2)\nDIC&lt;-c(R1$dic[[1]],R2$dic[[1]],R3$dic[[1]],R4$dic[[1]],R5$dic[[1]],R6$dic[[1]])\nDIC&lt;-round(DIC,2)\nResults&lt;-data.frame(labels,Marginal_Likelihood,WAIC,DIC)\nknitr::kable(Results)\n\n#plot maps\ntICAR&lt;-tm_shape(DKnut2_sf2)+tm_polygons(\"ICAR\")\ntBYM&lt;-tm_shape(DKnut2_sf2)+tm_polygons(\"BYM\")\ntLEROUX&lt;-tm_shape(DKnut2_sf2)+tm_polygons(\"LEROUX\")\ntSLM&lt;-tm_shape(DKnut2_sf2)+tm_polygons(\"SLM\")\n#arrange in grid using tmap arrange\ncurrent.mode &lt;- tmap_mode(\"plot\")\ntmap_arrange(tFIXED,tIID,tICAR,tBYM,tLEROUX,tSLM)\ntmap_mode(current.mode)\n\n\n\n13.4.4 Stan\n\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.19.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:dismo':\n\n    kfold\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n#S=variance stabilise\n# Fit model\n\n#brm.lag&lt;- brm (COVIDdeathraw ~ 1+Age65raw+Income+sar(NY.nb, type = \"lag\"),\n#               data = DKnut2_sf2, data2 = list(NY.nb = NY.nb),\n#            chains = 2, cores = 2)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Geospatial analysis</span>"
    ]
  },
  {
    "objectID": "geospatial-analysis.html#machine-learning-in-spatial-analysis",
    "href": "geospatial-analysis.html#machine-learning-in-spatial-analysis",
    "title": "13  Geospatial analysis",
    "section": "13.5 Machine learning in spatial analysis",
    "text": "13.5 Machine learning in spatial analysis\nUp until now we have used frequentist and Bayesian spatial regression methods. Spatial data can also be analysed using machine learning such as random forest.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Geospatial analysis</span>"
    ]
  },
  {
    "objectID": "geospatial-analysis.html#spatio-temporal-regression",
    "href": "geospatial-analysis.html#spatio-temporal-regression",
    "title": "13  Geospatial analysis",
    "section": "13.6 Spatio-temporal regression",
    "text": "13.6 Spatio-temporal regression\nSpatio-temporal regression combines a spatial model with a temporal model. In many cases of low disease incidene in each region it may not be possible to identify any temporal trend at a regional level.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Geospatial analysis</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "14  Appendix",
    "section": "",
    "text": "14.1 Brief introduction to Matrix\nEigenvector (characteristic vector) and eigenvalue (characteristic value) are useful in solving system of linear equations and providing quicker solution to solving task such as finding the nth power of a matrix. The eigenvector v of a n x n square matrix M is defined as a non-zero vector of M such that the product of Mev is equal to the product of the scalar eigenvalue λ and v.\nEigenvector analysis can be used to describe pattern. Here, the eigenvector has an interpretation in term of the direction of the data and the eigenvalue provides a scaling measure of the length or change in direction of the vector (when both are multiplied). Using the description above regarding finding the nth power of a matrix M, the eigenvectors remain unchanged but the eigenvalues change in proportion to the nth power of M.\nIn terms of applications, eigen decomposition approach provides a non-bias method for discovering pattern in complex data; this is typically performed by eigen decomposition of the covariance matrix. This approach had been used in describing pattern of infarct in hypoxic ischaemic injury780, facial recognition.\nIn the section on regression, we can show that the eigenvalue can be interpreted in term of the variance of data. Eigenvalue can be described\nIn geometric term, the eigenvalue describes the length of its associated eigenvector and in regression analyss, it can be considered as the variance of the data. Eigenvalue is calculated by finding the solution for the root of characteristic polynomial equation. For a 2 x 2 matrix,\nThe characteristic polynomial is given by\nThe Trace(M) is given by The determinant is given by\nThe quadratic formula can be used to solve for λ.\nIn this case, the eigenvalue is -1 and 3. The eigenvector of M is given by\nThe identity matrix consist of ones on the diagnonal and zero elsewhere. For a 2 x 2 matrix, the identity matrix is\nFor the eigenvalue -1, its eigenvector is given by\nThe eigenvectors include (1, 1) . For the eigenvalue 3, its associated eigenctor include (1, -1). The solution for the characteristic equation becomes more complex as the size of the matrix increase. For large matrices, the power method is used to derive the eigenvalue. The determinant is equal to the product if the eigenvalues (-1 x 3= -3). The trace is equal to the sum of the eigenvalues (-1+3=2) or the sum of the diagonal of the matrix (1+1 =2).\nA matrix is invertible (non-singular) if it satisfies the following\nThe matrix M-1 is the inverse of matrix M. The inverse of M is calculated as follow.\nw=1, x=0, y=1,z=0 Matrix which cannot be inverted is termed singular. The determinant of a square matrix is zero. The significance of invertible matrix will be seen in the section on collinearity when dealing with regression analsysis.\nSparse matrix is a matrix populated mostly by zeros. In a network sense it implies lack of cohesion in the network. Inverting sparse matrix is challenging due to the reduced rank associate with this type of matrix. The solutions require various form of penalisation (penalised regression is discuss next under Regression).\nThe null space of a matrix can be considered as the solutions to homogenous system of linear equations.\nThe null space of a matrix is also termed the kernel of that matrix (3, -9, 1). As shown above, the null space of a matrix contains a zero vector. In other word the coefficient matrix is zeros. The augemented matrix is displayed on the left. The null space is a subspace of this matrix.\nThe rank of matrix of a matrix can be considered as a measure of the ‘non-singularness’ of a matrix. For example, a 3 x 4 matrix with 2 independent rows has rank of 2. In other word, it describes the number of independent columns of a matrix or its eigenvector. It also describes the dimension of the image of the linear transformation that is performed on the matrix. Often the expression that the matrix is full rank (contains independent rows and columns of data) is used in regression to infer that the matrix is invertible and the data are non-collinear. An example of zero rank or collinear matrix is shown here\nObserve that the second column is three times the first column. The determinant of this matrix is 3 x 6 – 2 x 9=0. This matrix is not invertible and rank deficient. The interpretation is that the rank of the augmented matrix is equal or to that for the coefficient matrix, the solution to system of linear equation is stable. If the rank of an augmented matrix is larger than the coefficient matrix, the matrix is rank deficient.mA special form of multivariate regression use the rank of matrix in regression (reduced rank regression). This type of regression is useful in the case of of minority class distribution where the majority of the data are at one end of the spectrum.\nA positive semi-definite matrix is invertible and has full rank. It is defined as a matrix which can be obtained by the multiplication of a matrix and its transpose (denoted by T in upper case).\nSuch a matrix is symmetrical. Examples include correlation and covariance matrices.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#brief-introduction-to-matrix",
    "href": "appendix.html#brief-introduction-to-matrix",
    "title": "14  Appendix",
    "section": "",
    "text": "I is the n x n identity matrix",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#regression",
    "href": "appendix.html#regression",
    "title": "14  Appendix",
    "section": "14.2 Regression",
    "text": "14.2 Regression\nIn the section below, a matrix approach to regression is provided. This step was performed as it explains for issues encountered when doing regression analyses. It is easier to explain collinearity using matrix. Examples of performing regression analyses and steps required to check for errors are given. This section deals with univariable and multivariable analyses. The next chapter will discuss the different multivariate analyses.\n\n14.2.1 Linear regression\nA matrix description of parameter estimation is given here because it is easier to describe the multiple regression (Smith 1998). \\(Y= \\beta_1X_1+ \\beta_2X_2+...\\beta_jX_j\\) A matrix is an array of data as rows and columns. For the imaging data, the individual voxel is represented on each column and each row refers to another patient. A vector refers to a matrix of one column.\nIn matrix form, the multiple regression equation takes the form \\(Y= \\beta_j X + E\\) where X is the predictor matrix, Y is the dependent matrix, β is the regression coefficient and E is the error term (not the intercept). The X is a \\(j^{th}\\) rows by ith columns matrix and Y and E is a \\(j^{th}\\) column vector.\nAlgebraic manipulation of equation 1, shows that the solution for β is \\((X^TX)^{-1}X^TY\\). \\(X^T\\) is the transpose of X such that the columns of are now written as rows. The correlation matrix of X is given by \\(X^TX\\). The solution for β is possible if the correlation matrix \\(X^TX\\) can be inverted. The inverse of a matrix can be found if it is has a square shape (columns and rows of the matrix are equal) and the determinant of the matrix is not singular or nonzero (Smith 1998). The uniqueness of the matrix inverse is that when a matrix is multiplied by its inverse, the solution is an identity matrix. The diagonal elements of a matrix are ones and the remainders of the square matrix are zeros for an identity matrix. For a rectangular matrix, multiplication of the matrix by its Moore-Penrose pseudo-inverse results in an identity matrix.\nThe terms in the inverse of \\(X^TX\\) are divided by the determinants of \\(X^TX\\) . For simplicity, the determinant of a 2 x 2 matrix is given by \\(ad-bc\\) for a matrix A. \\(A=\\left[\\begin{array}{cc}a & b\\\\c & d\\end{array}\\right]\\) The inverse of this matrix A is given by \\(\\frac{1}{ad-bc}\\left[\\begin{array}{cc}d & -b\\\\-c & a\\end{array}\\right]\\) From this equation, it can be seen that the determinant and the inverse exist if the result is nonzero. For a correlation matrix \\(X^TX\\) of the form \\(\\left[\\begin{array}{cc}n & nX_*\\\\nX_* & nX_*^2\\end{array}\\right]\\) then the determinant is zero. \\(n \\times nX_*^2 -nX_* \\times nX_*\\) Hence, there is no unique solution for this equation. If near collinearity exists and the determinant approach zero, there are infinite possible combinations that can result in a least squares estimate of the parameter. In this example, the matrix is singular then the columns of X are likely to be linearly related to each other (collinearity). In this case, the regression coefficient is unstable with the variance of the regression coefficient large. Further, small changes in the dependent variables lead to fluctuations in the regression solution.\n\n14.2.1.1 Leasts squares\nThe least squares solution for the parameter β refers to the fitting of the line between the intercept and the variables of X such that the Euclidian distance between the observed variables Y and expected or predicted variables are as small as possible. The metric for the fit is the sum of squared errors SSE (or residual mean square error/residual sum of squares) and is given by \\(SSE=\\sum(Y-\\beta X)^T(Y-\\beta X)\\) The variance-covariance matrix of \\(\\beta\\) is given by \\(Var(\\beta)=\\delta^2(X^TX)\\)\n\n\n14.2.1.2 Collinearity\nCollinearity or relatedness among the predictors is often forgotten in many analysis. This issue can lead to instability in the regression coefficients. There are several tests for collinearity: variance inflation factor and condition index. The variance inflation factor (VIF) is proportional to \\(VIF = 1/1-R^2\\). In this example, as the predictors become strogly correlated \\(R^2\\) apporaches 1 and VIF will approaches infintity. Collinearity is present if VIF &gt;10(Kleinbaum, Kupper, and Muller 1978). Collinearity can also be assessed by measuring the condition index (Phan et al. 2006). This can be given as ratio between the largest and the corresponding eignvalue \\(CI_i=\\sqrt(\\frac{\\lamda_{max}{\\lambda_i})\\). Collinearity is present when the condition index is &gt;30(Kleinbaum, Kupper, and Muller 1978).\n\n\n14.2.1.3 weighted least squares\nIn the section above, it was not stated explicitly but the least squares regression model is appropriate when the variances of the predictor variables are uniform (Smith 1998). In this case the variance of the error matrix is a diagonal matrix with equal diagonal elements. When there are unreliable data or errors in measurement of some of the data, the variance of the error matrix contains unequal diagonal elements. The consequence is that the least squares formula leads to instability of the parameter estimate. Weighted least squares regression is similar to least squares regression except that the variance matrix is weighted \\(w\\) by the variance of the columns of the predictor variables. \\(\\beta=(X^TV^-1)^-1X^TV^-1Y\\).The diagonal matrix \\(V\\) contains weights expressed as 1/w along the diagonal elements. These weights are used to down play the importance of the regions where noise occurs and gives appropriate importance to the true data region. The result is a reduction in the variance of the regression coefficients and hence stability in their estimate. Weighted least squares is introduced here because the weighted PLS is used in the PLS-PLR model.\n\n\n\n14.2.2 Logistic regression\nIn logistic regression, there is no algebraic solution to determine the parameter estimate (β coefficient) and a numerical method (trial and error approach) such as maximum likelihood estimate is used to determine the parameter estimate. The General Linear Model \\(Y=\\beta X\\) is modified to the form of the Generalised Linear Model (GLIM) by adding a non-linear link function \\(g(\\mu)\\) . The equation now resembles \\(Y=g(\\beta X)\\).\nConsider the binary response (1, 0) as proportions of the predictor variables. is the probability of an event and is the probability of an event not occurring. The odds ratio \\(OR\\) is given by \\(OR=\\frac{1}{1-p}\\). A logit transformation take the form \\(\\n=logit_i(p_i)=ln(\\frac{p_i}{1-p_i})=\\sum(X_ij\\beta_i)\\). The logistic equation takes the form \\(p_i=\\frac{e^{X_j\\beta_j}}{1-e^{X_j\\beta_j}}\\)\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKleinbaum, David G, Lawrence L. Kupper, and Keith E. Muller. 1978. “Applied Regression Analysis and Other Multivariable Methods.” In.\n\n\nPhan, T. G., G. A. Donnan, M. Koga, L. A. Mitchell, M. Molan, G. Fitt, W. Chong, M. Holt, and D. C. Reutens. 2006. “The ASPECTS template is weighted in favor of the striatocapsular region.” Neuroimage 31 (2): 477–81.\n\n\nSmith, Norman R. Draper. Harry. 1998. Applied Regression Analysis, Third Edition. 3rd ed. Wiley Series in Probability and Statistics. John Wiley & Sons, Inc.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aboumatar, H., and R. A. Wise. 2019. “Notice of Retraction.\nAboumatar et al. Effect of a\nProgram Combining Transitional\nCare and Long-term\nSelf-management Support on\nOutcomes of Hospitalized Patients\nWith Chronic Obstructive\nPulmonary Disease: A\nRandomized Clinical Trial.\nJAMA.\n2018;320(22):2335-2343.” JAMA 322 (14): 1417–18.\n\n\nAgresti, Alan, and Brent A. Coull. 1998. “Approximate Is Better\nThan ‘Exact’ for Interval Estimation of Binomial\nProportions.” The American Statistician 52 (2): 119–26.\nhttps://doi.org/10.1080/00031305.1998.10480550.\n\n\n“Atlas-Based Whole Brain White Matter Analysis Using Large\nDeformation Diffeomorphic Metric Mapping: Application to Normal Elderly\nand Alzheimer’s Disease Participants.” 2009. Neuroimage\n46 (2): 486–99. https://doi.org/10.1016/j.neuroimage.2009.01.002.\n\n\nAung T, Kromhout D, Halsey J. 2018. “Associations of Omega-3 Fatty\nAcid Supplement Use with Cardiovascular Disease Risks: Meta-Analysis of\n10 Trials Involving 77 917 Individuals.” Jama Cardiol,\nMarch. https://doi.org/doi:\n10.1001/jamacardio.2017.5205.\n\n\nBalduzzi, Sara, Gerta Rücker, and Guido Schwarzer. 2019. “How to\nPerform a Meta-Analysis with R: A Practical\nTutorial.” Evidence-Based Mental Health, no. 22: 153–60.\n\n\nBeare, R., J. Chen, T. G. Phan, R. K. Lees, M. Ali, A. Alexandrov, P. M.\nBath, et al. 2015. “Googling\nStroke\nASPECTS\nto Determine Disability:\nExploratory Analysis from\nVISTA-Acute\nCollaboration.” PLoS ONE 10 (5):\ne0125687.\n\n\nBerkhemer, O. A., P. S. Fransen, D. Beumer, L. A. van den Berg, H. F.\nLingsma, A. J. Yoo, W. J. Schonewille, et al. 2015. “A randomized trial of intraarterial\ntreatment for acute ischemic stroke.” N. Engl. J.\nMed. 372 (1): 11–20.\n\n\nBJ, Biggerstaff. 2000. “Comparing Diagnostic Tests: A Simple\nGraphic Using Likelihood Ratios.” Stat Med 19 (March):\n649–63. https://doi.org/10.1002/(sici)1097-0258(20000315)19:5&lt;649::aid-sim371&gt;3.0.co;2-h.\n\n\nBrin, Sergey, and Lawrence Page. 1998. “The\nAnatomy of a Large-Scale\nHypertextual Web Search\nEngine.” Computer Networks and ISDN Systems\n30 (1-7): 107–17. https://doi.org/10.1016/S0169-7552(98)00110-X.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial Proportion.”\nStatist. Sci. 16 (2): 101–33. https://doi.org/10.1214/ss/1009213286.\n\n\nCampbell, B. C., P. J. Mitchell, T. J. Kleinig, H. M. Dewey, L.\nChurilov, N. Yassi, B. Yan, et al. 2015. “Endovascular therapy for ischemic stroke\nwith perfusion-imaging selection.” N. Engl. J.\nMed. 372 (11): 1009–18.\n\n\nCentor, Robert M., Rabih Geha, and Reza Manesh. 2019. “The Pursuit of Diagnostic Excellence.”\nJAMA Network Open 2 (12): e1918040–40. https://doi.org/10.1001/jamanetworkopen.2019.18040.\n\n\nCohen, Jacob. 1977. “CHAPTER 6 - Differences Between\nProportions.” In Statistical Power Analysis for the\nBehavioral Sciences, edited by Jacob Cohen, 179–213. Academic\nPress. https://doi.org/https://doi.org/10.1016/B978-0-12-179060-8.50011-6.\n\n\nD. S. Nagin, V. L. Passos, B. L. Jones, and R. E. Tremblay. 2018.\n“Group-Based Multi-Trajectory Modeling.” Statistical\nMethods in Medical Research 27: 2015–23. https://doi.org/10.1177/0962280216673085.\n\n\nDecroocq, Méghane, Carole Frindel, Pierre Rougé, Makoto Ohta, and\nGuillaume Lavoué. 2023. “Modeling and Hexahedral Meshing of\nCerebral Arterial Networks from Centerlines.” Medical Image\nAnalysis 89: 102912. https://doi.org/https://doi.org/10.1016/j.media.2023.102912.\n\n\nDerek B Archer, Stephen A Coombes, David E Vaillancourt. 2018. “A\nTemplate and Probabilistic Atlas of the Human Sensorimotor Tracts Using\nDiffusion MRI.” Cerebral Cortex 28 (5): 1685--1699. https://doi.org/10.1093/cercor/bhx066.\n\n\nDiamond, G. A. 1992. “What price\nperfection? Calibration and discrimination of clinical\nprediction models.” J Clin Epidemiol 45 (1):\n85–89.\n\n\nDurif, G., L. Modolo, J. Michaelsson, J. E. Mold, S. Lambert-Lacroix,\nand F. Picard. 2018. “High\ndimensional classification with combined adaptive sparse\nPLS and logistic\nregression.” Bioinformatics 34 (3): 485–93.\n\n\nFornito, Alex. 2016. “Graph Theoretic Analysis of Human Brain\nNetworks.” In Neuromethods, edited by Massimo Filippi,\n2nd ed., 119:283–314. Neuromethods 119. United States of America: Humana\nPress. https://doi.org/10.1007/978-1-4939-5611-1_10.\n\n\nFort, G., and S. Lambert-Lacroix. 2005. “Classification using partial least squares\nwith penalized logistic regression.”\nBioinformatics 21 (7): 1104–11.\n\n\nFriedman, J. H., and C. B. Roosen. 1995. “An introduction to multivariate adaptive\nregression splines.” Stat Methods Med Res 4 (3):\n197–217.\n\n\nFriston, K., J. Phillips, D. Chawla, and C. Buchel. 2000. “Nonlinear\nPCA: characterizing interactions\nbetween modes of brain activity.” Philos. Trans. R.\nSoc. Lond., B, Biol. Sci. 355 (1393): 135–46.\n\n\nGopal, A. D., N. R. Desai, T. Tse, and J. S. Ross. 2015. “Reporting of noninferiority trials in\nClinicalTrials.gov and corresponding\npublications.” JAMA 313 (11): 1163–65.\n\n\nGraf E, Sauerbrei W, Schmoor C. 1999. “Assessment and Comparison\nof Prognostic Classification Schemes for Survival Data.” Stat\nMed 18 (September): 2529–45. https://doi.org/10.1002/(sici)1097-0258(19990915/30)18:17/18&lt;2529::aid-sim274&gt;3.0.co;2-5.\n\n\nGrambsch, P. M., and T. M. Therneau. 1994. “Proportional hazards tests and diagnostics based on\nweighted residuals.” Biometrika 81 (3): 515–26.\nhttps://doi.org/10.1093/biomet/81.3.515.\n\n\nGrinsztajn, Léo, Edouard Oyallon, and Gaël Varoquaux. 2022. “Why\nDo Tree-Based Models Still Outperform Deep Learning on Tabular\nData?” ArXiv abs/2207.08815. https://doi.org/10.48550/arXiv.2207.08815.\n\n\nGuo, Jingyi, and Andrea Riebler. 2015. “meta4diag: Bayesian Bivariate Meta-analysis of Diagnostic\nTest Studies for Routine Practice.” arXiv\ne-Prints, December, arXiv:1512.06220. https://arxiv.org/abs/1512.06220.\n\n\nHarrell FE Jr, Pryor DB, Califf RM. 1982. “Evaluating the Yield of\nMedical Tests.” JAMA 247 (May): 2543–46.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference and\nPrediction. 2nd ed. Springer. http://www-stat.stanford.edu/~tibs/ElemStatLearn/.\n\n\nHu, Hu, Y. 2019. “Marine Omega‐3 Supplementation and\nCardiovascular Disease: An Updated Meta‐analysis of 13 Randomized\nControlled Trials Involving 127 477 Participants.” JAHA,\nSeptember. https://doi.org/https://doi.org/10.1161/JAHA.119.013543.\n\n\nHubbard AE, Fleischer NL, Ahern J. 2010. “To GEE or Not to GEE:\nComparing Population Average and Mixed Models for Estimating the\nAssociations Between Neighborhood Risk Factors and Health.”\nEpidemiology 21: 467–74. https://doi.org/10.1097/EDE.0b013e3181caeb9.\n\n\nIngall, Timothy John, William Michael O’Fallon, Kjell Asplund, Lewis\nRobert Goldfrank, Vicki S. Hertzberg, Thomas Arthur Louis, and Teresa J.\nHengy Christianson. 2004. “Findings from the Reanalysis of the\nNINDS Tissue Plasminogen Activator for Acute Ischemic Stroke Treatment\nTrial.” Stroke 35 (10): 2418–24. https://doi.org/10.1161/01.STR.0000140891.70547.56.\n\n\nJ, Muschelli. 2020. “ROC and AUC with a Binary Predictor: A\nPotentially Misleading Metric.” J Classif 37 (October):\n696–708. https://doi.org/10.1007/s00357-019-09345-1.\n\n\nJaeschke, R., G. H. Guyatt, and D. L. Sackett. 1994. “Users’ guides to the medical literature.\nIII. How to use an\narticle about a diagnostic test. B. What are\nthe results and will they help me in caring for my patients?\nThe Evidence-Based\nMedicine Working\nGroup.” JAMA 271 (9): 703–7.\n\n\nJamakovic, A., and Piet Van Mieghem. 2008. “On the Robustness of\nComplex Networks by Using the Algebraic Connectivity.” In\nNetworking, edited by Amitabha Das, Hung Keng Pung, Francis\nBu-Sung Lee, and Lawrence Wai-Choong Wong, 4982:183–94. Lecture Notes in\nComputer Science. Springer. http://dblp.uni-trier.de/db/conf/networking/networking2008.html#JamakovicM08.\n\n\nK. Mehta, J. Ogden, B. Ljungquist, and G. A. Ascoli. 2023. “Online\nConversion of Reconstructed Neural Morphologies into Standardized SWC\nFormat.” Nature Communications 14: 7429. https://doi.org/10.1038/s41467-023-42931-x.\n\n\nKahan, Jairath, B. C. 2014. “The Risks and Rewards of Covariate\nAdjustment in Randomized Trials: An Assessment of 12 Outcomes from 8\nStudies.” Trials 15: 139. https://doi.org/10.1186/1745-6215-15-139.\n\n\nKaji, A. H., and R. J. Lewis. 2015. “Noninferiority Trials:\nIs a New Treatment\nAlmost as Effective as\nAnother?” JAMA 313 (23): 2371–72.\n\n\nKempster, P. A., C. A. McLean, and T. G. Phan. 2016. “Ten year clinical experience with stroke and\ncerebral vasculitis.” J Clin Neurosci 27 (May):\n119–25.\n\n\nKleinbaum, David G, Lawrence L. Kupper, and Keith E. Muller. 1978.\n“Applied Regression Analysis and Other Multivariable\nMethods.” In.\n\n\nLeffondre K, Regeasse A, Abrahamowicz M. 2004. “Statistical\nMeasures Were Proposed for Identifying Longitudinal Patterns of Change\nin Quantitative Health Indicators.” J Clin Epidemiol 57:\n1049–62. https://doi.org/10.1016/j.jclinepi.2004.02.012.\n\n\nLongato, Enrico, Martina Vettoretti, and Barbara Di Camillo. 2020.\n“A Practical Perspective on the Concordance Index for the\nEvaluation and Selection of Prognostic Time-to-Event Models.”\nJournal of Biomedical Informatics 108: 103496.\nhttps://doi.org/https://doi.org/10.1016/j.jbi.2020.103496.\n\n\nLudwig, L., P. Darmon, and B Guerci. 2020. “Computing and interpreting the Number Needed to Treat for\nCardiovascular Outcomes Trials:Perspective on GLP-1 RA and SGLT-2i\ntherapies.” Cardiovasc Diabetol, May. https://doi.org/10.1186/s12933-020-01034-3.\n\n\nLuo, Dehui, Xiang Wan, Jiming Liu, and Tiejun Tong. 2018.\n“Optimally Estimating the Sample Mean from the Sample Size,\nMedian, Mid-Range, and/or Mid-Quartile Range.” Statistical\nMethods in Medical Research 27 (6): 1785–805. https://doi.org/10.1177/0962280216669183.\n\n\nMarx, Heiner C, Arthur. Bucher. 2003. “Numbers Needed to Treat\nDerived from Meta-Analysis: A Word of Caution.” BMJ\nEvidence-Based Medicine 8 (2): 36–37. https://doi.org/10.1136/ebm.8.2.36.\n\n\nMathur MB, Riddell CA, Ding P. 2018. “Web Site and r Package for\nComputing e-Values.” Epidemiology, e45–47. https://doi.org/10.1097/EDE.0000000000000864.\n\n\nMazziotta J, Evans A, Toga A. 2001. “A Probabilistic Atlas and\nReference System for the Human Brain: International Consortium for Brain\nMapping (ICBM).” Philos Trans R Soc Lond B Biol Sci 356\n(1412): 1293–1322. https://doi.org/10.1098/rstb.2001.0915.\n\n\nMcElfresh, Duncan C., Sujay Khandagale, Jonathan Valverde, C.\nVishakPrasad, Ben Feuer, Chinmay Hegde, Ganesh Ramakrishnan, Micah\nGoldblum, and Colin White. 2023. “When Do Neural Nets Outperform\nBoosted Trees on Tabular Data?” ArXiv abs/2305.02997. https://doi.org/10.48550/arXiv.2305.02997.\n\n\nMcGrath, Sean, Stephan Katzenschlager, Alexander J. Zimmer, Alexander\nSeitel, Russell J. Steele, and Andrea Benedetti. 2022. “Standard\nError Estimation in Meta-Analysis of Studies Reporting Medians.”\nStatistical Methods in Medical Research 32: 373–88. https://doi.org/10.1177/09622802221139233.\n\n\nMcGrath, Sean, XiaoFei Zhao, Russell Steele, Brett D. Thombs, Andrea\nBenedetti, the DEPRESsion Screening Data (DEPRESSD) Collaboration,\nBrooke Levis, et al. 2020. “Estimating the Sample Mean and\nStandard Deviation from Commonly Reported Quantiles in\nMeta-Analysis.” Statistical Methods in Medical Research\n29 (9): 2520–37. https://doi.org/10.1177/0962280219889080.\n\n\nMolnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018.\n“Iml: An r Package for Interpretable Machine Learning.”\nJOSS 3 (26): 786. https://doi.org/10.21105/joss.00786.\n\n\nMoses, L. E., D. Shapiro, and B. Littenberg. 1993. “Combining independent studies of a\ndiagnostic test into a summary\nROC curve: data-analytic\napproaches and some additional considerations.” Stat\nMed 12 (14): 1293–1316.\n\n\nPhan, T. G., J. Chen, G. Donnan, V. Srikanth, A. Wood, and D. C.\nReutens. 2010. “Development of a\nnew tool to correlate stroke outcome with infarct topography: a\nproof-of-concept study.” Neuroimage 49 (1):\n127–33.\n\n\nPhan, T. G., J. Chen, S. Singhal, H. Ma, B. B. Clissold, J. Ly, and R.\nBeare. 2018. “Exploratory\nUse of Decision Tree\nAnalysis in Classification of\nOutcome in Hypoxic-Ischemic\nBrain Injury.” Front\nNeurol 9: 126.\n\n\nPhan, T. G., B. B. Clissold, H. Ma, J. V. Ly, and V. Srikanth. 2017.\n“Predicting\nDisability after Ischemic Stroke\nBased on Comorbidity Index and\nStroke Severity-From the\nVirtual International Stroke\nTrials Archive-Acute\nCollaboration.” Front Neurol 8: 192.\n\n\nPhan, T. G., B. Clissold, J. Ly, H. Ma, C. Moran, V. Srikanth, K. R.\nLees, et al. 2016. “Stroke\nSeverity and Comorbidity Index\nfor Prediction of Mortality after\nIschemic Stroke from the Virtual\nInternational Stroke Trials\nArchive-Acute\nCollaboration.” J Stroke Cerebrovasc\nDis 25 (4): 835–42.\n\n\nPhan, T. G., A. Demchuk, V. Srikanth, B. Silver, S. C. Patel, P. A.\nBarber, S. R. Levine, and M. D. Hill. 2013. “Proof of concept study: relating infarct\nlocation to stroke disability in the\nNINDS\nrt-PA trial.” Cerebrovasc.\nDis. 35 (6): 560–65.\n\n\nPhan, T. G., G. A. Donnan, S. M. Davis, and G. Byrnes. 2006.\n“Proof-of-principle phase\nII MRI\nstudies in stroke: sample size estimates from dichotomous and continuous\ndata.” Stroke 37 (10): 2521–25.\n\n\nPhan, T. G., G. A. Donnan, M. Koga, L. A. Mitchell, M. Molan, G. Fitt,\nW. Chong, M. Holt, and D. C. Reutens. 2006. “The\nASPECTS\ntemplate is weighted in favor of the striatocapsular\nregion.” Neuroimage 31 (2): 477–81.\n\n\nPhan, T. G., T. Kooblal, C. Matley, S. Singhal, B. Clissold, J. Ly, A.\nG. Thrift, V. Srikanth, and H. Ma. 2019. “Stroke Severity\nVersus Dysphagia Screen as\nDriver for Post-stroke\nPneumonia.” Front Neurol 10: 16.\n\n\nPhan, T. G., N. Krishnadas, V. W. Y. Lai, M. Batt, L. A. Slater, R. V.\nChandra, V. Srikanth, and H. Ma. 2019. “Meta-Analysis of\nAccuracy of the Spot Sign for\nPredicting Hematoma Growth and\nClinical Outcomes.”\nStroke 50 (8): 2030–36.\n\n\nReitsma, J. B., A. S. Glas, A. W. Rutjes, R. J. Scholten, P. M. Bossuyt,\nand A. H. Zwinderman. 2005. “Bivariate analysis of sensitivity and\nspecificity produces informative summary measures in diagnostic\nreviews.” J Clin Epidemiol 58 (10): 982–90.\n\n\nRodriguez-Linares, Leandro, Xose Vila, Maria Jose Lado, Arturo Mendez,\nAbraham Otero, and Constantino Antonio Garcia. 2017. Heart Rate\nVariability Analysis with the r Package RHRV. Use r! Springer.\nhttps://doi.org/\n10.1007/978-3-319-65355-6.\n\n\nSalminen, P., H. Paajanen, T. Rautio, P. Nordstrom, M. Aarnio, T.\nRantanen, R. Tuominen, et al. 2015. “Antibiotic Therapy vs\nAppendectomy for Treatment of\nUncomplicated Acute Appendicitis:\nThe\nAPPAC\nRandomized Clinical\nTrial.” JAMA 313 (23): 2340–48.\n\n\nSeneviratne, U., Z. M. Low, Z. X. Low, A. Hehir, S. Paramaswaran, M.\nFoong, H. Ma, and T. G. Phan. 2019. “Medical health care utilization cost of\npatients presenting with psychogenic nonepileptic\nseizures.” Epilepsia 60 (2): 349–57.\n\n\nShi, Jiandong, Dehui Luo, Xiang Wan, Yue Liu, Jiming Liu, Zhaoxiang\nBian, and Tiejun Tong. 2023. “Detecting the Skewness of Data from\nthe Five-Number Summary and Its Application in Meta-Analysis.”\nStatistical Methods in Medical Research 32 (7): 1338–60. https://doi.org/10.1177/09622802231172043.\n\n\nSignori, Alessio, Fabio Pellegrini, Francesca Bovis, Luca Carmisciano,\nCarl de Moor, and Maria Pia Sormani. 2020. “Comparison of Placebos and Propensity Score Adjustment in\nMultiple Sclerosis Nonrandomized Studies.” JAMA\nNeurology, April. https://doi.org/10.1001/jamaneurol.2020.0678.\n\n\nSinghal, Shaloo, Jian Chen, Richard Beare, Henry Ma, John Ly, and Thanh\nG. Phan. 2012. “Application of Principal Component Analysis to\nStudy Topography of Hypoxic–Ischemic Brain Injury.”\nNeuroImage 62 (1): 300–306. https://doi.org/https://doi.org/10.1016/j.neuroimage.2012.04.025.\n\n\nSmeeth, A. Ebrahim, L. Haines. 1999. “Numbers\nneeded to treat derived from meta-analyses–sometimes informative,\nusually misleading.” BMJ 318: 1548–51. https://doi.org/10.1136/bmj.318.7197.1548.\n\n\nSmith, Norman R. Draper. Harry. 1998. Applied Regression Analysis,\nThird Edition. 3rd ed. Wiley Series in Probability and Statistics.\nJohn Wiley & Sons, Inc.\n\n\nStensrud, M. J., and M. A. Hernan. 2020. “Why Test for\nProportional Hazards?”\nJAMA, March.\n\n\nTJ, Fagan. 1975. “Nomogram for Bayes’s Theorem.” N Engl\nJ Med 293 (July): 257. https://doi.org/10.1056/NEJM197507312930513.\n\n\nTzourio-Mazoyer N, Papathanassiou D, Landeau B. 2002. “Automated\nAnatomical Labeling of Activations in SPM Using a Macroscopic Anatomical\nParcellation of the MNI MRI Single-Subject Brain.”\nNeuroimage 15 (1): 273–89. https://doi.org/10.1006/nimg.2001.0978.\n\n\nVanderWeele TJ, Ding P. 2017. “Sensitivity Analysis in\nObservational Research: Introducing the e-Value.” Ann Intern\nMed 167 (August): 268–74. https://doi.org/10.7326/M16-2607.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in r with\nthe Metafor Package.” Journal of Statistical Software 36\n(3): 1–48. https://doi.org/10.18637/jss.v036.i03.\n\n\nWan, Wang, X. 2014. “Estimating the Sample Mean and Standard\nDeviation from the Sample Size, Median, Range and/or Interquartile\nRange.” BMC Med Res Methodol, December.\nhttps://doi.org/https://doi.org/10.1186/1471-2288-14-135.\n\n\nWang, Rui, Stephen W. Lagakos, James H. Ware, David J. Hunter, and\nJeffrey M. Drazen. 2007. “Statistics in Medicine — Reporting of\nSubgroup Analyses in Clinical Trials.” New England Journal of\nMedicine 357 (21): 2189–94. https://doi.org/10.1056/NEJMsr077003.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. 2nd ed. Chapman & Hall/CRC the r Series. Chapman;\nHall/CRC; 2 edition (May 30, 2019).",
    "crumbs": [
      "References"
    ]
  }
]